## 7.4 最优行动值估计: Q-learning

本节介绍强化学习中最经典的算法之一——Q-Learning算法[38,39]。需注意的是，Sarsa算法仅能估计给定策略的行动值，必须结合策略改进步骤才能获得最优策略；而Q-Learning算法可直接估计最优行动值，从而直接求解最优策略。

### 7.4.1算法描述

Q-Learning算法为

$$\begin{aligned}&q_{t+1}(s_{t},a_{t})=q_{t}(s_{t},a_{t})-\alpha_{t}(s_{t},a_{t})\left[q_{t}(s_{t},a_{t})-\left(r_{t+1}+\gamma\max_{a\in\mathcal{A}(s_{t+1})}q_{t}(s_{t+1},a)\right)\right],\quad(7.18)\\&q_{t+1}(s,a)=q_{t}(s,a),\quad\text{for all }(s,a)\neq(s_{t},a_{t}),\end{aligned}$$

<<<<<<< HEAD
其中 $t =0,1,2, \dots$。此处 $q_t(s_t, a_t)$表示状态-动作对 $(s_t, a_t)$的最优行动值估计值，$\alpha_t(s_t, a_t)$为对应状态-行动对的学习率。

Q-learning的表达式与Sarsa相似，二者的区别仅在于**TD目标**：Q-learning的TD目标为$r_{t+1} + \gamma \max_a q_t(s_{t+1}, a)$，而Sarsa的TD目标为$r_{t+1} + \gamma q_t(s_{t+1}, a_{t+1})$。此外，给定$(s_t, a_t)$后，Sarsa在每次迭代中需要$(r_{t+1}, s_{t+1}, a_{t+1})$，而Q-learning仅需$(r_{t+1}, s_{t+1})$。
=======
其中 $t =0,1,2, \dots$。此处 $q_t(s_t, a_t)$表示状态-动作对 $(s_t, a_t)$的最优行动值估计值，$\alpha_t(s_t, a_t)$为对应状态-动作对的学习率。

Q-learning的表达式与Sarsa相似，二者的区别仅在于TD目标：Q-learning的TD目标为$r_{t+1} + \gamma \max_a q_t(s_{t+1}, a)$，而Sarsa的TD目标为$r_{t+1} + \gamma q_t(s_{t+1}, a_{t+1})$。此外，给定$(s_t, a_t)$后，Sarsa在每次迭代中需要$(r_{t+1}, s_{t+1}, a_{t+1})$，而Q-learning仅需$(r_{t+1}, s_{t+1})$。
>>>>>>> 50b98123eef511e961e9c05b37b46f2a667dafb0

为什么Q-learning被设计为$(7.18)$式中的表达式，它在数学上实现了什么功能？Q-learning是一种随机近似算法，用于求解以下方程：

$$q(s,a)=\mathbb{E}\left[R_{t+1}+\gamma\max_{a}q(S_{t+1},a)|S_{t}=s,A_{t}=a\right].\tag{7.19}$$

这是用行动值表示的贝尔曼最优方程。其证明过程见Box 7.5。Q-Learning的收敛性分析与定理7.1类似，此处从略。更多细节可参阅文献[32,39]。

<<<<<<< HEAD
!!! note
    这里得到的$q$值不是说哪一个策略的$q$值，而是最优的$q$值，对应的即为最优的策略。

### 7.4.2 Off-policy和On-policy

接下来我们引入两个重要概念：策略学习(on-policy learning)与非策略学习(off-policy learning)。Q学习与TD算法的细微差异在于：Q学习属于非策略学习，而其他算法均为策略学习。

任何强化学习任务中都存在两种策略：行为策略和目标策略。行为策略用于生成经验样本，而目标策略则持续更新以收敛至最优策略。当行为策略与目标策略相同时，该学习过程称为**on-policy**；若二者不同，则称为**off-policy**。

Off-policy learning的优势在于能够基于其他策略生成的样本进行最优策略学习，这些样本可能来自人类执行的策略等来源。一个重要特例是：行为策略可选择具有强探索性的策略，能够探索所有的状态-行动对。如果我们想要估计所有状态-动作对的行动值，就必须确保轨迹能充分访问每个状态-动作对。虽然Sarsa算法采用$\epsilon$-贪婪策略维持基础探索能力，但$\epsilon$值通常较小，因而探索能力有限。相比之下，若采用具有强探索能力的策略轨迹生成访问每个状态-行动对的足够多的回合，再通过off-policy learning获取最优策略，即可显著提升学习效率。
=======
### 7.4.2 Off-policy和On-policy

接下来我们引入两个重要概念：on-policy learning与off-policy learning。Q学习与其他时序差分（TD）算法的细微差异在于：Q学习属于off-policy learning，而其他算法均为on-policy learning。

任何强化学习任务中都存在两种策略：行为策略和目标策略。行为策略用于生成经验样本，而目标策略则持续更新以收敛至最优策略。当行为策略与目标策略相同时，该学习过程称为**on-policy**；若二者不同，则称为**off-policy**。

Off-policy learning的优势在于能够基于其他策略生成的样本进行最优策略学习，这些样本可能来自人类操作者执行的策略等来源。一个重要特例是：行为策略可选择具有强探索性的策略。例如，若需估计所有状态-动作对的行动值函数，必须确保采样轨迹能充分访问每个状态-动作对。虽然Sarsa算法采用$\epsilon$-贪婪策略维持基础探索能力，但$\epsilon$值通常较小，因而探索能力有限。相比之下，若采用具有强探索能力的策略生成轨迹，再通过off-policy learning获取最优策略，可显著提升学习效率。
>>>>>>> 50b98123eef511e961e9c05b37b46f2a667dafb0

要判断一个算法是on-policy还是off-policy，可以从两方面考察：第一是算法求解的数学问题；第二是算法所需的经验样本。

- Sarsa是一种on-policy算法:

<<<<<<< HEAD
    Sarsa算法在每次迭代中包含两个步骤：第一步是通过求解策略$\pi$的贝尔曼方程来评估该策略，这需要$\pi$生成的样本，因此$\pi$此时作为行为策略；第二步是基于$\pi$的估计值获取改进策略，此时$\pi$作为不断更新并最终收敛至最优策略的目标策略。因此行为策略与目标策略是同一策略。
=======
    原因如下。Sarsa算法在每次迭代中包含两个步骤：第一步是通过求解策略$\pi$的Bellman方程来评估该策略，这需要$\pi$生成的样本，因此$\pi$此时作为行为策略；第二步是基于$\pi$的估计值获取改进策略，此时$\pi$作为不断更新并最终收敛至最优策略的目标策略。由此可见，行为策略与目标策略实为同一策略。
>>>>>>> 50b98123eef511e961e9c05b37b46f2a667dafb0

    从另一个角度来看，我们可以分析算法所需的样本。Sarsa算法在每次迭代中需要的样本包括$(s_t, a_t, r_{t+1}, s_{t+1}, a_{t+1})$。这些样本的生成方式如下所示：

    $$s_t\xrightarrow{\pi_b}a_t\xrightarrow{\mathrm{model}}r_{t+1},s_{t+1}\xrightarrow{\pi_b}a_{t+1}$$

<<<<<<< HEAD
    可以看出，行为策略$\pi_b$是在状态$s_t$下生成行动$a_t$、在状态 $s_{t+1}$下生成行动$a_{t+1}$的策略。Sarsa算法的目标是估计目标策略 $\pi_T$在状态-行动对 $(s_t, a_t)$处的行动值；之所以称为目标策略，是因为它在每次迭代中会根据估计值进行改进。实际上，$\pi_T$与 $\pi_b$是相同的，因为对 $\pi_T$的评估依赖于样本 $(r_{t+1}, s_{t+1}, a_{t+1})$，其中 $a_{t+1}$正是遵循 $\pi_b$(也即$\pi_t(s_{t+1})$)生成的。换言之，Sarsa所评估的策略正是用于生成经验样本的策略。

- Q-Learning是一种off-policy算法:
    
    根本原因在于，Q-learning是用于求解贝尔曼最优方程的算法，而Sarsa则是求解给定策略的贝尔曼方程。求解贝尔曼方程可以评估对应策略的性能，而求解贝尔曼最优方程则能直接生成最优值与最优策略。

    特别地，Q-learning每次迭代所需的样本为$(s_t, a_t, r_{t+1}, s_{t+1})$。这些样本的生成方式如下所示：

    $$s_t\xrightarrow{\pi_b}a_t\xrightarrow{\mathrm{model}}r_{t+1},s_{t+1}$$

    可以看出，行为策略 $\pi_b$是生成状态$s_t$下行动$a_t$的策略。Q-Learning算法的目标是估计$(s_t, a_t)$的最优行动值函数。这一估计过程依赖于样本$(r_{t+1}, s_{t+1})$。如果$(s_t, a_t)$是给定的，那么$(r_{t+1},s_{t+1})$不依赖于策略$\pi_b$，因为它由系统模型(即$p(s^\prime|s,a),p(r|s,a)$)决定。因此，对$(s_t, a_t)$最优行动值的估计与$\pi_b$无关，我们可以使用任意$\pi_b$来生成$s_t$状态下的$a_t$。此外，此处目标策略$\pi_T$是基于估计的最优值得到的贪婪策略(算法7.3)。行为策略不必与$\pi_T$相同。

    !!! note
        注: Q-learning的行为策略就是需要从$s_t$出发根据一个策略得到一个$a_t$，而目标策略就是根据计算得到的$q$来选择对应的行动，当$q$逐渐收敛到最优$q$值时，也就收敛到了最优策略。

- MC learning是一种on-policy算法:

    原因与Sarsa算法类似。待评估和改进的目标策略与生成样本的行为策略是相同的。
=======
    可以看出，行为策略 $\pi_b$是在状态 $s_t$下生成动作 $a_t$、在状态 $s_{t+1}$下生成动作 $a_{t+1}$的策略。Sarsa算法的目标是估计目标策略 $\pi_T$在状态-动作对 $(s_t, a_t)$处的动作值函数；之所以称为目标策略，是因为它在每次迭代中会根据估计值进行改进。实际上，$\pi_T$与 $\pi_b$是相同的，因为对 $\pi_T$的评估依赖于样本 $(r_{t+1}, s_{t+1}, a_{t+1})$，其中 $a_{t+1}$正是遵循 $\pi_b$生成的。换言之，Sarsa所评估的策略正是用于生成样本的策略。

- Q-Learning是一种off-policy算法:
    
    根本原因在于，Q-learning是用于求解贝尔曼最优方程的算法，而 Sarsa则是求解给定策略的贝尔曼方程。求解贝尔曼方程可以评估对应策略的性能，而求解贝尔曼最优方程则能直接生成最优值函数与最优策略。

    特别地，Q-learning每次迭代所需的样本为 $(s_t, a_t, r_{t+1}, s_{t+1})$。这些样本的生成方式如下所示：

    $$s_t\xrightarrow{\pi_b}a_t\xrightarrow{\mathrm{model}}r_{t+1},s_{t+1}$$

    可以看出，行为策略 $\pi_b$是生成状态$s_t$下行动$a_t$的策略。Q-Learning算法的目标是估计$(s_t, a_t)$的最优行动值函数。这一估计过程依赖于样本$(r_{t+1}, s_{t+1})$。生成$(r_{t+1},s_{t+1})$的过程不涉及$\pi_b$，因为它由系统模型(或通过与环境的交互)决定。因此，对$(s_t, a_t)$最优行动值的估计与 $\pi_b$无关，我们可以使用任意$\pi_b$来生成$s_t$状态下的$a_t$。此外，此处目标策略$\pi_T$是基于估计的最优值得到的贪婪策略(算法7.3)。行为策略不必与 $\pi_T$相同。

- MC learning是一种on-policy算法:

    原因与Sarsa算法类似。待评估和改进的目标策略（target policy）与生成样本的行为策略（behavior policy）是相同的。
>>>>>>> 50b98123eef511e961e9c05b37b46f2a667dafb0

    另一个可能与on-policy/off-policy混淆的概念是online/offline。在线学习指智能体在与环境交互的同时更新价值函数和策略；离线学习则指智能体使用预先收集的经验数据进行策略更新，而无需与环境实时交互。若算法属于on-policy，则可在online，但不能使用其他策略生成的预存数据；若算法属于off-policy，则既可online实现也可offline实现。

 ![](../img/07/3.png)
 > 算法7.2

<<<<<<< HEAD
!!! note
    看on-policy的Q-learning的update q-value可以看到，与Sarsa的公式不同的是，Sarsa中的最后项是$q_t(s_{t+1},a_{t+1})$，而Q-learning的最后项是$\gamma \max_a q_t(s_{t+1},a)$。可以看到，on-policy版本的Q-learning是根据策略生成经验样本更新$q$值，利用$q$值更新策略，然后重复。

 ![](../img/07/4.png)
 > 算法7.3

!!! note
    这里与on-policy的不同之处在于update target policy从$\varepsilon$-贪婪策略变为了贪婪策略。

=======
 ![](../img/07/4.png)
 > 算法7.3

>>>>>>> 50b98123eef511e961e9c05b37b46f2a667dafb0
 ![](../img/07/5.png)
 > 图$7.3$：Q-Learning算法的演示示例。所有训练回合均从左上角状态开始，并在到达目标状态后终止。目标是找到从起始状态到目标状态的最优路径。奖励设置为 $r_{\text{target}} =0$、$r_{\text{forbidden}} = r_{\text{boundary}} = -10$，以及 $r_{\text{other}} = -1$。学习率 $\alpha =0.1$，$\gamma$值为0.1。左图显示算法获得的最终策略，右图展示每个训练回合的总奖励与路径长度。

### 7.4.3 算法实现

由于Q-learning属于off-policy算法，因此既可采用on-policy也可采用off-policy方式实现。

Q学习的on-policy版本如算法7.2所示。该实现方式与算法7.1中的Sarsa算法类似。其中，行为策略与目标策略相同，均为$\varepsilon$-贪婪策略。

off-policy版本如算法$7.3$所示。行为策略$\pi_b$可以是任意策略，只要其能生成足够的经验样本。当$\pi_b$具有探索性时通常更有利。此处目标策略$\pi_T$采用贪婪策略而非$\varepsilon$-贪婪策略，因其不用于生成样本，故无需具备探索性。此外，本文提出的Q-learning off-policy版本采用offline实现方式：所有经验样本均需先采集后处理。

### 7.4.4 示例

接下来我们通过示例展示Q-learning算法的应用。

第一个示例如图$7.3$所示，它展示了on-policy Q-learning的过程。该算法的目标是从初始状态找到一条通往目标状态的最优路径。具体实验设置见图$7.3$的图注说明。可以看出，Q-learning最终能够找到最优路径。在学习过程中，每个训练周期的路径长度逐渐缩短，而对应的总奖励值则持续增加。

第二组示例如图$7.4$和图$7.5$所示，展示了off-policy Q-learning过程。其目标是为所有状态寻找最优策略。奖励设置为 $r_{\text{boundary}} = r_{\text{forbidden}} = -1$，且 $r_{\text{target}} =1$。折扣率 $\gamma =0.9$，学习率 $\alpha =0.1$。

- 基准真值(ground truth)：为验证 Q-learning的有效性，首先需要获知最优策略与最优状态值的基准真值。此处通过基于模型的策略迭代算法获取基准真值，具体数据见图$7.4(a)$与$(b)$。

- 经验样本(experience samples)：行为策略采用均匀分布，即在任意状态下采取任一动作的概率均为$0.2$(图$7.4(c)$)。通过生成一个包含100,000步的单一回合(图$7.4(d)$)，由于行为策略具备良好的探索能力，该回合会多次访问所有状态-行动对。

- 学习结果(learned results)：基于行为策略生成的回合，通过Q-learning得到的最终目标策略如图$7.4(e)$所示。该策略是最优的，因为如图$7.4(f)$所示，其状态值估计误差(均方根误差)收敛至零。此外，可以注意到学习得到的最优策略与图$7.4(a)$中的策略并不完全相同。事实上，存在多个具有相同最优状态值的最优策略。

- 不同初始值的影响(different initial values)：由于Q-learning采用自举法(bootstraps)，算法性能取决于行动值的初始估计值。如图$7.4(g)$所示，当初始估计接近真实值时，约$10,000$步内即可收敛；反之则需要更多迭代步数(图$7.4(h)$)。尽管如此，这些结果表明即使初始值不精确，Q-learning仍能保持较快的收敛速度。

<<<<<<< HEAD
- 不同行为策略的表现差异(different behavior policies)：当行为策略不具备探索性时，学习性能会显著下降。例如，考虑图$7.5$所示的策略，它们是$\epsilon$-贪心策略($\epsilon=0.5$或$0.1$)；图$7.4(c)$中的均匀策略可视为$\epsilon=1$的$\epsilon$-贪心策略）。研究表明，当$\epsilon$从1降至0.5再降至0.1时，学习速度急剧下降。这是因为策略的探索能力较弱，导致经验样本不足。

 ![](../img/07/6.png)
 >图$7.4$：通过Q-learning演示off-policy学习的示例。$(a)$和$(b)$分别显示了最优策略和最优状态值。$(c)$和$(d)$分别展示了行为策略和生成的轨迹。$(e)$和$(f)$分别呈现了估计策略与误差演变过程。$(g)$和$(h)$则显示了不同初始值条件下的对比情况。

 ![](../img/07/7.png)
 >图$7.5$：当行为策略不具备探索性时，Q-learning算法的性能会下降。左列各图展示了行为策略的分布特征。中间列各图呈现了对应行为策略下生成的轨迹序列，每个示例中的轨迹均包含$100,000$步。右列各图则显示了状态值估计值的均方根误差演变过程。
=======
- 不同行为策略的表现差异(different behavior policies)：当行为策略不具备探索性时，学习性能会显著下降。例如，考虑图$7.5$所示的策略，它们是$\epsilon$-贪心策略（$\epsilon=0.5$或$0.1$；图$7.4(c)$中的均匀策略可视为$\epsilon=1$的$\epsilon$-贪心策略）。研究表明，当$\epsilon$从1降至0.5再降至0.1时，学习速度急剧下降。这是因为策略的探索能力较弱，导致经验样本不足。
>>>>>>> 50b98123eef511e961e9c05b37b46f2a667dafb0
