## 7.6 总结

<<<<<<< HEAD
本章介绍了一类重要的强化学习算法——时序差分(TD)学习。具体涵盖的算法包括 Sarsa、n步-Sarsa以及Q-learning。这些算法均可视为求解贝尔曼方程或贝尔曼最优方程的随机近似算法。

本章介绍的TD算法(除Q-learning外)均用于策略评估，即通过经验样本估计给定策略的状态值/行动值。结合策略改进机制，这些算法可用于学习最优策略。此外，这些算法均属于on-policy方法：目标策略同时作为行为策略来生成经验样本。

与TD算法相比，Q-learning具有一个特殊性质：它是一种off-policy算法。在Q-learning中，目标策略可以与行为策略不同。其根本原因在于，Q-learning旨在求解贝尔曼最优方程，而非特定策略对应的贝尔曼方程。
=======
本章介绍了一类重要的强化学习算法——时序差分(TD)学习。具体涵盖的算法包括 Sarsa、n步 Sarsa以及 Q学习。这些算法均可视为求解贝尔曼方程或贝尔曼最优方程的随机近似算法。

本章介绍的时序差分(TD)算法(除Q-learning外)均用于策略评估，即通过经验样本估计给定策略的状态值/行动值。结合策略改进机制，这些算法可用于学习最优策略。此外，这些算法均属于on-policy方法：目标策略同时作为行为策略来生成经验样本。

与其他时序差分(TD)算法相比，Q-learning具有一个特殊性质：它是一种off-policy算法。在Q-learning中，目标策略可以与行为策略不同。其根本原因在于，Q-learning旨在求解贝尔曼最优方程，而非特定策略对应的贝尔曼方程。
>>>>>>> 50b98123eef511e961e9c05b37b46f2a667dafb0

值得一提的是，存在若干方法可将on-policy算法转化为off-policy算法。重要性采样是其中广泛应用的一种[3,40]，将在第10章详述。此外，本章介绍的时序差分算法存在多种变体与扩展[41–45]，例如$TD(\lambda)$方法为时序差分学习提供了更通用统一的框架。更多信息可参阅文献[3,20,46]。