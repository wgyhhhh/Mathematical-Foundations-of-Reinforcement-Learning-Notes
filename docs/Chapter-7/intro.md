本章介绍强化学习中的时序差分（Temporal-Difference, TD）方法。与蒙特卡洛（Monte Carlo, MC）学习类似，TD学习同样属于无模型方法，但其增量式更新形式具有独特优势。通过第6章的铺垫，读者在接触TD学习算法时将不会感到陌生。事实上，TD学习算法可视为求解贝尔曼方程或贝尔曼最优方程的特殊随机算法。

由于本章介绍了多种时序差分（TD）算法，我们首先概述这些算法并厘清它们之间的关系。

- 第7.1节介绍了最基本的TD算法，该算法能够估计给定策略的状态值。在研究其他时序差分算法之前，首先理解这一基础算法至关重要。

- 7.2节介绍了Sarsa算法，该算法能够估计给定策略的行动值。通过结合策略改进步骤，该算法可用于求解最优策略。只需将7.1节TD算法中的状态价值估计替换为行动值估计，即可直接导出Sarsa算法。

- 7.3节将介绍 n步 Sarsa算法，该算法是 Sarsa算法的广义形式。研究表明，Sarsa与蒙特卡洛（MC）学习均为 n步 Sarsa的两个特例。

- 7.4节介绍了 Q-learning算法，这是最经典的强化学习算法之一。其他 TD算法的目标是求解给定策略的 Bellman方程，而 Q-learning则旨在直接求解 Bellman最优性方程以获得最优策略。

- 7.5节比较了本章介绍的 TD算法，并提供了统一的观点。