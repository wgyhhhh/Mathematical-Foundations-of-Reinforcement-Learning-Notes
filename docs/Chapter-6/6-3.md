## 6.3 Dvoretzky定理

到目前为止，RM算法的收敛性尚未得到理论证明。为此，我们接下来介绍Dvoretzky 定理[31, 32]，它是随机逼近领域的一个经典结果。该定理可用于分析 RM 算法和许多强化学习算法的收敛性。

本节的数学内容稍显密集。建议对随机算法收敛性分析感兴趣的读者学习这一部分。否则，可以跳过本节。

!!! note 定理
    **定理6.2**. (Dvoretzky 定理). 考虑一个随机过程 

    $$\Delta_{k+1}=(1-\alpha_k)\Delta_k+\beta_k\eta_k,$$

    其中$\{\alpha_k\}_{k=1}^\infty$, $\{\beta_k\}_{k=1}^\infty$, $\{\eta_k\}_{k=1}^\infty$是随机序列。对于所有$k$有$\alpha_k\geq 0,\beta_k\geq 0$。那么，如果满足以下条件，$\Delta_k$几乎必然收敛为零

    (a) $\sum_{k=1}^{\infty}\alpha_{k}=\infty,\sum_{k=1}^{\infty}\alpha_{k}^{2}<\infty,$并且$\sum_{k=1}^\infty\beta_k^2<\infty$几乎必然一致的;

    (b) $\mathbb{E}[\eta_k|\mathcal{H}_k]=0$与$\mathbb{E}[\eta_{k}^{2}|\mathcal{H}_{k}]\leq C$是几乎必然的;

    在这里$\mathcal{H}_k=\{\Delta_k,\Delta_{k-1},...,\eta_{k-1},...,\alpha_{k-1},...,\beta_{k-1},...\}$。

    在介绍该定理的证明之前，我们首先要澄清一些问题。

    - 在 RM 算法中，系数序列$\{\alpha_k\}$是确定的。然而，Dvoretzky定理允许$\{\alpha_k\},\{\beta_k\}$成为取决于$\mathcal{H}_k$的随机变量。因此，在$\alpha_k$或$\beta_k$是$\Delta_k$的函数的情况下，该定理更为有用。

    - 第一个条件是 "几乎必然一致"。这是因为$\alpha_k$和$\beta_k$可能是随机变量，因此它们的极限定义必须在随机情况下。第二个条件也表述为"几乎必然"。这是因为$\mathcal{H}k$是一个随机变量序列，而不是具体的值。因此，$\mathbb{E}[\eta_{k}|\mathcal{H}_{k}]$和$\mathbb{E}[\eta_{k}^{2}|\mathcal{H}_{k}]$都是随机变量。在这种情况下，条件期望的定义是以“几乎必然”的意义给出的(附录 $B$)。

    - 定理$6.2$的陈述与[32]略有不同，因为定理$6.2$的第一个条件中并不要求$\sum_{k=1}^{\infty}\beta_{k}=\infty$。当$\sum_{k=1}^{\infty}\beta_{k}<\infty$时，特别是当$\beta_k = 0$对所有$k$成立时，该序列仍然可以收敛。
  

### 6.3.1 Dvoretzky的证明

见Box$6.3.1$。

