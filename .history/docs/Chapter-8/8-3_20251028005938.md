---
title: 8.3 基于值函数的时序差分算法:行动值估计
comments: true  # 开启评论
---
8.2节介绍了状态值估计问题，本节将阐述如何估计行动值。表格型Sarsa算法和表格型Q-learning算法将被推广至值函数近似的情形。读者将会发现，这种推广是直接了当的。

### 8.3.1基于函数近似的Sarsa算法

通过将状态值替换为行动值，可以很容易地从式$(8.13)$推导出函数逼近的Sarsa算法。具体而言，假设 $q_\pi(s, a)$由 $\hat{q}(s, a, \mathbf{w})$近似表示，将式（8.13）中的 $\hat{v}(s, \mathbf{w})$替换为 $\hat{q}(s, a, \mathbf{w})$即可得到

$$w_{t+1}=w_{t}+\alpha_{t}\left[r_{t+1}+\gamma\hat{q}(s_{t+1},a_{t+1},w_{t})-\hat{q}(s_{t},a_{t},w_{t})\right]\nabla_{w}\hat{q}(s_{t},a_{t},w_{t}).\tag{8.35}$$

$(8.35)$式的分析与$(8.13)$式类似，此处从略。当采用线性函数时，可得

$$\hat{q}(s,a,w)=\phi^T(s,a)w,$$

其中$\phi(s, a)$为特征向量。此时$\nabla_w \hat{q}(s, a, w) = \phi(s, a)$。

式$(8.35)$中的价值估计步骤可与策略改进步骤相结合以学习最优策略，该过程总结于算法$8.2$中。需注意的是，要准确估计给定策略的动作价值需要充分多次运行式$(8.35)$。然而在实际操作中，仅在切换至策略改进步骤前执行一次式$(8.35)$，这与表格型Sarsa算法类似。此外，算法$8.2$的实现旨在解决从预设起始状态到目标状态的路径寻优问题，因此无法为所有状态找到最优策略。但若具备充足的实验数据，该实现过程可轻松调整为求解每个状态的最优策略。

图$8.9$展示了一个示例性案例。该任务要求智能体从左上角状态出发，找到一条能抵达目标位置的最优策略。随着训练进行，每轮次的总奖励值与路径长度均逐渐收敛至稳定值。本案例中，线性特征向量选用5阶傅里叶函数，其数学表达式如式$(8.18)$所示。

 ![](../img/08/8.png)

 > 图$8.9$：采用线性函数逼近的Sarsa算法。参数设置为：$\gamma=0.9$，$\varepsilon=0.1$，边界奖励 \( r_{\text{boundary}} \) =禁区奖励 \( r_{\text{forbidden}} \) = −10，目标奖励 \( r_{\text{target}} \) =1，学习率$\alpha=0.001$。

  ![](../img/08/9.png)

 > 算法8.2: 基于函数近似的Sarsa

### 8.3.2 基于函数近似的Q-learning


表格型Q学习也可以推广到函数近似的情况。其更新规则为

$$w_{t+1}=w_{t}+\alpha_{t}\left[r_{t+1}+\gamma\max_{a\in\mathcal{A}(s_{t+1})}\hat{q}(s_{t+1},a,w_{t})-\hat{q}(s_{t},a_{t},w_{t})\right]\nabla_{w}\hat{q}(s_{t},a_{t},w_{t}).\tag{8.36}$$

上述更新规则与式$(8.35)$类似，不同之处在于将式$(8.35)$中的$\hat{q}(s_{t+1}, a_{t+1}, w_t)$替换为$\max_{a \in \mathcal{A}(s_{t+1})} \hat{q}(s_{t+1}, a, w_t)$。

与表格形式类似，$(8.36)$既可采用同策略(on-policy)方式实现，也可采用异策略(off-policy)方式实现。算法$8.3$给出了同策略版本的实现。图$8.10$展示了演示同策略版本的实例，该任务要求寻找能使智能体从左上角状态抵达目标状态的最优策略。

 ![](../img/08/10.png)

 > 算法$8.3$：基于函数近似的Q-learning(同策略版本)

  ![](../img/08/11.png)

 > 图$8.10$：采用线性函数逼近的Q学习算法。参数设置为：$\gamma =0.9$，$\epsilon =0.1$，$r_{\text{boundary}} = r_{\text{forbidden}} = -10$，$r_{\text{target}} =1$，且学习率$\alpha =0.001$。


读者可能注意到，在算法$8.2$和算法$8.3$中，虽然价值函数以函数形式表示，但策略6$\pi(a|s)$仍以表格形式呈现。这意味着该方法仍假设状态和动作空间是有限的。在第$9$章中我们将看到，策略同样可以表示为函数形式，从而能够处理连续的状态和动作空间。

---