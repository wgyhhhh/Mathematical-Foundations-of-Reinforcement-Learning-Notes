---
title: 8.4 深度Q-learning
comments: true  # 开启评论
---
<<<<<<< HEAD
我们可以将深度神经网络整合到Q-learning中，从而得到一种称为深度Q-learning或深度Q网络(DQN)的方法[22,60,61]。深度Q-learning是最早且最成功的深度强化学习算法之一。值得注意的是，神经网络并不必须具有深度结构。对于简单任务(例如我们的网格世界示例)，仅含一两个隐藏层的浅层网络可能已足够。

深度Q-learning可视为式$(8.36)$算法的扩展，但其数学表述与实现技术存在显著差异，需要特别关注。

### 8.4.1 算法描述

从数学角度而言，深度Q-learning旨在最小化以下目标函数：
=======
我们可以将深度神经网络整合到Q学习中，从而得到一种称为深度Q学习或深度Q网络（DQN）的方法[22,60,61]。深度Q学习是最早且最成功的深度强化学习算法之一。值得注意的是，神经网络并不必须具有深度结构。对于简单任务(例如我们的网格世界示例)，仅含一两个隐藏层的浅层网络可能已足够。

深度Q学习可视为式(8.36)算法的扩展，但其数学表述与实现技术存在显著差异，需要特别关注。

### 8.4.1 算法描述

从数学角度而言，深度Q学习旨在最小化以下目标函数：
>>>>>>> 50b98123eef511e961e9c05b37b46f2a667dafb0

$$J=\mathbb{E}\left[\left(R+\gamma\max_{a\in\mathcal{A}(S^{\prime})}\hat{q}(S^{\prime},a,w)-\hat{q}(S,A,w)\right)^2\right],\tag{8.37}$$

其中 $(S, A, R, S')$为随机变量，分别表示状态、动作、即时奖励和下一状态。该目标函数可视为贝尔曼最优性误差的平方项，这是因为

$$q(s,a)=\mathbb{E}\left[R_{t+1}+\gamma\max_{a\in\mathcal{A}(S_{t+1})}q(S_{t+1},a)|S_t=s,A_t=a\right],\quad\mathrm{for\;all}\;s,a$$

这正是贝尔曼最优方程(证明过程见Box 7.5。因此，当 $\hat{q}(S, A,w)$能准确近似最优行动值时，期望意义上应有 $R + \gamma \max_{a\in \mathcal{A}(S')} \hat{q}(S', a, w) - \hat{q}(S, A,w)$等于零。

为最小化式$(8.37)$中的目标函数，可采用梯度下降算法。为此，需计算$J$关于$w$的梯度。值得注意的是，参数$w$不仅出现在$\hat{q}(S, A, w)$中，还隐含于$y \doteq R + \gamma \max_{a \in \mathcal{A}(S')} \hat{q}(S', a, w)$的表达式内，这使得梯度计算具有非平凡性。为简化分析，假设$y$中的$w$值在短时间内保持固定，从而显著降低梯度计算复杂度。具体实现时，我们引入两个网络：一个主网络表示$\hat{q}(s, a, w)$，另一个目标网络表示$\hat{q}(s, a, w_T)$。此时目标函数转化为

<<<<<<< HEAD
!!! note
    为什么可以假设$y$中的$w$值在短时间内保持固定

=======
>>>>>>> 50b98123eef511e961e9c05b37b46f2a667dafb0
$$J=\mathbb{E}\left[\left(R+\gamma\max_{a\in\mathcal{A}(S^{\prime})}\hat{q}(S^{\prime},a,w_T)-\hat{q}(S,A,w)\right)^2\right],$$

其中$w_T$为目标网络的参数。当$w_T$固定时，目标函数$J$的梯度为

$$\nabla_wJ=-\mathbb{E}\left[\left(R+\gamma\max_{a\in\mathcal{A}(S^{\prime})}\hat{q}(S^{\prime},a,w_T)-\hat{q}(S,A,w)\right)\nabla_w\hat{q}(S,A,w)\right],\tag{8.38}$$

在不失一般性的前提下，此处省略了某些常系数。

为利用式(8.38)中的梯度最小化目标函数，需注意以下技术要点。

<<<<<<< HEAD
- 第一种技术是使用两个网络——主网络(main network)和目标网络(target network)，我们在计算式$(8.38)$梯度时所提及。具体实现细节如下：令$w$和$w_T$分别表示主网络与目标网络的参数，二者的初始值设定为相同数值。

    在每次迭代中，我们从经验回放池(Experience replay)中抽取一个小批量样本集 $\{(s, a, r, s')\}$。主网络的输入为状态$s$和行动$a$，其输出 $y = \hat{q}(s, a, w)$表示估计的Q值。目标输出值定义为 $y_T \doteq r + \gamma \max_{a \in \mathcal{A}(s')} \hat{q}(s', a, w_T)$。主网络通过在小批量$\{(s, a, y_T)\}$最小化TD误差$\mathbb{E}[(y - y_T)^2]$进行更新。  
=======
- 第一种技术是使用两个网络——主网络和目标网络，如我们在计算式$(8.38)$梯度时所提及。具体实现细节如下：令$w$和$w_T$分别表示主网络与目标网络的参数，二者的初始值设定为相同数值。

    在每次迭代中，我们从经验回放池（稍后将解释其机制）中抽取一个小批量样本集 $\{(s, a, r, s')\}$。主网络的输入为状态$s$和行动$a$，其输出 $y = \hat{q}(s, a, w)$表示估计的Q值。目标输出值定义为 $y_T \doteq r + \gamma \max_{a \in \mathcal{A}(s')} \hat{q}(s', a, w_T)$。主网络通过最小化TD误差（亦称损失函数）$\mathbb{E}[(y - y_T)^2]$进行更新，优化基于样本集 $\{(s, a, y_T)\}$。  
>>>>>>> 50b98123eef511e961e9c05b37b46f2a667dafb0

    主网络中的权重$w$更新并不显式使用式$(8.38)$中的梯度，而是依赖现有的神经网络训练软件工具。因此，我们需要通过小批量样本训练网络，而非基于式$(8.38)$使用单一样本更新主网络。这是深度与非深度强化学习算法之间的显著差异之一。

    主网络在每次迭代中更新。相比之下，目标网络每隔固定次数的迭代会被设置为与主网络相同，以满足计算式$(8.38)$梯度时$w_T$保持不变的假设。

<<<<<<< HEAD
- 第二种技术是经验回放(experience replay)[22,60,62]。具体而言，当我们收集到若干经验样本后，并不按其采集顺序直接使用，而是将其存储在称为回放缓冲区(replay buffer)的数据集$\mathcal{B} := \{(s, a, r, s')\}$中。每次更新主网络时，可从回放缓冲区中均匀采样一个小批量经验样本，该采样过程即称为经验回放。

    深度Q-learning中为何需要经验回放？为何回放必须遵循均匀分布？答案在于式$(8.37)$中的目标函数。具体而言，为了明确定义该目标函数，我们必须指定状态$S$、动作$A$、奖励$R$及次态$S'$的概率分布。当$(S,A)$给定时，$R$与$S'$的分布由系统模型决定。描述状态-行动对$(S,A)$分布的最简方式，就是假设其服从均匀分布。

    然而，由于状态-行动样本是根据行为策略生成的样本序列，实际中它们可能并非均匀分布。为了满足均匀分布假设，必须打破序列中样本间的相关性。为此，可采用经验回放技术，通过从回放缓冲区中均匀抽取样本实现。这正是经验回放的必要性及其必须遵循均匀分布的数学依据。随机采样的优势在于每个经验样本可多次重复使用，从而提高数据利用效率。这一特性在数据量有限时尤为重要。


!!! note
    $R\sim p(R|S,A),S^{\prime}\sim p(S^{\prime}|S,A)$。其他的分布比如高斯分布行不行？我想给一些$(S,A)$更大的权重，那么在采样的时候当然也要更多次的去运用到他们，它的一个问题就是你要有先验知识，要知道谁是重要的，谁是不重要的。如果没有这个先验知识，就要一视同仁，所有的$(S,A)$的概率都应该是相同的，这时候就是均匀分布

=======
- 第二种技术是经验回放(experience replay)[22,60,62]。具体而言，当我们收集到若干经验样本后，并不按其采集顺序直接使用，而是将其存储在称为回放缓冲区的数据集 $\mathcal{B} := \{(s, a, r, s')\}$中。每次更新主网络时，可从回放缓冲区中均匀采样一个小批量经验样本，该采样过程即称为经验回放。

    深度Q-learning中为何需要经验回放？为何回放必须遵循均匀分布？答案在于式(8.37)中的目标函数。具体而言，为了明确定义该目标函数，我们必须指定状态$S$、动作$A$、奖励$R$及次态$S'$的概率分布。当$(S,A)$给定时，$R$与$S'$的分布由系统模型决定。描述状态-动作对$(S,A)$分布的最简方式，就是假设其服从均匀分布。

    然而，由于状态-动作样本是根据行为策略生成的样本序列，实际中它们可能并非均匀分布。为了满足均匀分布假设，必须打破序列中样本间的相关性。为此，可采用经验回放技术，通过从回放缓冲区中均匀抽取样本实现。这正是经验回放的必要性及其必须遵循均匀分布的数学依据。随机采样的优势在于每个经验样本可多次重复使用，从而提高数据利用效率。这一特性在数据量有限时尤为重要。
>>>>>>> 50b98123eef511e961e9c05b37b46f2a667dafb0

深度Q学习的实现流程总结于算法8.3中。该实现采用**异策略**方法，必要时也可调整为**同策略**形式。

### 8.4.2 例子

图$8.11$给出了一个演示算法$8.3$的实例。该示例旨在学习每个状态-行动对的最优行动值，一旦获得最优行动值，即可立即推导出最优贪婪策略。

单次训练回合由如图$8.11(a)$所示的行为策略生成。该行为策略具有探索性，表现为在任何状态下采取任意动作的概率均等。如图$8.11(b)$所示，该回合仅包含$1,000$个时间步。尽管步数有限，但由于行为策略强大的探索能力，该回合几乎覆盖了所有状态-动作对。经验回放缓冲区存储了$1,000$个经验样本，其中小批量规模设为$100$，意味着每次采样时需从缓冲区均匀抽取$100$个样本。

主网络与目标网络具有相同结构：一个包含100个神经元单隐藏层的神经网络（层数和神经元数量可调）。该神经网络具有三个输入和一个输出，前两个输入为状态的行、列归一化索引，第三个输入为动作索引的归一化值。此处的“归一化”指将数值转换至区间$[0,1]$。网络输出为估计值。

我们将输入设计为状态的行列坐标而非状态索引，是因为已知状态对应网格中的二维位置。在网络设计中使用的状态信息越丰富，网络性能就越好。此外，神经网络结构亦可采用其他设计方式。例如，可构建双输入五输出架构：两个输入为状态归一化的行坐标与列坐标，输出则对应输入状态下五个动作的估计价值[22]。

如图8.11(d)所示，定义为每个小批量平均时序差分误差平方的损失函数收敛至零，表明网络能够良好拟合训练样本。图8.11(e)显示，状态值估计误差同样收敛至零，这意味着最优动作值估计已达到足够精度。此时，对应的贪婪策略即为最优策略。

本示例证明了深度Q学习的高效性。具体而言，仅需1,000步的短训练周期即可在此获得最优策略；相比之下，如图7.4所示，表格型Q学习需要100,000步的训练周期。高效性原因之一在于函数逼近方法具有强大的泛化能力，另一原因在于经验样本可被重复利用。

接下来，我们通过设计一个经验样本较少的场景，刻意对深度Q学习算法进行挑战。图8.12展示了一个仅包含100步的回合示例。在此例中，虽然从某种意义上当损失函数收敛至零，状态估计误差仍无法收敛到零时，网络仍能得到良好训练。这表明网络能够正确拟合给定的经验样本，但经验样本数量不足，难以准确估计最优行动值。

 ![](../img/08/12.png)

 > 图$8.11$：基于深度Q-learning的最优策略学习。其中 $\gamma =0.9$，$r_{\text{boundary}} = r_{\text{forbidden}} = -10$，且 $r_{\text{target}} =1$。批处理规模为100。

  ![](../img/08/12.png)

 > 图$8.12$：通过深度Q-learning实现的最优策略学习。其中，$\gamma =0.9$，$r_{\text{boundary}} = r_{\text{forbidden}} = -10$，且 $r_{\text{target}} =1$。批处理大小为50。
---