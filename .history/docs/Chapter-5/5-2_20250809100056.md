## 5.2- 最简单的基于蒙特卡洛的算法

本节介绍第一个也是最简单的基于蒙特卡洛(MC)的强化学习算法。该算法通过用无模型的MC估计步骤替换第$4.2$节中介绍的基于模型的策略迭代算法中的模型策略评估步骤而得到。

### 5.2.1 将策略迭代转化为无模型方法

策略迭代算法的每一步迭代包含两个步骤(见第$4.2$节)。第一步是策略评估，其目的是通过求解方程$v_{\pi_k} = r_{\pi_k} + \gamma P_{\pi_k}v_{\pi_k}$来计算$v_{\pi_k}$。第二步是策略改进，其目的是计算Greedy策略$\pi_{k+1} = \arg\max_{\pi} \left( r_{\pi} + \gamma P_{\pi} v_{\pi k} \right).$。策略改进步骤的逐元素形式为

$$\begin{aligned}\pi_{k+1}(s) &= \arg\max_{\pi} \sum_a \pi(a|s) \left[ \sum_r p(r|s,a)r + \gamma \sum_{s'} p(s'|s,a)v_{\pi k}(s') \right] \\&= \arg\max_{\pi} \sum_a \pi(a|s)q_{\pi k}(s,a), \quad s \in S.\end{aligned}$$

必须指出的是，行动值位于这两个步骤的核心。具体而言，在第一步中，状态值被计算出来，以便于计算行动值。在第二步中，基于计算出的动作值生成新的策略。让我们重新考虑如何计算行动值。有两种方法可供选择。

- 第一种方法是基于模型的方法。这种方法被策略迭代算法所采用。具体来说，我们首先可以通过求解贝尔曼方程来计算状态值$v_{\pi_k}$。然后，我们可以通过使用以下公式来计算动作值：

    $$q_{\pi k}(s,a) = \sum_r p(r|s,a)r + \gamma \sum_{s'} p(s'|s,a)v_{\pi k}(s').\tag{5.1}$$
    
    这种方法要求已知系统模型$\{p(r|s,a), p(s^\prime|s,a)\}$。

- 第二种方法是无模型方法。回想一下，动作值的定义是
    
    $$\begin{aligned}q_{\pi_k}(s,a) &= \mathbb{E}[G | S_t = s, A_t = a]\\&= \mathbb{E}[R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \cdots | S_t = s, A_t = a],\end{aligned}$$
