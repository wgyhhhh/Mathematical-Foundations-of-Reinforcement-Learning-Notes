## 5.3 MC Exploring Starts算法

接下来，我们将MC Basic算法扩展，得到另一种基于蒙特卡洛(MC)的强化学习算法，该算法稍显复杂，但样本效率更高。

### 5.3.1 更有效地利用样本

基于蒙特卡洛(MC)的强化学习的一个重要方面是如何更有效地利用样本。具体来说，假设我们通过遵循策略$\pi$获得了一组样本序列：

$$s_1\xrightarrow{a_2}s_2\xrightarrow{a_4}s_1\xrightarrow{a_2}s_2\xrightarrow{a_3}s_5\xrightarrow{a_1}\ldots\tag{5.3}$$

其中下标指的是状态或行动的索引，而非时间步。每当一个状态-行动对在一次经历中出现时，就称为对该状态-动作对的一次访问。可以采用不同的策略来利用这些访问。

第一个也是最简单的策略是使用首次访问(initial visit)。也就是说，一个回合仅用于估计该回合开始时的初始状态-动作对的行动值。对于$(5.3)$中的例子，首次访问策略仅估计状态-行动对$(s1, a2)$的行动值。MC Basic算法就采用了首次访问策略。然而，这种策略并不是样本高效的，因为该回合还访问了许多其他状态-行动对，例如$(s_2,a_4),(s_2,a_3),(s_5,a_1)$。这些访问也可以用来估计相应的行动值。特别是，我们可以将$(5.3)$中的回合分解为多个子回合：

$$\begin{aligned}s_{1}\xrightarrow{a_{2}}s_{2}\xrightarrow{a_{4}}s_{1}\xrightarrow{a_{2}}s_{2}\xrightarrow{a_{3}}s_{5}\xrightarrow{a_{1}}\ldots&\begin{aligned}[\text{original episode}]\end{aligned}[\text{original episode}]\\s_{2}\xrightarrow{a_{4}}s_{1}\xrightarrow{a_{2}}s_{2}\xrightarrow{a_{3}}s_{5}\xrightarrow{a_{1}}\ldots&[\text{subepisode starting from }(s_2,a_4)]\\s_{1}\xrightarrow{a_{2}}s_{2}\xrightarrow{a_{3}}s_{5}\xrightarrow{a_{1}}\ldots&[\text{subepisode starting from }(s_1,a_2)]\begin{aligned}[\text{subepisode starting from }(s_1,a_2)]\end{aligned}\\s_{2}\xrightarrow{a_{3}}s_{5}\xrightarrow{a_{1}}\ldots&[\text{subepisode starting from }(s_2,a_3)]\\s_{5}\xrightarrow{a_{1}}\ldots&[\text{subepisode starting from }(s_5,a_1)]\end{aligned}$$