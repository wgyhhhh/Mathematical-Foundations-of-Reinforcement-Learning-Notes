## 5.4- MC $\varepsilon$-Greedy算法

接下来，我们通过移除探索开始(Exploring Starts)条件来扩展MC探索开始算法。这一条件实际上要求每个状态-行动对都能被访问足够多次，这也可以通过软策略来实现。

### 5.4.1 $\varepsilon$-greedy策略

如果一个策略在任何状态下采取任何动作的概率都为正，则该策略是软策略。考虑一种极端情况，即我们只有一个回合。在这种情况下，如果采用软策略，一个足够长的回合可以多次访问每个状态-动作对(见图$5.8$中的例子)。因此，我们不需要从不同的状态-动作对开始生成大量回合，这时便可以移除探索开始的要求。

一种常见的软策略是$\varepsilon$-Greedy策略。$\varepsilon$-Greedy策略是一种随机策略，它选择Greedy动作的概率较高，而选择任何其他动作的概率则相同且不为零。这里的Greedy动作是指具有最大动作值的动作。具体来说，假设$\varepsilon\in[0,1]$，相应的$\varepsilon$-Greedy策略具有以下形式：