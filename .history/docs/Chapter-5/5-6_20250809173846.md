本章中的算法是本书中首次介绍的无模型(model-free)强化学习算法。我们首先通过考察一个重要的均值估计问题，引入了蒙特卡洛(MC)估计的思想，然后介绍了三种基于蒙特卡洛的算法。

- MC Basic算法：这是最简单的基于蒙特卡洛的强化学习算法。该算法通过用基于蒙特卡洛的无模型估计组件替换策略迭代算法中的基于模型的策略评估步骤而得到。在有足够样本的情况下，该算法可以保证收敛到最优策略和最优状态值。

- MC Exploring Starts算法：该算法是MC基本算法的一种变体。它可以通过使用首次访问或每次访问策略，从MC基本算法中获得，从而更有效地利用样本。

- MC $\varepsilon$-Greedy算法：该算法是 MC Exploring Starts的变体。具体而言，在策略改进步骤中，它搜索的是最优的$\varepsilon$-Greedy策略，而非Greedy策略。通过这种方式，策略的探索能力得到增强，从而可以去除探索性起始的条件限制。

最后，通过考察$\varepsilon$-Greedy策略的特性，引入了探索与利用之间的权衡。随着$\varepsilon$值的增加，$\varepsilon$-Greedy策略的探索能力增强，而Greedy动作的利用能力则减弱。另一方面，如果$\varepsilon$值减小，则可以更好地利用Greedy动作，但探索能力会受到损害。