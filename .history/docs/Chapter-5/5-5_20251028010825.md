---
title: 5.5 探索与利用:以Greedy策略为例
comments: true  # 开启评论
---
探索(exploration)与利用(exploitation)是强化学习中的一个重要权衡。“探索”意味着策略会尝试尽可能多的动作，从而使得所有行动都能被良好地评估；“利用”意味着策略会尽可能选取价值高的动作，从而充分利用当前价值评估的结果。从另一个角度来理解，“探索”是为了更好地评估，防止错过高价值的动作；“利用”是为了更好地利用评估的结果，否则难以得到最优的策略；这里“权衡”的核心在于：当前的价值评估可能是不好的，我们要通过探索更好地评估价值。如果策略具有过多的探索性，则没有很好地利用评估结果，因此也离最优策略比较远；如果策略过多地利用当前的评估结果，则具有较少的探索性，难以全面评估所有动作。

下面我们以具体的例子来讲，$\varepsilon$-Greedy策略提供了一种平衡探索与利用的方法。一方面，$\varepsilon$-Greedy策略有更高的概率采取价值最大的行动，从而能够“充分”利用估计的值；另一方面，$\varepsilon$-Greedy策略也有机会采取其他动作，从而充分“探索”其他动作。$\varepsilon$越大，策略的探索性越强，利用性越弱；$\varepsilon$越小，策略的探索性越弱，利用性越强。此外，“利用”与最优性有关，如果策略“利用”得越充分，它就越接近最优，这是因为我们已经知道在MC Basic算法中贪婪策略是最优的。$\varepsilon$-Greedy策略的基本思想是通过牺牲最优性/利用性来增强探索性。

我们将基于一些有趣的例子来讨论这种权衡。这里的强化学习任务是一个$5\times 5$的网格世界。奖励设置为：$r_\text{boundary} = −1$，$r_\text{forbidden} = −10$，$r_\text{target} = 1$。折扣因子为$\gamma= 0.9$。

$\varepsilon$-Greedy策略不仅用于基于蒙特卡洛(MC)的强化学习，也用于其他强化学习算法中，例如第$7$章介绍的时序差分学习。

### $\varepsilon$-Greedy策略的最优性

接下来展示当$\varepsilon$变化时$\varepsilon$-Greedy策略的最优性也会发生变化。

- 首先，在图$5.6$中展示了一系列$\varepsilon$-Greedy的策略，这些策略有一个共同点。它们在每一个状态赋予最大概率的行动是相同的，这样的策略被称为是一致(consistent)的。

    从$(a)\sim(d)$图可以看出，随着$\varepsilon$值的增加，这些$\varepsilon$-Greedy策略的状态值不断下降，这表明这些$\varepsilon$-Greedy策略的最优性在不断变差。值得注意的是，当$\varepsilon$值达到$0.5$时，目标状态的值会变得最小。这是因为，当$\varepsilon$值较大时，从目标区域出发的智能体可能会进入周围的禁区，从而更有可能获得负奖励。

- 其次，图$5.7$展示了一系列最优的$\varepsilon$-Greedy策略(它们在$\Pi_\varepsilon$中是最优的)。当$\varepsilon=0$时，该最优策略是所有策略$\Pi$中最优的。当$\varepsilon$小到$0.1$时，最优的$\varepsilon$-Greedy策略与最优的Greedy策略一致(即最大概率行动相同)。然而，当$\varepsilon$增加到例如$0.2$时，所得到的$\varepsilon$-Greedy策略与最优的Greedy策略不再一致。因此，如果我们想要得到与最优Greedy策略一致的$\varepsilon$-Greedy策略，$\varepsilon$的值应足够小。

    当$\varepsilon$较大时，为什么$\varepsilon$-Greedy策略与最优Greedy策略不一致呢？我们可以以目标状态为例来直观地解释。目标状态下的最优策略是保持不动以获得正奖励。然而，当$\varepsilon$较大时，进入禁区并获得负奖励的概率很高。此时，在目标状态下的最优策略是逃离，而不是保持不动。

!!! note
    在实际应用中，我希望$\varepsilon$-Greedy策略与Greedy策略是一致的，$\varepsilon$过大会导致策略不是最优的。

 ![](../img/05/6.png)
 > 图$5.6$：一些$\varepsilon$-Greedy策略及其状态值。这些$\varepsilon$-Greedy策略在每一个状态赋予最大概率的行动是一致的。从状态值可以看出，当$\varepsilon$的值增大时，$\varepsilon$-Greedy策略的状态值会降低，最优性变差。

 ![](../img/05/7.png)
 > 图$5.7$：给定不同的$\varepsilon$值，最优的$\varepsilon$-Greedy策略及其对应的状态值。可以看出，当$\varepsilon$值增大时，最优$\varepsilon$-Greedy策略不再与$(a)$中的最优策略一致。

### $\varepsilon$-Greedy策略的探索能力

接下来，我们说明当$\varepsilon$较大时，$\varepsilon$-Greedy策略的探索能力较强。

- 首先，考虑一个$\varepsilon=1$的$\varepsilon$-Greedy策略(见图$5.5(a)$)。该策略在任意状态下采取任意动作的概率为$0.2$，因此具有较强的探索能力。从$(s_1,a_1)$开始，由$\varepsilon$-策略生成的不同长度的轨迹如图$5.8(a)-(c)$所示。可以看出，当回合足够长时，这回合可以访问所有状态-行动对，具有很强的探索能力。此外，所有状态-行动对被访问的次数也接近均匀(图$5.8(d)$)。

- 其次，考虑一个$\varepsilon=0.5$的$\varepsilon$-Greedy策略(见图$5.6(d)$)。在这种情况下，$\varepsilon$-Greedy策略的探索能力比$\varepsilon = 1$时弱。从$(s_1, a_1)$开始，由$\varepsilon$-策略生成的不同步数的轨迹如图$5.8(e)-(g)$所示。可以看到，当轨迹比较短时，很多状态-行动并没有被访问到，虽然当轨迹足够长时每一个行动都可以被访问到，但访问次数的分布可能极不均匀。例如，当轨迹有一百万步时，有些动作访问超过$250000$次，而大多数动作的访问次数仅为数百次甚至数十次，如图$5.8(h)$所示。

以上示例表明，当$\varepsilon$减小时，$\varepsilon$-Greedy策略的探索能力会下降。使用$\varepsilon$-Greedy策略的一个常见技巧是 ，初期选择较大的$\varepsilon$从而获得较强的探索性，末期选择较小的$\varepsilon$获得较好的最优性[[21](https://arxiv.org/abs/1910.13701),[22](https://www.nature.com/articles/nature14236),[23](https://arxiv.org/abs/2006.01782v1)]。

 ![](../img/05/8.png)
 > 图$5.8$：不同$\varepsilon$值的$\varepsilon$-Greedy策略的探索能力。
 
---