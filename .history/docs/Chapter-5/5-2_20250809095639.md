## 5.2- 最简单的基于蒙特卡洛的算法

本节介绍第一个也是最简单的基于蒙特卡洛(MC)的强化学习算法。该算法通过用无模型的MC估计步骤替换第$4.2$节中介绍的基于模型的策略迭代算法中的模型策略评估步骤而得到。

### 5.2.1 将策略迭代转化为无模型方法

策略迭代算法的每一步迭代包含两个步骤(见第$4.2$节)。第一步是策略评估，其目的是通过求解方程$v_{\pi_k} = r_{\pi_k} + \gamma P_{\pi_k}v_{\pi_k}$来计算$v_{\pi_k}$。第二步是策略改进，其目的是计算Greedy策略$\pi_{k+1} = \arg\max_{\pi} \left( r_{\pi} + \gamma P_{\pi} v_{\pi k} \right).$。策略改进步骤的逐元素形式为

$$\begin{aligned}\pi_{k+1}(s) &= \arg\max_{\pi} \sum_a \pi(a|s) \left[ \sum_r p(r|s,a)r + \gamma \sum_{s'} p(s'|s,a)v_{\pi k}(s') \right] \\&= \arg\max_{\pi} \sum_a \pi(a|s)q_{\pi k}(s,a), \quad s \in S.\end{aligned}$$