## 5.2- 最简单的基于蒙特卡洛的算法

本节介绍第一个也是最简单的基于蒙特卡洛(MC)的强化学习算法。该算法通过用无模型的MC估计步骤替换第$4.2$节中介绍的基于模型的策略迭代算法中的模型策略评估步骤而得到。

### 5.2.1 将策略迭代转化为无模型方法

策略迭代算法的每一步迭代包含两个步骤(见第$4.2$节)。第一步是策略评估，其目的是通过求解方程$v_{\pi_k} = r_{\pi_k} + \gamma P_{\pi_k}v_{\pi_k}$来计算$v_{\pi_k}$。第二步是策略改进，其目的是计算Greedy策略$\pi_{k+1} = \arg\max_{\pi} \left( r_{\pi} + \gamma P_{\pi} v_{\pi k} \right).$。策略改进步骤的逐元素形式为

$$\begin{aligned}\pi_{k+1}(s) &= \arg\max_{\pi} \sum_a \pi(a|s) \left[ \sum_r p(r|s,a)r + \gamma \sum_{s'} p(s'|s,a)v_{\pi k}(s') \right] \\&= \arg\max_{\pi} \sum_a \pi(a|s)q_{\pi k}(s,a), \quad s \in S.\end{aligned}$$

必须指出的是，行动值位于这两个步骤的核心。具体而言，在第一步中，状态值被计算出来，以便于计算行动值。在第二步中，基于计算出的动作值生成新的策略。让我们重新考虑如何计算行动值。有两种方法可供选择。

- 第一种方法是基于模型的方法。这种方法被策略迭代算法所采用。具体来说，我们首先可以通过求解贝尔曼方程来计算状态值$v_{\pi_k}$。然后，我们可以通过使用以下公式来计算动作值：

    $$q_{\pi k}(s,a) = \sum_r p(r|s,a)r + \gamma \sum_{s'} p(s'|s,a)v_{\pi k}(s').\tag{5.1}$$
    
    这种方法要求已知系统模型$\{p(r|s,a), p(s^\prime|s,a)\}$。

- 第二种方法是无模型方法。回想一下，动作值的定义是
    
    $$\begin{aligned}q_{\pi_k}(s,a) &= \mathbb{E}[G | S_t = s, A_t = a]\\&= \mathbb{E}[R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + ... | S_t = s, A_t = a],\end{aligned}$$

    这是从$(s,a)$开始时所获得的期望回报。由于$q_{\pi_k}(s,a)$是一个期望值，因此可以通过第$5.1$节中所展示的蒙特卡洛(MC)方法进行估计。为此，从$(s,a)$开始，智能体可以按照策略$\pi_k$与环境交互，然后获得一定数量的轨迹。假设共有$n$条轨迹，第$i$条轨迹的回报为$g^{(i)}_{\pi_k}(s,a)$，那么$q_{\pi_k}(s,a)$可以近似为：

    $$q_{\pi k}(s,a) = \mathbb{E}[G | S_t = s, A_t = a] \approx \frac{1}{n} \sum_{i=1}^{n} g_{\pi_k}^{(i)}(s,a).$$

    我们已经知道，如果试验次数n足够大，根据大数定律，这种近似将会足够精确。

基于蒙特卡洛(MC)的强化学习的基本思想是，使用一种无模型的方法来估计动作值，如式$(5.2)$所示，以替代策略迭代算法中基于模型的方法。

### 5.2.2 基于蒙特卡洛的算法

我们现在介绍第一个基于蒙特卡洛(MC)的强化学习算法。该算法从初始策略$\pi_0$开始，在第$k$次迭代$(k=0,1,2,...)$中包含两个步骤。

- 步骤一. **策略评估**。此步骤用于估计所有状态-动作对$(s,a)$的$q_{\pi_k}(s,a)$。具体而言，对于每个$(s,a)$，我们收集足够多的轨迹，并使用回报的平均值$q_k(s,a)$来近似$q_{\pi_k}(s, a)$。

- 步骤二. **策略评估**。 这一步骤通过求解$\pi_{k+1}(s)=\arg\max_\pi \sum_a\pi(a|s)q_k(s,a)$来确定所有状态$s\in \mathcal{S}$下的最优策略。Greedy最优策略为$\pi_{k+1}(a_{k}^{*}|s)=1$，其中$a_{k}^{*}=\arg\max_{a}q_{k}(s,a).$。

这是基于蒙特卡洛（MC）的强化学习中最简单的算法，在本书中称为MC Basic。MC Basic算法的伪代码见算法$5.1$。可以看出，它与策略迭代算法非常相似，唯一的区别在于，MC Basic直接从经验样本中计算行动值，而策略迭代则是先计算状态值，再根据系统模型计算行动值。需要注意的是，这种无模型算法直接估计行动值。如果改为估计状态值，则仍需使用系统模型从这些状态值计算动作值，如式(5.1)所示。

由于策略迭代是收敛的，因此在给定足够样本的情况下，MC基本算法也是收敛的。也就是说，对于每个状态-行动对$(s,a)$，假设从$(s,a)$开始的足够多的回合(episode)，那么这些回合回报的平均值可以很好地近似$(s,a)$的动作值。在实际应用中，我们通常无法为每个$(s,a)$都获得足够多的回合，因此动作值的近似可能并不准确。尽管如此，该算法通常仍然可以正常运行。这与截断策略迭代算法类似，在截断策略迭代算法中，动作值也并非精确计算得到。

最后，MC Basic由于其样本效率较低，过于简单而难以实际应用。我们介绍这个算法的原因是让读者掌握基于MC的强化学习的核心思想。在学习本章后面介绍的更复杂的算法之前，充分理解这个算法是非常重要的。我们将看到，通过扩展MC Basic算法，可以很容易地获得更复杂且样本效率更高的算法。

### 5.2.3 示例

#### 一个简单的例子: 逐步实施

 ![](../img/05/2.png)
 > 图$5.3$: 一个用于说明MC基本算法的示例。

接下来，我们通过一个例子来演示MC Basic算法的实现细节。奖励设置为 $r_{boundary} = r_{forbidden} = −1，r_{target} = 1$。折现率为 $\gamma = 0.9$。初始策略$\pi_0$如图$5.3$所示。该初始策略对于状态$s_1$或$s_3$并非最优策略。

虽然所有动作值都应该被计算出来，但由于篇幅限制，我们仅展示了$s_1$的行动值。在$s_1$处，有五种可能的动作。对于每种动作，我们需要收集足够长的多个轨迹，以便有效地近似行动值。然而，由于这个例子在策略和模型方面都是确定性的，多次运行将生成相同的轨迹。因此，每个行动值的估计仅需要一个轨迹即可。

在$\pi_0$之后，我们可以通过分别从$(s_1, a_1),(s_1,a_2),...,(s_1,a_5)$开始，得到以下回合。

- 从$(s_1,a_1)$开始，该回合为$s_1\xrightarrow{a_1}s_1\xrightarrow{a_1}s_1\xrightarrow{a_1}\ldots$。行动值等于该回合的折现回报：

    $$q_{\pi_0}(s_1,a_1)=-1+\gamma(-1)+\gamma^2(-1)+\cdots=\frac{-1}{1-\gamma}.$$

- 从$(s_1,a_2)$开始，该回合为$s_1\xrightarrow{a_2}s_2\xrightarrow{a_3}s_5\xrightarrow{a_3}\ldots$。行动值等于该回合的折现回报：

    $$q_{\pi_0}(s_1,a_2)=0+\gamma0+\gamma^20+\gamma^3(1)+\gamma^4(1)+\cdots=\frac{\gamma^3}{1-\gamma}.$$

- 从$(s_1,a_3)$开始，该回合为$s_1\xrightarrow{a_3}s_4\xrightarrow{a_2}s_5\xrightarrow{a_3}\ldots$。行动值等于该回合的折现回报：

    $$q_{\pi_0}(s_1,a_3)=0+\gamma0+\gamma^20+\gamma^3(1)+\gamma^4(1)+\cdots=\frac{\gamma^3}{1-\gamma}.$$

- 从$(s_1,a_4)$开始，该回合为$s_1\xrightarrow{a_4}s_1\xrightarrow{a_1}s_1\xrightarrow{a_1}\ldots$。行动值等于该回合的折现回报：

    $$q_{\pi_0}(s_1,a_4)=-1+\gamma(-1)+\gamma^2(-1)+\cdots=\frac{-1}{1-\gamma}.$$

- 从$(s_1,a_5)$开始，该回合为$s_1\xrightarrow{a_5}s_1\xrightarrow{a_1}s_1\xrightarrow{a_1}\ldots$。行动值等于该回合的折现回报：
