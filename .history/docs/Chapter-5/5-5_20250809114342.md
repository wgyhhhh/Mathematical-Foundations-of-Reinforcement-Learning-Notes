## 5.5 探索与利用: 以Greedy策略为例

探索与利用(Exploration and exploitation)是强化学习中一个基本的权衡问题。其中，探索是指策略可以尽可能多地采取各种行动，从而确保所有行动都能被充分访问和评估；而利用则是指改进后的策略应采取行动值最大的Greedy动作。然而，由于当前时刻获得的行动值可能由于探索不足而不准确，因此在进行利用的同时，我们仍需保持探索，以避免错过最优行动。

$\varepsilon$-Greedy策略提供了一种平衡探索与利用的方法。一方面，$\varepsilon$-Greedy策略有更高的概率采取Greedy动作，从而能够利用估计的值；另一方面，$\varepsilon$-Greedy策略也有机会采取其他动作，从而保持探索。$\varepsilon$-Greedy策略不仅用于基于蒙特卡洛(MC)的强化学习，也用于其他强化学习算法中，例如第$7$章介绍的时序差分学习。

利用与最优性相关，因为最优策略应当是Greedy的。$\varepsilon$-Greedy策略的基本思想是通过牺牲最优性/利用性来增强探索性。如果我们希望增强利用性和最优性，就需要降低$\varepsilon$的值。然而，如果我们希望增强探索性，则需要提高$\varepsilon$的值。

接下来，我们将基于一些有趣的例子来讨论这种权衡。这里的强化学习任务是一个$5\times 5$的网格世界。奖励设置为：$r_\text{boundary} = −1$，$r_\text{forbidden} = −10$，$r_\text{target} = 1$。折现率为$\gamma= 0.9$。

### $\varepsilon$-Greedy策略的最优性

接下来我们将展示，当$\varepsilon$增加时，$\varepsilon$-Greedy策略的最优性会变差。

- 首先，在图$5.6(a)$中展示了Greedy的最优策略及其对应的最优状态值。一些一致的$\varepsilon$-Greedy策略的状态值如图$5.6(b)-(d)$。这里，两个$\varepsilon$-Greedy策略一致，当且仅当策略中具有最大概率的行动相同。

    随着$\varepsilon$值的增加，$\varepsilon$-Greedy策略的状态值会下降，这表明这些$\varepsilon$-Greedy策略的最优性会变差。值得注意的是，当$\varepsilon$值达到$0.5$时，目标状态的值会变得最小。这是因为，当$\varepsilon$值较大时，从目标区域出发的智能体可能会进入周围的禁区，从而更有可能获得负奖励。

- 其次，图$5.7$展示了最优的$\varepsilon$-Greedy策略（它们在$\Pi_\varepsilon$中是最优的）。当$\varepsilon=0$时，该策略是Greedy策略，并且在所有策略中是最优的。当$\varepsilon$小到$0.1$时，最优的$\varepsilon$-Greedy策略与最优的Greedy策略一致。然而，当$\varepsilon$增加到例如$0.2$时，所得到的$\varepsilon$-Greedy策略与最优的Greedy策略就不一致了。因此，如果我们希望得到与最优Greedy策略一致的$\varepsilon$-Greedy策略，$\varepsilon$的值应足够小。

    为什么当$\varepsilon$较大时，$\varepsilon$-Greedy策略与最优Greedy策略不一致？我们可以通过考虑目标状态来回答这个问题。在Greedy的情况下，目标状态下的最优策略是保持不动以获得正奖励。然而，当$\varepsilon$较大时，进入禁区并获得负奖励的可能性很高。因此，在这种情况下，目标状态下的最优策略是逃离，而不是保持不动。

### $\varepsilon$-Greedy策略的探索能力

接下来，我们说明当$\varepsilon$较大时，$\varepsilon$-Greedy策略的探索能力较强。

首先，考虑一个探索概率$\varepsilon=1$的$\varepsilon$-Greedy策略(见图$5.5(a)$)。在这种情况下，由于该策略在任何状态下采取任何动作的概率均为$0.2$，因此其探索能力很强。从$(s_1,a_1)$开始，由$\varepsilon$-策略生成的一条轨迹如图$5.8(a)-(c)$所示。可以看出，当回合足够长时，由于该策略具有很强的探索能力，这条轨迹可以多次访问所有状态-行动对。此外，如图$5.8(d)$所示，所有状态-动作对被访问的次数几乎均匀。