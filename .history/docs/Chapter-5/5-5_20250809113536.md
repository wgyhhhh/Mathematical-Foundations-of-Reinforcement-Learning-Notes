## 5.5 探索与利用: 以Greedy策略为例

探索与利用(Exploration and exploitation)是强化学习中一个基本的权衡问题。其中，探索是指策略可以尽可能多地采取各种行动，从而确保所有行动都能被充分访问和评估；而利用则是指改进后的策略应采取行动值最大的Greedy动作。然而，由于当前时刻获得的行动值可能由于探索不足而不准确，因此在进行利用的同时，我们仍需保持探索，以避免错过最优行动。

$\varepsilon$-Greedy策略提供了一种平衡探索与利用的方法。一方面，$\varepsilon$-Greedy策略有更高的概率采取Greedy动作，从而能够利用估计的值；另一方面，$\varepsilon$-Greedy策略也有机会采取其他动作，从而保持探索。$\varepsilon$-Greedy策略不仅用于基于蒙特卡洛(MC)的强化学习，也用于其他强化学习算法中，例如第$7$章介绍的时序差分学习。

利用与最优性相关，因为最优策略应当是Greedy的。$\varepsilon$-Greedy策略的基本思想是通过牺牲最优性/利用性来增强探索性。如果我们希望增强利用性和最优性，就需要降低$\varepsilon$的值。然而，如果我们希望增强探索性，则需要提高$\varepsilon$的值。

接下来，我们将基于一些有趣的例子来讨论这种权衡。这里的强化学习任务是一个$5\times 5$的网格世界。奖励设置为：$r_\text{boundary} = −1$，$r_\text{forbidden} = −10$，$r_\text{target} = 1$。折现率为$\gamma= 0.9$。

