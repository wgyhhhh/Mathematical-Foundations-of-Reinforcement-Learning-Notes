## 10.5 总结

在本章中，我们介绍了演员-评论家方法(actor-critic methods)，主要内容总结如下。

- $10.1$节介绍了最简单的演员-评论家算法QAC。该算法与前一章介绍的策略梯度算法REINFORCE类似，唯一区别在于QAC的 q值估计依赖于时序差分(TD)学习，而 REINFORCE依赖于蒙特卡洛估计。

- 第$10.2$节将QAC方法扩展至优势演员-评论家框架。研究表明，策略梯度对于任何附加基线均具有不变性；进一步分析表明，采用最优基线可有效降低估计方差。

- 第10.3节进一步将优势演员-评论家算法推广至异策略场景。为此，我们引入了一项称为重要性采样的关键技术。

- 最后，尽管之前介绍的所有策略梯度算法都依赖于随机策略，但我们在第$10.4$节中证明了策略可以被强制为确定性策略。文中推导了相应的梯度，并引入了确定性策略梯度算法。

策略梯度和行动者-评论家方法是现代强化学习中广泛使用的技术。文献中存在大量先进算法，例如SAC [76,77]、TRPO [78]、PPO [79]和 TD3 [80]。此外，单智能体情形也可扩展至多智能体强化学习场景[81–85]。经验样本亦可用于拟合系统模型，实现基于模型的强化学习[15,86,87]。分布式强化学习提供了与传统方法根本不同的研究视角[88,89]。强化学习与控制理论的关联性已在文献[90–95]中深入探讨。本书虽无法涵盖所有这些主题，但希望所奠定的理论基础能帮助读者在未来更好地开展相关研究。