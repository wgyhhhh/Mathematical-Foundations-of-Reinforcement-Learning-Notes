---
title: 10.4 确定性演员-评论性的方法
comments: true  # 开启评论
---
迄今为止，策略梯度方法中采用的所有策略均为随机策略，因为要求对于任意状态-行动对$(s, a)$满足$\pi(a|s, \theta) >0$。本节将论证确定性策略同样适用于策略梯度方法。此处"确定性"指：对于任意状态，仅单一动作被赋予概率$1$，其余动作概率均为$0$。研究确定性策略具有重要意义，因其天然具备异策略特性，并能有效处理连续动作空间。

我们一直使用$\pi(a|s, \theta)$表示通用策略，该策略既可以是随机策略也可以是确定性策略。本节中，我们采用

$$a=\mu(s,\theta)$$

为明确表示确定性策略，此处采用$\mu$区别于$\pi$：$\pi$给出动作的概率分布，而$\mu$直接输出动作，因其本质是从状态空间$\mathcal{S}$到动作空间$\mathcal{A}$的映射。该确定性策略可通过神经网络实现，例如以状态$s$为输入、动作$a$为输出、参数$\theta$为权重。为简洁起见，常将$\mu(s, \theta)$简写为 $\mu(s)$。

!!! note
    如果我在一个状态下有无数个行动，这时候就可以用确定性(deterministic)的情况。

### 10.4.1 确定性策略梯度定理

上一章介绍的政策梯度定理仅适用于随机策略。当要求策略为确定性时，必须推导新的政策梯度定理。

!!! info
    **定理10.2（确定性策略梯度定理）**。目标函数$J(\theta)$的梯度为
    
    $$\nabla_\theta J(\theta) = \int_{s \in \mathcal{S}} \eta(s) \nabla_\theta \mu(s) \left. \nabla_a q_\mu(s, a) \right|_{a=\mu(s)} = \mathbb{E}_{S \sim \eta} \left[ \nabla_\theta \mu(S) \left. \nabla_a q_\mu(S, a) \right|_{a=\mu(S)} \right],\tag{10.14}$$

    其中 $\eta$表示状态的概率分布。

定理$10.2$是对定理$10.3$和定理$10.4$所述结果的总结，因为这两个定理中的梯度具有相似的表达式。$J(\theta)$和$\eta$的具体表达式可参见定理$10.3$和定理$10.4$。

与随机策略的情况不同，式$(10.14)$所示的确定性策略梯度不涉及动作随机变量$A$。因此，当我们使用样本来近似真实梯度时，无需对动作进行采样。这使得确定性策略梯度方法具有异策略特性。此外，部分读者可能疑惑为何$\left. \nabla_a q_\mu(S, a) \right|_{a=\mu(S)}$不能简写为$\nabla_a q_\mu(S, \mu(S))$——这是因为若采用后者，将无法明确$q_\mu(S, \mu(S))$如何作为$a$的函数。更简洁且不易混淆的表达式可写作$\nabla_a q_\mu(S, a = \mu(S))$。

在本小节剩余部分，我们将详细推导定理$10.2$。具体而言，我们将推导两种常见指标的梯度：其一是平均值，其二是平均奖励。由于这两个指标已在第$9.2$节详细讨论过，我们有时会直接引用其性质而不加证明。对于多数读者而言，只需熟悉定理$10.2$的内容而无需了解其推导细节。感兴趣的读者可以有选择性地查阅本节后续推导内容。

#### 指标1: 平均值

我们首先推导平均值的梯度：

$$J(\theta)=\operatorname{E}[v_{\mu}(s)]=\sum_{s\in\mathcal{S}}d_{0}(s)v_{\mu}(s),\tag{10.15}$$

其中$d_0$表示状态的初始概率分布。为简化分析，此处设定$d_0$与参数$\mu$无关。$d_0$的选取存在两种特殊但重要的情形：第一种情形是$d_0(s_0) =1$且$d_0(s \neq s_0) =0$，其中$s_0$为特定目标状态。此时策略旨在最大化从$s_0$出发所能获得的折扣回报；第二种情形是$d_0$为不同于目标策略的给定行为策略所对应的状态分布。

为计算$J(\theta)$的梯度，首先需要求解任意状态$s \in \mathcal{S}$下$v_\mu(s)$的梯度。考虑折现因子$\gamma \in (0,1)$的折扣情形。

!!! info
    **引理$10.1$** ($v_\mu(s)$的梯度).在折现情形下，对任意状态 $s \in \mathcal{S}$有
    
    $$\nabla_{\theta}v_{\mu}(s)=\sum_{s^{\prime}\in\mathcal{S}}\Pr_{\mu}(s^{\prime}|s)\nabla_{\theta}\mu(s^{\prime})\left(\nabla_{a}q_{\mu}(s^{\prime},a)\right)|_{a=\mu(s^{\prime})},\tag{10.16}$$

    其中

    $$\Pr_\mu(s'|s) = \sum_{k=0}^\infty \gamma^k [P_\mu^k]_{ss'} = \left[ (I - \gamma P_\mu)^{-1} \right]_{ss'}$$
    
    表示策略$\mu$下从状态$s$转移到$s'$的折扣总概率。此处$[\cdot]_{ss'}$表示矩阵第$s$行第$s'$列的元素。

根据引理$10.1$的准备，我们现在可以推导出$J(\theta)$的梯度。

!!! info
    **定理10.3**(折现情况下的确定性策略梯度定理)。

    在折现因子$\gamma\in(0,1)$的情况下，式$(10.15)$中$J(\theta)$的梯度为
    
    $$\begin{aligned}\nabla_{\theta}J(\theta)&=\sum_{s\in\mathcal{S}}\rho_{\mu}(s)\nabla_{\theta}\mu(s)\left(\nabla_{a}q_{\mu}(s,a)\right)|_{a=\mu(s)}\\&=\mathbb{E}_{S\sim\rho_{\mu}}\left[\nabla_{\theta}\mu(S)\left(\nabla_{a}q_{\mu}(S,a)\right)|_{a=\mu(S)}\right],\end{aligned}$$

    其中状态分布$\rho_\mu$定义为
    
    $$\rho_{\mu}(s)=\sum_{s^{\prime}\in{\mathcal{S}}}d_{0}(s^{\prime})\mathrm{Pr}_{\mu}(s|s^{\prime}),\quad s\in{\mathcal{S}}.$$

    此处，转移概率$\mathrm{Pr}_{\mu}(s|s^{\prime})=\sum_{k=0}^{\infty}\gamma^{k}[P_{\mu}^{k}]_{s^{\prime}s}=[(I-\gamma P_{\mu})^{-1}]_{s^{\prime}s}$表示在策略$\mu$下从状态$s'$转移到$s$的折现总概率。

#### 指标2: 平均奖励

接下来我们推导平均奖励的梯度：

$$\begin{aligned}J(\theta)=\bar{r}_{\mu}&=\sum_{s\in\mathcal{S}}d_{\mu}(s)r_{\mu}(s)\\&=\mathbb{E}_{S\sim d_{\mu}}[r_{\mu}(S)],\end{aligned}\tag{10.20}$$

在这里

$$r_{\mu}(s)=\mathbb{E}[R|s,a=\mu(s)]=\sum rp(r|s,a=\mu(s))$$

该指标表示即时奖励的期望值。更多相关信息详见第$9.2$节。

函数$J(\theta)$的梯度由以下定理给出。

!!! info
    **定理10.4**(无折现情况下的确定性策略梯度定理)。在无折现情况下，$(10.20)$式中$J(\theta)$的梯度为

    $$\begin{aligned}\nabla_{\theta}J(\theta)&=\sum_{s\in\mathcal{S}}d_{\mu}(s)\nabla_{\theta}\mu(s)\left(\nabla_{a}q_{\mu}(s,a)\right)|_{a=\mu(s)}\\&=\mathbb{E}_{S\sim d_{\mu}}\left[\nabla_{\theta}\mu(S)\left(\nabla_{a}q_{\mu}(S,a)\right)|_{a=\mu(S)}\right],\end{aligned}$$

    其中 $d_\mu$为策略 $\mu$下状态的平稳分布。

### 10.4.2 算法描述

根据定理$10.2$给出的梯度，我们可以应用梯度上升算法来最大化$J(\theta)$：

$$\theta_{t+1}=\theta_t+\alpha_\theta\mathbb{E}_{S\sim\eta}\left[\nabla_\theta\mu(S)\left(\nabla_aq_\mu(S,a)\right)|_{a=\mu(S)}\right].$$

对应的随机梯度上升算法为

$$\theta_{t+1}=\theta_t+\alpha_\theta\nabla_\theta\mu(s_t)\left(\nabla_aq_\mu(s_t,a)\right)|_{a=\mu(s_t)}.$$

该实现方法总结于算法$10.4$中。需要注意的是，此算法属于异策略方法，因为行为策略$\beta$可能与目标策略$\mu$不同。首先，演员(actor)是异策略的,我们在阐述定理$10.2$时已说明原因。其次，评论家(critic)同样采用异策略机制，但需特别注意为何评论家虽为异策略却无需重要性采样技术。具体而言，评论家所需的经验样本为$(s_t, a_t, r_{t+1}, s_{t+1}, \tilde{a}_{t+1})$，其中$\tilde{a}_{t+1} = \mu(s_{t+1})$。该样本的生成涉及两个策略：第一个策略在状态$s_t$下生成动作$a_t$，第二个策略在状态$s_{t+1}$下生成动作$\tilde{a}_{t+1}$。由于$a_t$用于与环境交互，其生成策略必然是行为策略；而$\tilde{a}_{t+1}$的生成策略必须是$\mu$，因为评论家的目标正是评估该策略的性能，因此$\mu$作为目标策略存在。需要强调的是，$\tilde{a}_{t+1}$不会在下一时间步用于环境交互，故$\mu$并非行为策略。由此可见，评论家本质上属于异策略方法。

如何选择函数$q(s, a, w)$？提出确定性策略梯度方法的原始研究工作[74]采用了线性函数：$q(s, a, w) = \varphi^T(s, a)w$，其中$\varphi(s, a)$为特征向量。目前普遍采用神经网络来表示$q(s, a, w)$，如深度确定性策略梯度(DDPG)方法[75]所建议的。

 ![](../img/10/5.png)

 > 算法$10.4$：确定性策略梯度或确定性行动者-评论家方法

如何选择行为策略$\beta$？它可以是任意探索性策略，也可以是通过向$\mu$添加噪声获得的随机策略[75]。此时$\mu$即为行为策略，因此这种方式属于同策略实现。

!!! note
    $\mu$+noise的方式与我们之前的 $\varepsilon$-贪婪的方法类似，但是这里我们不能用，因为这里边他的行动是连续的，我不能在其它有限的行动上加一些比较小的概率

---