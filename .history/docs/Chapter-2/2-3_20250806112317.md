## 2.3 状态值

我们提到，回报可以用来评估政策。然而，它们不适用于随机系统，因为从一种状态开始可能会导致不同的回报。受这一问题的启发，我们在本节中引入了状态值的概念。

首先，我们需要引入一些必要的符号。考虑一连串的时间步骤$t = 0, 1, 2,\cdots$ 在时间$t$，智能体处于状态$S_t$，根据
策略$\pi$采取的行动是$A_t$。下一个状态是$S_{t+1}$，立即获得的奖励是$R_{t+1}$。这一过程可以简洁地表示为: 

$$S_t\xrightarrow{A_t}S_{t+1},R_{t+1}.$$

请注意$S_{t},S_{t+1},A_{t},R_{t+1}$是随机变量。而且$S_t,S_{t+1}\in \mathcal{S}，A_t\in \mathcal{A}(S_t)，R_{t+1}\in \mathcal{R}(S_t,A_t).$

从$t$时刻开始，我们可以得到状态-行动-奖励轨迹:

$$S_t\xrightarrow{A_t}S_{t+1},R_{t+1}\xrightarrow{A_{t+1}}S_{t+2},R_{t+2}\xrightarrow{A_{t+2}}S_{t+3},R_{t+3}\ldots.$$

根据定义，沿轨迹的贴现回报为

$$G_t= R_{t+1}+\gamma R_{t+2}+\gamma^2R_{t+3}+\ldots,$$

其中$\gamma \in (0,1)$是贴现率。因为$R_{t+1},R_{t+2},\cdots$都是随机变量,$G_t$也是一个随机变量。

由于$G_t$是一个随机变量，我们可以计算出它的期望值（也称为期望值或平均值）：

$$v_\pi(s)=\mathbb{E}[G_t|S_t=s].$$

这里，$v_\pi (s)$被称为状态值函数，或简称为$s$的状态值。下面是一些临时的重要说明。

- $v_\pi(s)$依赖于$s$.这是因为它的定义是一种条件期望，其条件是智能体从$S_t=s$开始。

- $v_\pi(s)$依赖于$\pi$，这是因为轨迹是根据政策$\pi$产生的，如果政策不同，状态评价也可能不同。
 
- $v_\pi(s)$并不依赖于当前时间$t$，如果智能体在状态空间中移动，t表示当前的时间步$t$。$v_\pi(s)$的值根据给出的政策确定。

!!! note
    状态值与回报之间的关系进一步明确如下。当策略和系统模型都是确定性的时候，从一个状态开始总是会导致相同的轨迹。在这种情况下，从一个状态出发获得的回报等于该状态的值。相反，当策略或系统模型是随机的时候，从同一状态出发可能会产生不同的轨迹。在这种情况下，不同轨迹的回报是不同的，而状态值就是这些回报的平均值。(回报是固定的，状态值是随机的)

尽管如第$2.1$节所示，回报可以用来评估政策，但使用状态值来评估政策更为合适。使用状态值来评估策略更为正规：能产生更大状态值的策略更好。因此，状态值是强化学习的一个核心概念。状态值固然重要，但随之而来的问题是如何计算状态值。如何计算它们。下一节将回答这个问题。