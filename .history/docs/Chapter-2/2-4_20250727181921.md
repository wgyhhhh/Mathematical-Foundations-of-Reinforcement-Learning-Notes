## 2.4 贝尔曼方程

现在我们来介绍贝尔曼方程，这是一种分析状态值的数学工具。简而言之，贝尔曼方程是一组线性方程，描述了所有状态值之间的关系。

首先，注意到$G_t$可以写成:

$$\begin{aligned}G_{t}&=R_{t+1}+\gamma R_{t+2}+\gamma^{2}R_{t+3}+\ldots\\&=R_{t+1}+\gamma(R_{t+2}+\gamma R_{t+3}+\ldots)\\&=R_{t+1}+\gamma G_{t+1},\end{aligned}$$

在这里$G_{t+1}=R_{t+2}+\gamma R_{t+3}+\cdots$。 这个等式确定了$G_t$和 $G_{t+1}$之间的关系。那么，状态值可以写成:

$$\begin{aligned}v_{\pi}(s)&=\mathbb{E}[G_{t}|S_{t}=s]\\&=\mathbb{E}[R_{t+1}+\gamma G_{t+1}|S_{t}=s]\\&=\mathbb{E}[R_{t+1}|S_{t}=s]+\gamma\mathbb{E}[G_{t+1}|S_{t}=s].\end{aligned}.\tag{2.4}$$

下面将对$(2.4)$中的两个项进行分析。

第一项$\mathbb{E}[R_{t+1}|S_t=s]$，是目前奖励的期望，是通过使用附录$A$中的期望值公式，可以计算得出:

$$\begin{aligned}\mathbb{E}[R_{t+1}|S_{t}=s]&=\sum_{a\in\mathcal{A}}\pi(a|s)\mathbb{E}[R_{t+1}|S_{t}=s,A_{t}=a]\\&=\sum_{a\in\mathcal{A}}\pi(a|s)\sum_{r\in\mathcal{R}}p(r|s,a)r.\end{aligned}.\tag{2.5}$$

这里，$\mathcal{A}$和$\mathcal{R}$分别是可能的行动集和奖励集。需要注意的是不同状态下的$\mathcal{A}$可能不同。在这种情况下，$\mathcal{A}$应写成$\mathcal{A}(s)$。同样，$\mathcal{R}$也可能取决于$(s, a)$。为简单起见，我们放弃对$s$或$(s, a)$的依赖。不过，在存在依赖关系的情况下，结论仍然有效。
