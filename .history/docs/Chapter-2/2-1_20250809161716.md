## 2.1 为什么回报很重要?

上一章介绍了回报的概念。事实上，回报在强化学习中起着根本性的作用，因为它可以评估一个策略是好还是坏，下面的例子就证明了这一点。


 ![](../img/02/1.png)

 > 图$2.2$: 举例说明回报的重要性。这三个例子对$s_1$采用了不同的政策。

请看图$2.2$所示的三种策略。可以看出，三种策略在$s_1$时是不同的。哪种策略最好，哪种策略最差？直观地说，最左边的策略是最好的，因为从$s_1$开始,智能体可以避开禁区。中间的策略直观上较差，因为从$s_1$开始的智能体移动到了禁区。最右边的策略介于其他策略之间，因为智能体进入禁区的概率为$0.5$。

上述分析是基于直觉，但这显得不太严谨，我们能否用数学来描述这种直觉。答案是肯定的，并且我们可以依赖于回报概念来描述。具体来说，假设智能体从$s_1$开始。


1.   按照第一种策略，轨迹为$s_1\rightarrow s_3\rightarrow s_4\rightarrow s_4\cdots$，相应的贴现回报为:

    $$\begin{aligned}\mathrm{return}_{1}&=0+\gamma1+\gamma^{2}1+\ldots\\&=\gamma(1+\gamma+\gamma^2+\ldots)\\&=\frac{\gamma}{1-\gamma},\end{aligned}$$    

    其中$\gamma \in (0,1)$是折现率。

2.   按照第二种策略，轨迹为$s_1\rightarrow s_2\rightarrow s_4\rightarrow s_4\cdots$，相应的贴现回报：
   
    $$\begin{aligned}\mathrm{return}_{2}&=-1+\gamma1+\gamma^{2}1+\ldots\\&=-1+\gamma(1+\gamma+\gamma^2+\ldots)\\&=-1+\frac{\gamma}{1-\gamma}.\end{aligned}$$

3.   按照第三种策略，有两种可能的策略。第一个是$s_1\rightarrow s_3\rightarrow s_4\rightarrow s_4\cdots$，另一个是$s_1\rightarrow s_2\rightarrow s_4\rightarrow s_4\cdots$。这两条轨迹的可能性均为$0.5$。那么平均回报为:

    $$\begin{aligned}\mathrm{return}_{3}&=0.5\left(-1+\frac{\gamma}{1-\gamma}\right)+0.5\left(\frac{\gamma}{1-\gamma}\right)\\&=-0.5+\frac{\gamma}{1-\gamma}.\end{aligned}$$

通过比较可以发现:

$$\mathrm{return}_1>\mathrm{return}_3>\mathrm{return}_2,\tag{2.1}$$

式子$(2.1)$说明第一个策略是最好的因为它的回报最大，第二个策略是最差的因为它的回报是最小的。这一数学结论与上述直觉是一致的：第一种策略是最好的，因为它可以避免进入禁区。而第二种策略是最差的，因为它会导致进入禁区。

上述例子证明了回报可以被用来评估策略：如果遵循该政策所获得的回报更大，则该政策更好。最后，值得注意的是$\mathrm{return}_3$并不严格遵循回报的定义，因为它更像是一个期望值。稍后我们将看到$\mathrm{return}_3$实际上是状态值。
