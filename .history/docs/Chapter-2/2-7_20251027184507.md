---
title: 2.7 根据贝尔曼方程求解状态值
comments: true  # 开启评论
---
给定一个策略，在得到对应的贝尔曼方程后，我们可以从中求解出所有状态值。求解给定策略的状态值是强化学习的一个基本问题，这个问题通常被称为**策略评估** (policy evaluation)。本节将给出两种求解贝尔曼方程的方法

### 2.7.1 方法1：解析解

由于$v_{\pi}=r_{\pi}+\gamma P_{\pi}v_{\pi}$是一个简单的线性方程，因此很容易求出其解析解：

$$v_{\pi}=(I-\gamma P_{\pi})^{-1}r_{\pi}.$$

下面给出了$(I-\gamma P_{\pi})^{-1}$的几个性质。

- 第一，$I-\gamma P_{\pi}$是可逆的。证明如下，根据圆盘定理[4](https://www.cambridge.org/core/books/matrix-analysis/9CF2CB491C9E97948B15FAD835EF9A8B)，$I-\gamma P_{\pi}$的每个特征值都至少位于一个格什高林圆内。具体来说，第$i$个格什高林圆的圆心位于$[I-\gamma P_{\pi}]_{ii}=1-\gamma p_{\pi}(s_{i}|s_{i})$，半径等于$\sum_{j\neq i}[I-\gamma P_{\pi}]_{ij}=-\sum_{j\neq i}\gamma p_{\pi}(s_{j}|s_{i}).$。因为$\gamma<1$，所以其半径小于该圆中心距离原点的距离: $\sum_{j\neq i}\gamma p_{\pi}(s_{j}|s_{i})<1-\gamma p_{\pi}(s_{i}|s_{i}).$。因此，所有格什高林圆都不包含原点，因此$I-\gamma P_{\pi}$的任何特征值都不为零。

- 第二，$(I-\gamma P_{\pi})^{-1} \geq I$，这意味着$(I - \gamma P_\pi)$的每个元素都是非负的，具体地说是大于等于单位矩阵中的相应元素。这是因为$P_\pi$的元素是非负的，考虑矩阵逆的级数展开可得$(I-\gamma P_{\pi})^{-1}=I+\gamma P_{\pi}+\gamma^{2}P_{\pi}^{2}+\cdots\geq I\geq0$。

- 第三，对于任何向量$r\geq 0$，有$(I-\gamma P_{\pi})^{-1}r\geq r \geq 0$。这个性质可由第二个性质推论得到，因为$[(I-\gamma P_{\pi})^{-1}-I]r\geq0$。类似地的，如果$r_1\geq r_2$，则有$(I-\gamma P_\pi)^{-1}r_1\geq(I-\gamma P_\pi)^{-1}r_2$。

!!! note 
    虽然这个公式非常优美，但是在实际求解中我们并不会使用，因为他涉及到一个对矩阵求逆的过程，而现实情况中状态空间是很大的。

### 2.7.2 方法2：数值解

尽管解析解有助于理论分析，但在实践中并不适用，因为它涉及到逆矩阵的运算，因此仍然需要复杂的数值算法来计算。事实上，我们可以用下面的数值迭代算法从贝尔曼方程求解出状态值：

$$v_{k+1}=r_{\pi}+\gamma P_{\pi}v_{k},\quad k=0,1,2,\ldots,\tag{2.11}$$

从一个状态猜测$v_0\in \mathbb{R}^n$，该算法给出一个序列$\{v_0,v_1,v_2,\cdots\}$，并且该序列会逐渐收敛到真实的状态值，即：

$$v_{k}\to v_{\pi}=(I-\gamma P_{\pi})^{-1}r_{\pi},\quad k\to\infty.\tag{2.12}$$

感兴趣的读者可以看证明Box $2.1$。


### 2.7.3 示例

下面我们应用$(2.11)$中的算法求解一些示例的状态值，具体求解过程我们不再展示，这里重点关注求解得到的状态值具有什么样的规律。

图$2.7$中橙色的单元格代表禁止区域，蓝色的单元格代表目标区域，奖励设置为$r_{boundary}=r_{forbidden}=-1$，$r_{target}=1$。折扣因子设置选取为$\gamma=0.9$。

图$2.7(a)$显示了两个"好"的策略及其通过$(2.11)$得到的对应的状态值。虽然这两个策略的状态值相同，但是在第四列的前两个状态的策略是不同的。因此，我们可以了解不同的政策可能具有相同的状态值。

 ![](../img/02/6.png)
 > 图$2.7$ (a)两种“好的”策略和其状态值。两种政策的状态值相同、但在第四列的前两个状态下，两种政策是不同的。

图$2.7(b)$显示了两种"不好"的策略及其对应的状态值。这两种政策之所以是不好的，是因为许多状态的行动在直觉上来看就是不合理的，而所获得的状态值更证明了这一推断，可以看出两种政策的状态值均为负值，且远小于图$2.7(a)$中的“好”的政策的状态值。

 ![](../img/02/7.png)
 > 图$2.7$ (b)两种“坏的”策略和对应的状态值，状态值小于"好的"策略的状态值
---

