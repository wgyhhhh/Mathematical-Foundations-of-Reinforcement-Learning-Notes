## 2.8从状态值到行动值

在本章中，我们已经讨论了状态值，现在我们转向行动值，它表示在一个状态下采取动作的“价值”。虽然行动值的概念很重要，但之所以在本章最后一节引入它，是因为它在很大程度上依赖于状态价值的概念，在研究行动值之前首先理解状态值是很重要的。

状态-行动对$(s,a)$的行动值定义为：

$$q_\pi(s,a)=\mathbb{E}[G_t|S_t=s,A_t=a].$$

可以看出，行动值被定义为在一个状态下采取动作后可以获得的预期回报。必须注意的是，$q_\pi(s,a)$依赖于状态-行动对$(s,a)$，而不是单独的动作，此外，$q_\pi(s,a)$依赖于策略$\pi$，将此值称为状态-行动值可能更严格，但为了简单起见，通常将其称为行动值。

行动值和状态值之间的关系是什么?

1. 首先，从条件期望的性质可以得出，

    $$\underbrace{\mathbb{E}[G_t|S_t=s]}_{v_\pi(s)}=\sum_{a\in\mathcal{A}}\underbrace{\mathbb{E}[G_t|S_t=s,A_t=a]}_{q_\pi(s,a)}\pi(a|s).$$

    因此有，

    $$v_\pi(s)=\sum_{a\in\mathcal{A}}\pi(a|s)q_\pi(s,a).\tag{2.13}$$

    因此，状态值是与该状态相关联的行动值的期望。

2. 其次，由于状态值由下式给出：

    $$v_{\pi}(s)=\sum_{a\in\mathcal{A}}\pi(a|s)\left[\sum_{r\in\mathcal{R}}p(r|s,a)r+\gamma\sum_{s^{\prime}\in\mathcal{S}}p(s^{\prime}|s,a)v_{\pi}(s^{\prime})\right],$$

    与公式$(2.13)$相比,

    $$\begin{aligned}q_\pi(s,a)=\sum_{r\in\mathcal{R}}p(r|s,a)r+\gamma\sum_{s^{\prime}\in\mathcal{S}}p(s^{\prime}|s,a)v_\pi(s^{\prime}).\end{aligned}\tag{2.14}$$

    可以看出，行动值由两项组成。第一项是即时回报的平均值，第二项是未来回报的平均值。

(2.13)式和(2.14)式都描述了状态值和行动值之间的关系。它们是同一枚硬币的两面：(2.13)式说明了如何从行动值获得状态值，而(2.14)式说明了如何从状态值获得行动值。

### 2.8.1 例子

 ![](../img/02/8.png)
 > 图2.8: 一个演示动作值计算过程的例子。

接下来，我们将通过一个示例来说明计算操作值的过程，并讨论初学者可能会犯的一个常见错误。

考虑图$2.8$所示的随机策略。接下来我们只考察$s_1$的作用。其他状态也可以类似地检查。$(s_1，a_2)$的行动值为

$$q_\pi(s_1,a_2)=-1+\gamma v_\pi(s_2),$$

其中$s_2$是下一个状态。类似地，可以得到

$$q_\pi(s_1,a_3)=0+\gamma v_\pi(s_3).$$

初学者可能会犯的一个常见错误是关于给定策略未选择的操作的值。例如，图$2.8$中的策略只能选择$a_2$或$a_3$，不能选择$a_1$、$a_4$、$a_5$。有人可能会说，由于策略没有选择$a_1$，$a_4$，$a_5$，我们不需要计算它们的行动值，或者我们可以简单地设置$q_\pi(s_1,a_1)=q_\pi(s_1,a_4)=q_\pi(s_1,a_5)=0$。这样是错的！

- 首先，即使策略不会选择某个操作，它仍然具有操作值。在本例中，尽管策略$\pi$在$s_1$处不取$a_1$，但我们仍然可以计算其行动值。具体地说，在取$a_1$之后，智能体被弹回到$s_1$(因此，即时奖励是$-1$)，然后继续从$s_1$开始遵循$\pi$在状态空间中移动(因此，未来奖励是$\gamma v_\pi(s_1)$)。因此,$(s_1,a_1)$的行动值为:
    $$q_\pi(s_1,a_1)=-1+\gamma v_\pi(s_1).$$

    类似地，对于$a_4$和$a_5$，它们也不可能被给定的策略选择，我们有

    $$q_{\pi}(s_{1},a_{4})=-1+\gamma v_{\pi}(s_{1}),\\q_{\pi}(s_{1},a_{5})=0+\gamma v_{\pi}(s_{1}).$$

- 第二，为什么我们关心给定政策不会选择的行动？虽然某些行为不可能被给定的策略选择，但这并不意味着这些行动不好。有可能因为给定的策略不好，所以它不能选择最佳的行动。强化学习的目的是找到最优策略。为此，我们必须继续探索所有行动，以确定每个状态更好的行动。
    
    最后，在计算动作值之后，我们还可以根据公式$(2.14)$计算状态值：

    $$\begin{aligned}v_{\pi}(s_{1})&=0.5q_{\pi}(s_{1},a_{2})+0.5q_{\pi}(s_{1},a_{3}),\\&=0.5[0+\gamma v_{\pi}(s_{3})]+0.5[-1+\gamma v_{\pi}(s_{2})].\end{aligned}$$

### 2.8.2 用行动值表示的贝尔曼方程

我们之前介绍的贝尔曼方程是基于状态值定义的。事实上，它也可以用行动价值观来表达。

特别是，将公式$(2.13)$代入公式$(2.14)$，

$$q_\pi(s,a)=\sum_{r\in\mathcal{R}}p(r|s,a)r+\gamma\sum_{s^{\prime}\in\mathcal{S}}p(s^{\prime}|s,a)\sum_{a^{\prime}\in\mathcal{A}(s^{\prime})}\pi(a^{\prime}|s^{\prime})q_\pi(s^{\prime},a^{\prime}),$$

这是一个行动值的方程。上述方程对每个状态-行动对都有效。如果我们把所有这些方程放在一起，它们的矩阵向量形式是

$$q_\pi=\tilde{r}+\gamma P\Pi q_\pi,\tag{2.15}$$

其中$q_\pi$是由状态-行动对索引的行动值向量：其第$(s,a)$个元素是$[q_\pi]=q_\pi(s,a)$。$\tilde{r}$是由状态-行动对索引的即时奖励向量：$[\tilde{r}]_{(s,a)}=\sum_{r\in\mathcal{R}}p(r|s,a)r$.矩阵$P$是概率转移矩阵，其行由状态-行动对索引，其列由状态索引：$[P]_{(s,a),s'}=p(s'|s,a)$。此外，矩阵$\Pi$是块对角矩阵，其中每个块是$1\times|\mathcal{A}|$向量:$\Pi_{s^{\prime},(s^{\prime},a^{\prime})}=\pi(a^{\prime}|s^{\prime})$，而$\Pi$其他项为零。

与用状态值定义的贝尔曼方程相比，用行动值定义的贝尔曼方程具有一些独特的特点。例如，$\tilde{r}$和$P$独立于策略，仅由系统模型确定。该政策嵌入在$\Pi$中。可以证明，(2.15)也是一个压缩映射，并且有唯一的解，可以迭代求解。更多的细节可以在[5]中找到。