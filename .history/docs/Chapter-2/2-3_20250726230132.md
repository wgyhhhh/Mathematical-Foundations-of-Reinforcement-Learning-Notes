## 2.3 状态值

我们提到，回报可以用来评估政策。然而，它们不适用于随机系统，因为从一种状态开始可能会导致不同的回报。受这一问题的启发，我们在本节中引入了状态值的概念。

首先，我们需要引入一些必要的符号。考虑一连串的时间步骤$t = 0, 1, 2,\cdots$ 在时间$t$，智能体处于状态$S_t$，根据
策略$\pi$采取的行动是$A_t$。下一个状态是$S_{t+1}$，立即获得的奖励是$R_{t+1}$。这一过程可以简洁地表示为: 

$$S_t\xrightarrow{A_t}S_{t+1},R_{t+1}.$$

请注意$S_{t},S_{t+1},A_{t},R_{t+1}$是随机变量。而且$S_t,S_{t+1}\in \mathcal{S}，A_t\in \mathcal{A}(S_t)，R_{t+1}\in \mathcal{R}(S_t,A_t).$

从$t$时刻开始，我们可以得到状态-行动-奖励轨迹:

$$S_t\xrightarrow{A_t}S_{t+1},R_{t+1}\xrightarrow{A_{t+1}}S_{t+2},R_{t+2}\xrightarrow{A_{t+2}}S_{t+3},R_{t+3}\ldots.$$

根据定义，沿轨迹的贴现收益为

$$G_t\doteq R_{t+1}+\gamma R_{t+2}+\gamma^2R_{t+3}+\ldots,$$

其中$\gamma \in (0,1)$是贴现率。注意，$G_t$是一个随机变量，因为$R_{t+1}、R_{t+2}、\cdots$都是随机变量。