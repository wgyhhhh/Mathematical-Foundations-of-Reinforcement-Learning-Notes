## 2.4 贝尔曼方程

现在我们来介绍贝尔曼方程，这是一种分析状态值的数学工具。简而言之，贝尔曼方程是一组线性方程，描述了所有状态值之间的关系。

首先，注意到$G_t$可以写成:

$$\begin{aligned}G_{t}&=R_{t+1}+\gamma R_{t+2}+\gamma^{2}R_{t+3}+\ldots\\&=R_{t+1}+\gamma(R_{t+2}+\gamma R_{t+3}+\ldots)\\&=R_{t+1}+\gamma G_{t+1},\end{aligned}$$

在这里$G_{t+1}=R_{t+2}+\gamma R_{t+3}+\cdots$。 这个等式确定了$G_t$和 $G_{t+1}$之间的关系。那么，状态值可以写成:

$$\begin{aligned}v_{\pi}(s)&=\mathbb{E}[G_{t}|S_{t}=s]\\&=\mathbb{E}[R_{t+1}+\gamma G_{t+1}|S_{t}=s]\\&=\mathbb{E}[R_{t+1}|S_{t}=s]+\gamma\mathbb{E}[G_{t+1}|S_{t}=s].\end{aligned}.\tag{2.4}$$

下面将对$(2.4)$中的两个项进行分析。

第一项$\mathbb{E}[R_{t+1}|S_t=s]$，是目前奖励的期望，是通过使用附录$A$中的期望值公式，可以计算得出:

$$\begin{aligned}\mathbb{E}[R_{t+1}|S_{t}=s]&=\sum_{a\in\mathcal{A}}\pi(a|s)\mathbb{E}[R_{t+1}|S_{t}=s,A_{t}=a]\\&=\sum_{a\in\mathcal{A}}\pi(a|s)\sum_{r\in\mathcal{R}}p(r|s,a)r.\end{aligned}.\tag{2.5}$$

这里，$\mathcal{A}$和$\mathcal{R}$分别是可能的行动集和奖励集。需要注意的是不同状态下的$\mathcal{A}$可能不同。在这种情况下，$\mathcal{A}$应写成$\mathcal{A}(s)$。同样，$\mathcal{R}$也可能取决于$(s, a)$。为简单起见，我们放弃对$s$或$(s, a)$的依赖。不过，在存在依赖关系的情况下，结论仍然有效。

第二项$\mathbb{E}[G_{t+1}|S_t=s]$，是未来奖励的期望，可以计算得出:

$$\begin{aligned}\mathbb{E}[G_{t+1}|S_{t}=s]&=\sum_{s^{\prime}\in\mathcal{S}}\mathbb{E}[G_{t+1}|S_{t}=s,S_{t+1}=s^{\prime}]p(s^{\prime}|s)\\&=\sum_{s^{\prime}\in\mathcal{S}}\mathbb{E}[G_{t+1}|S_{t+1}=s^{\prime}]p(s^{\prime}|s)\quad(\text{due to the Markov property})\\&=\sum_{s^{\prime}\in\mathcal{S}}v_{\pi}(s^{\prime})p(s^{\prime}|s)\\&=\sum_{s^{\prime}\in\mathcal{S}}v_{\pi}(s^{\prime})\sum_{a\in\mathcal{A}}p(s^{\prime}|s,a)\pi(a|s).&(2.6)\end{aligned}$$

上述推导利用了以下事实：$\mathbb{E}[G_{t+1}|S_{t}=s,S_{t+1}=s^{\prime}]=\mathbb{E}[G_{t+1}|S_{t+1}=s^{\prime}],$，这与马尔科夫性质有关，即未来的奖励完全取决于现在的情况，而不是现在的情况。

将(2.5)-(2.6)代入(2.4)可得

$$\begin{aligned}v_{\pi}(s)&=\mathbb{E}[R_{t+1}|S_{t}=s]+\gamma\mathbb{E}[G_{t+1}|S_{t}=s],\\&=\underbrace{\sum_{a\in\mathcal{A}}\pi(a|s)\sum_{r\in\mathcal{R}}p(r|s,a)r}_{\text{mean of immediate rewards}}+\underbrace{\gamma\sum_{a\in\mathcal{A}}\pi(a|s)\sum_{s^{\prime}\in\mathcal{S}}p(s^{\prime}|s,a)v_{\pi}(s^{\prime})}_{\text{mean of future rewards}}\\&=\sum_{a\in\mathcal{A}}\pi(a|s)\left[\sum_{r\in\mathcal{R}}p(r|s,a)r+\gamma\sum_{s^{\prime}\in\mathcal{S}}p(s^{\prime}|s,a)v_{\pi}(s^{\prime})\right],\quad\text{for all }s\in\mathcal{S}.\end{aligned}\tag{2.7}$$