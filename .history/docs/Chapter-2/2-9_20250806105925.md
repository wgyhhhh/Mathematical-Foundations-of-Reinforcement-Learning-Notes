## 2.9总结

本章介绍的最重要的概念是状态值。从数学上来说，状态值是智能体从某个状态开始所能获得的预期回报。不同状态的值是相互关联的，也就是说，状态值$s$的值依赖于一些其他状态的值，这些其他状态的值也依赖于状态$s$本身的值。对于初学者来说，这一现象可能是本章中最令人困惑的部分了，它与一个叫bootstrapping的重要概念有关，bootstrapping涉及到从自身计算一些东西。尽管其可能会有一些反直觉，但如果我们研究贝尔曼方程的矩阵向量形式，就会清楚地知道，贝尔曼方程是描述所有状态值之间的关系的一组线性方程。

由于状态值可以用来评估一个策略是否好，因此从贝尔曼方程求解策略的状态值的过程称为策略评估。正如我们将在本书后面看到的，策略评估是许多强化学习算法中的重要步骤。

另一个重要的概念，行动值，被引入来描述在一个状态下采取一个行动的值。我们将在本书后面看到，当我们试图寻找最优政策时，行动值比状态值可以发挥更直接的作用。最后，贝尔曼方程并不局限于强化学习领域。相反，它广泛存在于控制理论和运筹学等领域。在不同的领域，贝尔曼方程可能有不同的表达式。在这本书中，贝尔曼方程下研究离散马尔可夫决策过程。关于这个主题的更多信息可以在[2]中找到。

