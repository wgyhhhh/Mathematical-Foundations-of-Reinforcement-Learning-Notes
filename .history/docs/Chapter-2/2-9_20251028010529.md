---
title: 2.9 总结
comments: true  # 开启评论
---
本章介绍的最重要的概念是**状态值**。数学上，状态值是智能体从一个状态出发所能获得的回报的期望值。

不同状态的值是相互关联的。例如，状态值$s$的值依赖于其他状态的值，而其他状态的值可能又依赖于更多其他状态或者状态$s$本身的值。对于初学者来说，这一现象可能是本章中最令人困惑的地方了，它涉及一个**自举** (bootstrapping)的重要概念，自举的现代引申意义指的是自迭代算法，即从一个初始值出发不断计算。

尽管理解自举这一概念可能令人困惑，但我们从数学上来看，贝尔曼方程清晰地描述了不同状态值之间的关系，以及如何通过迭代的方法求解状态值。此外，由于状态值可用于评价策略的好坏，因此根据贝尔曼方程求解某一策略的状态值的过程被称为状态评估。状态评估是强化学习算法中的重要步骤。

另一个重要的概念行动值，它可以描述在某个状态下采取某个行动的价值。我们将在本书后面看到，当我们试图寻找最优策略时，行动值相比于状态值将发挥更直接的作用。

最后，贝尔曼方程并不局限于强化学习领域。相反，它广泛存在于控制理论和运筹学等领域。在不同的领域，贝尔曼方程可能有不同的表达式。本书是在离散马尔科夫决策过程的框架中介绍贝尔曼方程的。更多信息可以在[2](https://www.wiley.com/en-cn/Markov+Decision+Processes%3A+Discrete+Stochastic+Dynamic+Programming-p-9780470316887)中找到。

---
