---
title: 2.5 说明贝尔曼方程的例子
comments: true  # 开启评论
---
### 2.5.1 确定性策略

接下来我们用两个例子来演示如何得到贝尔曼方程进而求解状态值。建议读者仔细阅读这些例子，以加深对贝尔曼方程的理解。

 ![](../img/02/3.png)
 > 图$2.4$:演示贝尔曼方程的示例。本例中的政策是确定性的。

考虑图$2.4$中的第一个例子，其策略是确定性的。接下来，我们写出贝尔曼方程，并根据方程求出状态值。

首先，考虑状态$s_1$。其对应的策略是$\pi(a = a_3|s_1) = 1$和$\pi(a \neq a_3|s_1) = 0$。状态转移概率为$p(s' = s_3|s_1,a_3) = 1$和$p(s'\neq s_3|s_1,a_3)=0$。对应的奖励概率为
$p(r = 0|s_1, a_3) = 1$ 和 $p(r \neq 0|s_1, a_3) = 0$.将这些值代入$(2.7)$得出。

$$v_\pi(s_1) = 0 + \gamma v_\pi (s_3)$$

有趣的是，尽管$(2.7)$中贝尔曼方程的表达式看起来很复杂，但上式可却非常简单。由于这个例子非常简单，我们也可以直接根据贝尔曼方程的基本思想快速写出上式，而不是使用复杂的$(2.7)$。具体来说，从$s_1$出发的回报等于即时奖励(这里是0)加上从下一个状态出发的回报，这里是($v_\pi(s_3)$)，这样就可以直接写出上式。

同样，对其他状态可以得到：

$$\begin{aligned}
    v_\pi(s_2)=1+\gamma v_\pi(s_4),\\
    v_\pi(s_3)=1+\gamma v_\pi(s_4),\\
    v_\pi(s_4)=1+\gamma v_\pi(s_4).\\
\end{aligned}$$

下一步我们可以从这些方程中解出状态值。由于这些方程很简单，我们可以手动求解。更复杂的方程将在$2.7$节介绍。在这里，状态值的解法如下:

$$\begin{aligned}
    v_\pi(s_4)=\frac{1}{1-\gamma},\\
    v_\pi(s_3)=\frac{1}{1-\gamma},\\
    v_\pi(s_2)=\frac{1}{1-\gamma},\\
    v_\pi(s_1)=\frac{\gamma}{1-\gamma}.\\
\end{aligned}$$

进一步地将$\gamma=0.9$代入上式可得：

$$\begin{aligned}
    &v_\pi(s_4)=\frac{1}{1-0.9}=10,\\
    &v_\pi(s_3)=\frac{1}{1-0.9}=10,\\
    &v_\pi(s_2)=\frac{1}{1-0.9}=10,\\
    &v_\pi(s_1)=\frac{0.9}{1-0.9}=9.\\
\end{aligned}$$

### 2.5.2 随机性策略

 ![](../img/02/4.png)
 > 图$2.5$:演示贝尔曼方程的示例。本例子中的策略是随机的。

考虑图$2.5$中的例子，策略是随机性的，接下来，我们将写出贝尔曼方程，然后从中求解状态值。

智能体在状态$s_1$下，向右和向下移动的概率同样为$0.5$，即$\pi(a=a_2|s_1) =0.5$和$\pi(a=a_3|s_1)=0.5$，状态转移概率是$p(s'=s_3|s_1, a_3)=1$，且$p(s'=s_2|s_1,a_2)= 1$。奖励的概率是$p(r=0|s_1,a_3)=1$和$p(r=-1|s_1,a_2)=1$。将这些概率值带入$(2.7)$中则有：

$$v_\pi(s_1)=0.5[0+\gamma v_\pi(s_3)]+0.5[-1+\gamma v_\pi(s_2)].$$

类似的，对其他状态可得：

$$\begin{aligned}
    v_\pi(s_2)=1+\gamma v_\pi(s_4),\\
    v_\pi(s_3)=1+\gamma v_\pi(s_4),\\
    v_\pi(s_4)=1+\gamma v_\pi(s_4).\\
\end{aligned}$$

根据上述等式可以求解状态值得到：

$$\begin{aligned}
    v_\pi(s_4)&=\frac{1}{1-\gamma},\\
    v_\pi(s_3)&=\frac{1}{1-\gamma},\\
    v_\pi(s_2)&=\frac{1}{1-\gamma},\\
    v_\pi(s_1)&=0.5[0+\gamma v_\pi(s_3)]+0.5[-1+\gamma v_\pi(s_2)]\\
    &=-0.5+\frac{\gamma}{1-\gamma}
\end{aligned}$$

进一步地将$\gamma=0.9$代入上式可得：

$$\begin{aligned}
    &v_\pi(s_4)=\frac{1}{1-0.9}=10,\\
    &v_\pi(s_3)=\frac{1}{1-0.9}=10,\\
    &v_\pi(s_2)=\frac{1}{1-0.9}=10,\\
    &v_\pi(s_1)=-0.5+\frac{0.9}{1-0.9}=8.5\\
\end{aligned}$$

最后，我们比较上述例子中两种策略的状态值，可以看出

$$v_{\pi 1} (s_i)\geq v_{\pi 2}(s_i), i=1,2,3,4,$$

这表明图$2.4$中的策略更好，这个数学结论与直觉是一致的，因为当智能体从$s_1$开始时，它可以识别并避开禁区。因此，上述两个例子表明可以利用状态值来评价政策的好坏。

---