## 2.8从状态值到行动值

在本章中，我们已经讨论了状态值，现在我们转向行动值，它表示在一个状态下采取动作的“价值”。虽然行动值的概念很重要，但之所以在本章最后一节引入它，是因为它在很大程度上依赖于状态价值的概念，在研究行动值之前首先理解状态值是很重要的。

状态-行动对$(s,a)$的行动值定义为：

$$q_\pi(s,a)\doteq\mathbb{E}[G_t|S_t=s,A_t=a].$$

可以看出，行动值被定义为在一个状态下采取动作后可以获得的预期回报。必须注意的是，$q_\pi(s,a)$依赖于状态-行动对$(s,a)$，而不是单独的动作。将此值称为状态-行动值可能更严格，但为了简单起见，通常将其称为行动值。

行动值和状态值之间的关系是什么?

1. 首先，从条件期望的性质可以得出，

    $$\underbrace{\mathbb{E}[G_t|S_t=s]}_{v_\pi(s)}=\sum_{a\in\mathcal{A}}\underbrace{\mathbb{E}[G_t|S_t=s,A_t=a]}_{q_\pi(s,a)}\pi(a|s).$$

    因此有，

    $$v_\pi(s)=\sum_{a\in\mathcal{A}}\pi(a|s)q_\pi(s,a).\tag{2.13}$$

    因此，状态值是与该状态相关联的行动值的期望。

2. 其次，由于状态值由下式给出：

    $$v_{\pi}(s)=\sum_{a\in\mathcal{A}}\pi(a|s)\left[\sum_{r\in\mathcal{R}}p(r|s,a)r+\gamma\sum_{s^{\prime}\in\mathcal{S}}p(s^{\prime}|s,a)v_{\pi}(s^{\prime})\right],$$

    与公式$(2.13)$相比,

    $$\begin{aligned}q_\pi(s,a)=\sum_{r\in\mathcal{R}}p(r|s,a)r+\gamma\sum_{s^{\prime}\in\mathcal{S}}p(s^{\prime}|s,a)v_\pi(s^{\prime}).\end{aligned}\tag{2.14}$$

    可以看出，行动值由两项组成。第一项是即时回报的平均值，第二项是未来回报的平均值。

(2.13)式和(2.14)式都描述了状态值和行动值之间的关系。它们是同一枚硬币的两面：(2.13)式说明了如何从行动值获得状态值，而(2.14)式说明了如何从状态值获得行动值。

### 2.8.1 例子

 ![](../img/02/8.png)
 > 图2.8: 一个演示动作值计算过程的例子。

接下来，我们将通过一个示例来说明计算操作值的过程，并讨论初学者可能会犯的一个常见错误。

考虑图$2.8$所示的随机策略。接下来我们只考察$s_1$的作用。其他状态也可以类似地检查。$(s_1，a_2)$的行动值为

$$q_\pi(s_1,a_2)=-1+\gamma v_\pi(s_2),$$

其中$s_2$是下一个状态。类似地，可以得到

$$q_\pi(s_1,a_3)=0+\gamma v_\pi(s_3).$$

初学者可能会犯的一个常见错误是关于给定策略未选择的操作的值。例如，图$2.8$中的策略只能选择$a_2$或$a_3$，不能选择$a_1$、$a_4$、$a_5$。有人可能会说，由于策略没有选择$a_1$，$a_4$，$a_5$，我们不需要计算它们的行动值，或者我们可以简单地设置$q_\pi(s_1,a_1)=q_\pi(s_1,a_4)=q_\pi(s_1,a_5)=0$。这样是错的！

- 首先，即使策略不会选择某个操作，它仍然具有操作值。在本例中，尽管策略$\pi$在$s_1$处不取$a_1$，但我们仍然可以计算其行动值。具体地说，在取$a_1$之后，智能体被弹回到$s_1$(因此，即时奖励是$-1$)，然后继续从$s_1$开始遵循$\pi$在状态空间中移动(因此，未来奖励是$\gamma v_\pi(s_1)$)。因此,$(s_1,a_1)$的动作值为