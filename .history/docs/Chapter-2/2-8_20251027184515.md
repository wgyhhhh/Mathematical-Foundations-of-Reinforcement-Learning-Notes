---
title: 2.8从状态值到行动值
comments: true  # 开启评论
---
到目前为止，我们已经讨论了状态值，现在我们转向另一个重要概念-**行动值** (action value)。虽然行动值的概念很重要，但之所以在本章的最后一节引入它，是因为它依赖于状态值的概念，我们要首先理解状态值，才能更好地理解行动值。

针对一个**状态-行动对** (state-action pair)$(s,a)$，其行动值被定义为：

$$q_\pi(s,a)=\mathbb{E}[G_t|S_t=s,A_t=a].$$

上式表明行动值被定义为在一个状态下采取动作之后获得的回报的期望值。需要注意的是，$q_\pi(s,a)$依赖于一个状态-行动对$(s,a)$，而仅仅是一个行动，因此，或许将此值称为**状态-行动值**可能更严谨，但为了简便起见，通常将其称为行动值。那么行动值和状态值有什么关系呢?

1. 第一，从条件期望的性质可以得出，

    $$\underbrace{\mathbb{E}[G_t|S_t=s]}_{v_\pi(s)}=\sum_{a\in\mathcal{A}}\underbrace{\mathbb{E}[G_t|S_t=s,A_t=a]}_{q_\pi(s,a)}\pi(a|s).$$

    上式可以简化为：

    $$v_\pi(s)=\sum_{a\in\mathcal{A}}\pi(a|s)q_\pi(s,a)=\mathbb{E}_{A_t\sim \pi(s)}[q_\pi(s,A_t)].\tag{2.13}$$

    由上式可以看出，状态值是该状态相关联的行动值的期望值。

2. 第二，因为状态值具有以下表达式：

    $$v_{\pi}(s)=\sum_{a\in\mathcal{A}}\pi(a|s)\left[\sum_{r\in\mathcal{R}}p(r|s,a)r+\gamma\sum_{s^{\prime}\in\mathcal{S}}p(s^{\prime}|s,a)v_{\pi}(s^{\prime})\right],$$

    与公式$(2.13)$相比，可得：

    $$\begin{aligned}q_\pi(s,a)&=\sum_{r\in\mathcal{R}}p(r|s,a)r+\gamma\sum_{s^{\prime}\in\mathcal{S}}p(s^{\prime}|s,a)v_\pi(s^{\prime})\\&=\mathbb{E}[R_{t+1}+\gamma v_\pi(S_{t+1})\mid S_t=s,A_t=a]\end{aligned}\tag{2.14}$$

    可以看出，行动值是一个包含状态值的变量的期望值。

$(2.13)$式和$(2.14)$式都描述了状态值和行动值之间的关系。它们是同一枚硬币的两面：$(2.13)$式说明了如何从行动值获得状态值，而$(2.14)$式说明了如何从状态值获得行动值。

### 2.8.1 示例

 ![](../img/02/8.png)
 > 图2.8: 一个演示行动值计算过程的例子。

接下来，我们将通过一个示例来说明如何计算行动值，并特别指出初学者可能会犯的一个错误。

考虑图$2.8$所示的随机策略。接下来我们只考察状态$s_1$对应的行动值。其他状态也可以按照类似的方法计算。策略使得智能体在状态$s_1$可能采取两个动作$a_2$或者$a_3$。首先，$(s_1，a_2)$的行动值为：

$$q_\pi(s_1,a_2)=-1+\gamma v_\pi(s_2),$$

其中$s_2$是下一个状态。类似地，$(s_1,a_3)$的行动值是：

$$q_\pi(s_1,a_3)=0+\gamma v_\pi(s_3).$$

初学者可能会犯的一个常见错误是：因为因为图$2.8$中的策略只能选择$a_2$或$a_3$，而并不会选择$a_1$、$a_4$、$a_5$。所以我们不需要计算它们的行动值，或者我们可以简单地设置$q_\pi(s_1,a_1)=q_\pi(s_1,a_4)=q_\pi(s_1,a_5)=0$。而这样做是错误的。

- 第一，即使策略不会选择某个行动，它仍然具有行动值。在这个例子中，尽管策略$\pi$在$s_1$处不会选取$a_1$，但我们仍然可以计算“假如”采取这个这一行动将获得的回报。具体地说，在状态$s_1$选取$a_1$后，智能体被弹回到$s_1$(因此即时奖励是$-1$)，然后继续从$s_1$开始遵循$\pi$在状态空间中移动(因此未来奖励是$\gamma v_\pi(s_1)$)。综上，$(s_1,a_1)$的行动值为:
  
    $$q_\pi(s_1,a_1)=-1+\gamma v_\pi(s_1).$$

    类似地，对于$a_4$和$a_5$有：

    $$q_{\pi}(s_{1},a_{4})=-1+\gamma v_{\pi}(s_{1}),\\q_{\pi}(s_{1},a_{5})=0+\gamma v_{\pi}(s_{1}).$$

- 第二，为什么我们关心政策不会选择的行动？虽然有些行动不会被策略所选择，这并不意味着这些行动就不好。相反，这个动作可能是最好的动作，但因为当前的策略不好而不能选择这个行动。强化学习的目的是找到最优策略。为此我们必须探索所有行动，以确定每个状态下最优的行动。
    
    最后，在计算动作值之后，我们还可以根据公式$(2.14)$计算状态值：

    $$\begin{aligned}v_{\pi}(s_{1})&=0.5q_{\pi}(s_{1},a_{2})+0.5q_{\pi}(s_{1},a_{3}),\\&=0.5[0+\gamma v_{\pi}(s_{3})]+0.5[-1+\gamma v_{\pi}(s_{2})].\end{aligned}$$

### 2.8.2 基于行动值的贝尔曼方程

我们之前介绍的贝尔曼方程是基于状态值。事实上，贝尔曼方程也可以用行动值来表达，只是该形式并不常见。不感兴趣的读者可以跳过本节，它并不影响后续课程的学习。

具体来说，将公式$(2.13)$代入公式$(2.14)$可以得到：

$$q_\pi(s,a)=\sum_{r\in\mathcal{R}}p(r|s,a)r+\gamma\sum_{s^{\prime}\in\mathcal{S}}p(s^{\prime}|s,a)\sum_{a^{\prime}\in\mathcal{A}(s^{\prime})}\pi(a^{\prime}|s^{\prime})q_\pi(s^{\prime},a^{\prime}),$$

这也是一个贝尔曼方程。只不过它只包含行动值而没有状态值，上述方程对每个状态-行动对都是成立的。如果我们把所有这些方程放在一起，则可以得到它们的矩阵向量形式：

$$q_\pi=\tilde{r}+\gamma P\Pi q_\pi,\tag{2.15}$$

其中$q_\pi$是一个行动值向量：他对应$(s,a)$的元素是$[q_\pi]=q_\pi(s,a)$。$\tilde{r}$是由$(s,a)$索引的即时奖励向量$[\tilde{r}]_{(s,a)}=\sum_{r\in\mathcal{R}}p(r|s,a)r$.矩阵$P$是概率转移矩阵，其每一行对应一个状态-行动对，每一列对应一个状态$[P]_{(s,a),s'}=p(s'|s,a)$。矩阵$\Pi$是一个**块对角矩阵** (block diagonal matrix)，其中每个块是一个$1\times|\mathcal{A}|$维的向量：$\Pi_{s^{\prime},(s^{\prime},a^{\prime})}=\pi(a^{\prime}|s^{\prime})$，而$\Pi$其他元素都为$0$。

相比于基于状态值的贝尔曼方程，基于行动值的贝尔曼方程具有一些独特的性质。例如，$\tilde{r}$和$P$不依赖于策略，仅由系统模型确定。该政策嵌入在$\Pi$中。更多的细节可以在[5](http://athenasc.com/ndpcontents.html)中找到。
---