## 2.3 状态值

我们提到，回报可以用来评估政策。然而，它们不适用于随机系统，因为从一种状态开始可能会导致不同的回报。受这一问题的启发，我们在本节中引入了状态值的概念。

首先，我们需要引入一些必要的符号。考虑一连串的时间步骤$t = 0, 1, 2,\cdots$ 在时间$t$，智能体处于状态$S_t$，根据
策略$\pi$采取的行动是$A_t$。下一个状态是$S_{t+1}$，立即获得的奖励是$R_{t+1}$。这一过程可以简洁地表示为: 

$$S_t\xrightarrow{A_t}S_{t+1},R_{t+1}.$$

请注意$S_{t},S_{t+1},A_{t},R_{t+1}$是随机变量。而且$S_t,S_{t+1}\in \mathcal{S}，A_t\in \mathcal{A}(S_t)，R_{t+1}\in \mathcal{R}(S_t,A_t).$