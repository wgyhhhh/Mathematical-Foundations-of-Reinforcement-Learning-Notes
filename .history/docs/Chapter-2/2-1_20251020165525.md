## 2.1 为什么回报很重要?

上一章介绍了回报的概念，它在强化学习中扮演着重要的角色，因为它可以评估一个策略的好坏，我们通过下面的例子来说明这一点。图$2.2$给出了三种策略：这三个策略在状态$s_1$是不同的，在其他状态是相同的。那么这三个策略哪一个最好，哪一个最差呢？


 ![](../img/02/1.png)

 > 图$2.2$: 举例说明回报的重要性。这三个例子对$s_1$采用了不同的政策。

直观上来判断，最左边的策略是最好的，因为智能体从$s_1$出发可以避开禁区。中间的策略最差，因为从$s_1$出发的智能体进入了禁区。最右边的策略介于两种策略之间，因为进入禁区的概率为$0.5$，也有$0.5$的概率避开禁止区域。

上述分析的直观判断能否用数学来描述呢？答案是肯定的，具体来说，假设智能体的初始状态为$s_1$。


1.   按照第一种策略，轨迹为$s_1\rightarrow s_3\rightarrow s_4\rightarrow s_4\cdots$，相应的折扣回报为:

    $$\begin{aligned}\mathrm{return}_{1}&=0+\gamma1+\gamma^{2}1+\ldots\\&=\gamma(1+\gamma+\gamma^2+\ldots)\\&=\frac{\gamma}{1-\gamma},\end{aligned}$$    

    其中$\gamma \in (0,1)$是折扣因子。

2.   按照第二种策略，轨迹为$s_1\rightarrow s_2\rightarrow s_4\rightarrow s_4\cdots$，相应的折扣回报是：
   
    $$\begin{aligned}\mathrm{return}_{2}&=-1+\gamma1+\gamma^{2}1+\ldots\\&=-1+\gamma(1+\gamma+\gamma^2+\ldots)\\&=-1+\frac{\gamma}{1-\gamma}.\end{aligned}$$

3.   按照第三种策略，有两种可能的策略。第一个是$s_1\rightarrow s_3\rightarrow s_4\rightarrow s_4\cdots$，另一个是$s_1\rightarrow s_2\rightarrow s_4\rightarrow s_4\cdots$。这两条轨迹的可能性均为$0.5$。那么平均回报为:

    $$\begin{aligned}\mathrm{return}_{3}&=0.5\left(-1+\frac{\gamma}{1-\gamma}\right)+0.5\left(\frac{\gamma}{1-\gamma}\right)\\&=-0.5+\frac{\gamma}{1-\gamma}.\end{aligned}$$

通过上面计算出来的三个策略的回报可以发现:

$$\mathrm{return}_1>\mathrm{return}_3>\mathrm{return}_2,\tag{2.1}$$

不等式$(2.1)$表明：第一个策略是最好的，因为它的回报最大；第二个策略是最差的，因为它的回报是最小的。这一数学结论与上述直觉是一致的：第一种策略是最好的，因为它可以避免进入禁区。而第二种策略是最差的，因为它会导致智能体进入禁区。

上述例子证明了回报可以被用来评估策略的好坏。值得注意的是$\mathrm{return}_3$并不严格遵循回报的定义，回报的定义只是针对于一条轨迹，而$\mathrm{return}_3$是两条轨迹的平均值。稍后我们就会知道$\mathrm{return}_3$实际上是本章要介绍的状态值。
