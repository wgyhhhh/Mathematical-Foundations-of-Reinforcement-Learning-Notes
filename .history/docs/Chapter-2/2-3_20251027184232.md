---
title: 2.3 状态值
comments: true  # 开启评论
---
前面提到了回报可以用来评价政策。然而，回报不适用于随机系统，因为从一种状态出发可能会得到不同的轨迹和回报。为此，我们在本节引入了状态值的概念。

首先，我们需要引入一些的符号。在时刻$t = 0, 1, 2,\cdots$，智能体处于状态$S_t$，根据策略$\pi$采取的行动$A_t$。转移到下一个状态是$S_{t+1}$，获得的即时奖励是$R_{t+1}$。这一过程可以简洁地表示为: 

$$S_t\xrightarrow{A_t}S_{t+1},R_{t+1}.$$

这里$S_{t},S_{t+1},A_{t},R_{t+1}$都是随机变量。而且$S_t,S_{t+1}\in \mathcal{S}，A_t\in \mathcal{A}(S_t)，R_{t+1}\in \mathcal{R}(S_t,A_t).$

从时刻$t$开始，我们可以得到一条包含一系列“状态-行动-奖励”的轨迹:

$$S_t\xrightarrow{A_t}S_{t+1},R_{t+1}\xrightarrow{A_{t+1}}S_{t+2},R_{t+2}\xrightarrow{A_{t+2}}S_{t+3},R_{t+3}\ldots.$$

根据定义，沿这个轨迹的折扣回报是：

$$G_t= R_{t+1}+\gamma R_{t+2}+\gamma^2R_{t+3}+\ldots,$$

其中$\gamma \in (0,1)$是折扣因子。因为$R_{t+1},R_{t+2},\cdots$都是随机变量,所以由它们组合得到的$G_t$也是一个随机变量。

由于$G_t$是一个随机变量，我们可以计算出它的期望值：

$$v_\pi(s)=\mathbb{E}[G_t|S_t=s].$$

这里的$v_\pi (s)$被称为**状态值函数** (state-value function)，或简称为$s$的**状态值** (state value)。下面是对状态值的一些说明。

- 第一，$v_\pi(s)$依赖于$s$，即不同的状态的状态值一般是不同的。这是因为它在$(2.5)$中的定义是一种条件期望，而其中的条件是$S_t=s$。

- 第二，$v_\pi(s)$依赖于$\pi$，即不同策略的对应的状态值一般是不同的，这是因为轨迹是根据策略$\pi$产生的，不同的策略可能会导致不同的轨迹。
 
- 第三，$v_\pi(s)$并不依赖于$t$，虽然$v_\pi(s)$的定义涉及时刻$t$，但是不论$t$选取什么值得到的结果都应该是相同的，因此系统是平稳的，不会随着时间而变化。

!!! note
    状态值与回报之间的关系进一步明确如下。当策略和系统模型都是**确定性**的时候，从一个状态开始总是会导致相同的轨迹。在这种情况下，从一个状态出发获得的回报等于该状态的值。相反，当策略或系统模型是**随机**的时，从同一状态出发可能会产生不同的轨迹。在这种情况下，不同轨迹的回报是不同的，而状态值就是这些回报的平均值。(回报是固定的，状态值是随机的)

尽管如第$2.1$节所示，回报可以用来评估策略，但使用状态值来评估策略更为合适。因此，状态值是强化学习的一个核心概念。状态值固然重要，但随之而来的问题是如何计算状态值。这个问题将在下一节回答。
---