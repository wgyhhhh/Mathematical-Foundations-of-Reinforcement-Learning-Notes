## 如何计算回报?

虽然我们已经证明了回报的重要性，但紧接着的一个问题是，在遵循特定政策时，如何计算回报。

计算回报有两种方法。

 ![](../img/02/2.png)
 > 图2.3: 演示如何计算回报的示例。本例中没有目标单元格或禁止单元格。

- 第一个简单的定义是：回报等于沿轨迹收集的所有奖励的贴现总和。请看图 2.3 中的例子。让$v_i$表示时从$s_i$($i = 1, 2, 3, 4$)开始获得的回报。那么，从图 2.3 中的四种状态出发所得到的收益计算公式为:
  
$$\begin{gathered}v_{1}=r_1+\gamma r_2+\gamma^2r_3+\ldots,\\v_{2}=r_2+\gamma r_3+\gamma^2r_4+\ldots,\\v_{3}=r_3+\gamma r_4+\gamma^2r_1+\ldots,\\\mathrm{v}_4=r_{4}+\gamma r_{1}+\gamma^{2}r_{2}+\ldots.\end{gathered}\tag{2.2}$$

- 第二种方法更为重要，它基于自举(bootstrapping)的思想。通过观察式$(2.2)$中的收益表达式，我们可以将其重写为:

    $$\begin{gathered}v_{1}=r_1+\gamma(r_2+\gamma r_3+\ldots)=r_1+\gamma v_2,\\v_{2}=r_2+\gamma(r_3+\gamma r_4+\ldots)=r_2+\gamma v_3,\\v_{3}=r_3+\gamma(r_4+\gamma r_1+\ldots)=r_3+\gamma v_4,\\v_{4}=r_4+\gamma(r_1+\gamma r_2+\ldots)=r_4+\gamma v_1.\end{gathered}\tag{2.3}$$
    
    上述等式表明了一个有趣的现象，即
    的回报值相互依赖。更具体地说，$v_1$依赖于$v_2$，$v_2$依赖于$v_3$，$v_3$依赖于$v_4$，$v_4$又依赖于$v_1$。这反映了自举的思想，即从某些量本身获得值。
    
    乍一看，引导法是一个无休止的循环，因为一个未知值的计算依赖于另一个未知值。事实上，如果我们从数学的角度来看，引导法更容易理解。特别是，(2.3)的方程可以转换成一个线性矩阵-向量方程：
    
    $$\underbrace{\begin{bmatrix}v_1\\v_2\\v_3\\v_4\end{bmatrix}}_{v}=\begin{bmatrix}r_1\\r_2\\r_3\\r_4\end{bmatrix}+\begin{bmatrix}\gamma v_2\\\gamma v_3\\\gamma v_4\\\gamma v_1\end{bmatrix}=\underbrace{\begin{bmatrix}r_1\\r_2\\r_3\\r_4\end{bmatrix}}_{r}+\underbrace{\gamma\begin{bmatrix}0&1&0&0\\0&0&1&0\\0&0&0&1\\1&0&0&0\end{bmatrix}}_{P}\underbrace{\begin{bmatrix}v_1\\v_2\\v_3\\v_4\end{bmatrix}}_{v},$$
    
    其可以更简洁地写为:
    
    $$v=r+\gamma Pv.$$

