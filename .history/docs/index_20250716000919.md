# 读者指南

由于本份文档的长度(请注意主要内容只有100多页，其余为参考文献)，它可能无法让所有读者完整阅读本文档。因此，我们在这里提出一些阅读的策略和建议来帮助读者更好地利用这份文档。

我们建议所有读者在开始阅读此文档时先阅读主要介绍(第一节)，以掌握本文档的高级上下文。为了快速了解概况，读者可以浏览各类挑战的介绍(例如第二节、第三节和第四节)并且查看相关的表1、3和4，这几个表对三个类别中讨论的挑战进行了高度的精简。从这里开始，感兴趣深入了解的读者可以选择任何感兴趣的部分。请注意，所有挑战章节(例如Section 2.1)都具有独立完整性，因此可以按照**任意顺序**进行阅读。

## 机器学习与自然语言处理研究者

本文档的目标读者是机器学习、自然语言处理及相关领域的技术研究人员。我们尽可能减少背景知识预设，仅要求读者了解大语言模型(LLM)的基本概念、架构原理和训练方法。因此，具备机器学习或者自然语言处理专业研究生一年级同等知识水平的读者，均可理解第二节与第三节所述的全部技术挑战，第四节讨论的挑战大部分同样具有技术属性，其理解门槛与之相当。

本文档的主要目标是为初入该领域的研究人员(当然，资深专家也可能从中获得启发)指明具有研究价值且可操作的研究方向。建议读者根据自身兴趣选择性阅读相关章节。所列出的200多个问题，其规模均大致可构成一篇研究论文的基础。针对每项挑战及子挑战，我们在探讨未来研究方向之前，均会阐述其研究意义、背景知识及相关工作。这些内容能为刚接触特定挑战的研究者提供良好的起点，但本文并不试图对任何领域进行全面综述。

我们还指出，虽然本研究源于对大语言模型(LLM)安全性与对齐性的关注，但所揭示的诸多挑战从技术和科学视角来看仍然具有研究价值。因此，即使那些主要关注点不在安全性，而是寻求以LLM为核心的有趣问题的读者，本文档仍然可以提供有益参考。

## 社会技术研究者及其他利益

在第四节中，我们聚焦于社会技术挑战，强调所有大语言模型本质上都是社会技术系统，若缺乏这一视角的深入思考，便无法确保安全性。本节引言建立了我们所讨论的各类挑战与其他学科领域之间的映射关系-这些领域的研究进展可以推动相关挑战的解决。除少数内容外，本节仅需读者具备对大语言模型的基础认知，其目标受众范围较文档其他部分更为广泛。
