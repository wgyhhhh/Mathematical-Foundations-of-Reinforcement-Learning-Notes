## 3.1 启发示例：如何改进策略？

考虑图$3.2$中的示例，其中橙色和蓝色单元格分别表示禁止区域和目标区域。图中的箭头代表一个给定的策略。这里的策略从直观上来说不好，因为它在状态$s_1$中选择了$a_2$(向右移动)，从而进入禁止区域。那么我们能否改进这个策略进而得到一个更好的策略呢？答案是可以的。下面通过一个例子来介绍改进策略的思路。

 ![](../img/03/1.png)
 > 图3.2: 一个用于展示策略改进的例子。


- 第一，直觉告诉我们：如果我们在$s_1$处选择$a_3$(向下)而不是$a_2$(向右)，则策略会更好。这是因为向下移动能够使智能体避免进入禁区。
- 第二，从数学上，上面的直觉可以通过计算状态值和行动值来得到验证。

首先，计算给定策略的状态值。根据第2章的内容，不难写出该策略的贝尔曼方程是：

$$\begin{aligned}
v_{\pi}(s_1) &= -1 + \gamma v_{\pi}(s_2), \\
v_{\pi}(s_2) &= 1 + \gamma v_{\pi}(s_4), \\
v_{\pi}(s_3) &= 1 + \gamma v_{\pi}(s_4), \\
v_{\pi}(s_4) &= 1 + \gamma v_{\pi}(s_4).
\end{aligned}$$

如果设$\gamma=0.9$，可以求出：

$$\begin{aligned}
    v_\pi(s_4)&=v_\pi(s_3)=v_\pi(s_2)=10,\\
    v_\pi(s_1)&=8
\end{aligned}$$

然后，计算给定策略下的行动值。针对状态$s_1$，其对应的动作值为：

$$\begin{aligned}
    &q_{\pi}(s_1, a_1) = -1 + \gamma v_{\pi}(s_1) = 6.2, \\
&q_{\pi}(s_1, a_2) = -1 + \gamma v_{\pi}(s_2) = 8, \\
&q_{\pi}(s_1, a_3) = 0 + \gamma v_{\pi}(s_3) = 9, \\
&q_{\pi}(s_1, a_4) = -1 + \gamma v_{\pi}(s_1) = 6.2, \\
&q_{\pi}(s_1, a_5) = 0 + \gamma v_{\pi}(s_1) = 7.2.
\end{aligned}$$

上式表明行动$a_3$具有最大的行动值，即：

$$q_{\pi}(s_1, a_3) \geq q_{\pi}(s_1, a_i), \quad \text{for all } i \neq 3.$$

因此为了得到更大的回报，新的策略应该在状态$s_1$选择$a_2$。

这个例子说明了：如果我们更新策略从而使之选择具有最大行动值的行动，就可以获得更好的策略。

这个例子非常简单，因为给定的策略只对状态$s_1$不利。如果策略对其他状态也不利，那么在$s_1$选择最大行动值的行动能否得到更好的策略呢？此外还有很多问题，是否总存在最优策略？最优政策是什么样的？我们将在本章中回答这些问题。