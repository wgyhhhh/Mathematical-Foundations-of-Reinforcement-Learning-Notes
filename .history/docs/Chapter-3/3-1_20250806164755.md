## 3.1 例子：如何改进策略？

 ![](../img/03/1.png)
 > 图3.2: 一个展示策略改进的例子。

考虑图3.2所示的策略。这里，橙色和蓝色单元格分别表示禁止区域和目标区域。这里的策略不好，因为它在状态$s_1$中选择了$a_2$(向右)。我们如何改进现有的策略，以获得更好的策略？答案在于状态值和行动值。

- 直觉: 直觉上很清楚，如果在$s_1$处选择$a_3$(向下)而不是$a_2$(向下)，则策略可以改进。这是因为向下移动能够使智能体避免进入禁区。
- 数学：上述直观可以通过计算状态值和行动值来实现。

首先，我们计算给定策略的状态值。特别是，该政策的贝尔曼方程是

$$v_{\pi}(s_1) = -1 + \gamma v_{\pi}(s_2), \\
v_{\pi}(s_2) = 1 + \gamma v_{\pi}(s_4), \\
v_{\pi}(s_3) = 1 + \gamma v_{\pi}(s_4), \\
v_{\pi}(s_4) = 1 + \gamma v_{\pi}(s_4).
$$