## 3.5 影响最优政策的因素

BOE是一种用于分析最优策略的强大工具。接下来，我们将应用BOE来研究哪些因素会影响最优策略。通过观察BOE的逐元素表达式，可以很容易地回答这个问题：

$$v(s)=\max_{\pi(s)\in\Pi(s)}\sum_{a\in\mathcal{A}}\pi(a|s)\left(\sum_{r\in\mathcal{R}}p(r|s,a)r+\gamma\sum_{s^{\prime}\in\mathcal{S}}p(s^{\prime}|s,a)v(s^{\prime})\right),\quad s\in\mathcal{S}.$$

最优状态值和最优策略由以下参数决定：1)即时奖励$r$, 2)折现率$\gamma$，以及3) 系统模型 $p(s'|s,a), p(r|s,a)$。在系统模型固定的情况下，我们接下来讨论当$r$和$\gamma$的值发生变化时，最优策略如何变化。本节中所展示的所有最优策略均可通过定理$3.3$中的算法获得。该算法的实现细节将在第$4$章中给出。本章主要关注最优策略的基本性质。

### 一个基线例子

考虑图$3.4$中的示例。奖励设置为 $r_{boundary} = r_{forbidden} = −1,r_{target} = 1$。此外，对于每一步移动，智能体都会获得$r_{other}=0$的奖励。折现率选择为$\gamma= 0.9$。

在上述参数下，最优策略和最优状态值如图$3.4(a)$所示。有趣的是，智能体并不惧怕穿过禁区以到达目标区域。更具体地说，从状态(行=4，列=1)开始，智能体有两种到达目标区域的选项。第一种选项是避开所有禁区，长途跋涉到达目标区域；第二种选项是穿过禁区。尽管智能体进入禁区时会获得负奖励，但第二种路径的累积奖励却大于第一种路径。因此，由于$\gamma$值相对较大，最优策略具有远见性。

### 折现率的影响

如果我们把折现率从$\gamma= 0.9$改为$\gamma= 0.5$，并保持其他参数不变，最优策略将变为图$3.4(b)$所示的策略。有趣的是，智能体不再敢冒险，而是宁愿走很长的路到达目标，同时避开所有禁区。这是因为由于$\gamma$的值相对较小，最优策略变得目光短浅。

 ![](../img/03/3.png)
 ![](../img/03/4.png)
 > 图$3.4$: 给定不同参数值时的最优策略和最优状态值。

在极端情况下，当$\gamma= 0$时，相应的最优策略如图$3.4(c)$所示。在这种情况下，智能体无法到达目标区域。这因为每个状态下的最优策略是非常短视的，它仅仅选择具有最大即时奖励的动作，而不是选择具有最大总奖励的动作。

此外，状态值的空间分布呈现出一种有趣的模式：靠近目标的状态具有较大的状态值，而远离目标的状态则具有较小的状态值。这种模式在图$3.4$所示的所有示例中都可以观察到。这种现象可以用折现率来解释：如果一个状态需要沿着更长的轨迹才能到达目标，则由于折现率的作用，其状态值会较小。

### 奖励值的影响

如果我们希望严格禁止智能体进入任何禁区，可以增加其违规行为所受到的惩罚。例如，如果$r_{forbidden}$从$-1$改$-10$，那么得到的最优策略可以避免所有禁区(见图 $3.4(d)$)。

然而，改变奖励并不总是会导致不同的最优策略。一个重要的事实是，最优策略对奖励的仿射变换是不变的。换句话说，如果我们对所有奖励进行缩放，或者给所有奖励加上相同的值，最优策略仍然保持不变。

!!! note
    **定理3.6** (最优策略不变性). 考虑一个马尔可夫决策过程，其最优状态值为$v^∗\in \mathbb{R}^{|\mathcal{S}|}$，满足$v^∗ = max_{\pi\in\Pi}(r_\pi + \gamma P_\pi v&∗)$。如果每个奖励 r ∈ R 经过仿射变换变为 αr + β，其中 α, β ∈ R 且 α > 0，则相应的最优状态值 v 也是 v∗ 的仿射变换：

