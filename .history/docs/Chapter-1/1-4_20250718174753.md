## 策略(Policy)

策略(policy)告诉了智能体在每一个state下应该采取什么actions，policy可以被直观描述为图1.4(b)中的箭头。遵循策略，智能体可以产生一个从初始状态开始的轨迹(如图1.4(b)所示)。

 ![](../img/01/4.png)
 > 图1.4 用箭头和从不同初始状态出发得到的一些轨迹来表示一个policy。

数学上，策略可以用条件概率进行描述。将图1.4中的策略描述为$\pi(a|s)$，这是一个为每个状态定义的条件概率分布函数。例如，状态$s_1$的策略是:

$$\begin{gathered}\pi(a_1|s_1)=0,\\\pi(a_{2}|s_{1})=1,\\\pi(a_3|s_1)=0,\\\pi(a_4|s_1)=0,\\\pi(a_{5}|s_{1})=0,\end{gathered}$$

这表示在状态$s_1$下采取行动$a_2$的概率是1，而采取其他动作的概率为0。

上面的政策是确定性的(deterministic)。但是一般来说，策略是随机的(stochastic)。例如，在图1.5这种展示的策略就是随机的: 在状态$s_1$下，智能体可能采取行动来向右或者向下。采取这两个行动的概率是相同的(都是0.5)。在这种情况下，状态$s_1$的策略是

$$\begin{gathered}\pi(a_1|s_1)=0,\\\pi(a_{2}|s_{1})=0.5,\\\pi(a_3|s_1)=0.5,\\\pi(a_4|s_1)=0,\\\pi(a_{5}|s_{1})=0,\end{gathered}$$