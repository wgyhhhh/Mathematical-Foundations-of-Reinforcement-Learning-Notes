## 轨迹、回报、回合(Trajectories, returns, episodes)

 ![](../img/01/8.png)
 > 图1.6: 根据两种策略所获得轨迹，轨迹在图中用红色虚线标出。

轨迹(trajectory)是一个状态-行动-奖励链。例如，给定一个图1.6所示的策略，智能体将会根据以下所示的轨迹进行移动: 

$$s_1\xrightarrow{a_2}s_2\xrightarrow{a_3}s_5\xrightarrow{a_3}s_8\xrightarrow{a_2}s_9.$$

这条轨迹的回报(return)被定义为沿轨迹收集的所有奖励的总和：

$$\mathrm{return}=0+0+0+1=1.$$

回报也称为总回报(total rewards)或累计回报(cumulative rewards)。

回报可以被用来评估策略。例如，我们可以通过比较图1.6两个策略的回报。具体而言，从$s_1$开始，左侧策略获得的回报为$1$，计算结果如上。对于右策略，从$s_1$开始，对应的轨迹如下: 

$$s_1\xrightarrow{a_3}s_4\xrightarrow{a_3}s_7\xrightarrow{a_2}s_8\xrightarrow{a_2}s_9.$$

对应的回报为:

$$return_2=0-1+0+1=0.$$





