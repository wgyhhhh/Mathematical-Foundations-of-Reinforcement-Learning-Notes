## 轨迹、回报、回合(Trajectories, returns, episodes)

**轨迹** (trajectory)指的是一个“状态-行动-奖励”的链条。例如，给定一个图$1.6 (a)$所示的策略，智能体将会从$s_1$出发得到如下轨迹: 

$$s_1 \xrightarrow[r=0]{a_2} s_2 \xrightarrow[r=0]{a_3} s_5 \xrightarrow[r=0]{a_3} s_8 \xrightarrow[r=1]{a_2} s_9.$$

沿着一条轨迹，智能体将会获得一系列的即时奖励，这些即时奖励之和被称为**回报** (return)。例如，上述轨迹对应的回报为：

$$\mathrm{return}_1=0+0+0+1=1.\tag{1.1}$$

回报由即时奖励 (immediate reward)和未来奖励 (future reward)组成。这里，即时奖励是在初始状态执行动作后立刻获得的奖励：未来奖励指的是离开初始状态后获得的奖励之和。例如上述轨迹对应的即时奖励为$0$，但未来奖励为$1$，因此总奖励是$1$。回报也称为**总奖励** (total rewards)或**累计奖励** (cumulative rewards)。

回报可以被用来评估策略的优劣。例如，比较图$1.6$两个策略的回报，我们可以分别计算两条轨迹对应的回报。进而判断哪个策略更好。具体来说，如果按照图$1.6$左边的策略，从$s_1$开始获得的回报为$1$。对于右边的策略，从$s_1$开始，对应的轨迹如下: 

$$s_1 \xrightarrow[r=0]{a_3}s_4 \xrightarrow[r=-1]{a_3} s_7 \xrightarrow[r=0]{a_2} s_8 \xrightarrow[r=+1]{a_2} s_9$$

相应的回报为:

$$\mathrm{return}_2=0-1+0+1=0.\tag{1.2}$$

$\mathrm{return}_1$和$\mathrm{return}_2$表明，左边的策略比右边策略更好。这一数学结论与我们的直觉是一致的，即右边的策略更糟糕，因为它穿过了一个禁区。

 ![](../img/01/8.png)
 > 图1.6: 根据两种策略所获得轨迹，轨迹在图中用红色虚线标出。

刚刚提到的轨迹都是针对有限长轨迹的，而轨迹往往也可以**无限长**。例如，图$1.6$中的轨迹在到达$s_9$后可能不会停止，而是继续执行策略。具体来说，这里的策略是在到达$s_9$后保持不动，使得智能体在状态$s_9$会不断得到$+1$的奖励。将会产生以下无限长的轨迹：

$$s_1 \xrightarrow[r=0]{a_2} s_2 \xrightarrow[r=0]{a_3} s_5 \xrightarrow[r=0]{a_3} s_8 \xrightarrow[r=1]{a_2} s_9 \xrightarrow[r=1]{a_5} s_9 \xrightarrow[r=1]{a_5} s_9...$$

此时，我们将这条轨迹的奖励求和来计算回报:

$$\mathrm{return}=0+0+0+1+1+1+\cdots=\infty,$$

由于这条轨迹无限长，所计算的回报会发散到无穷的。因此，我们必须引入无限长轨迹的**折扣回报** (discounted return)的概念。具体来说，折扣回报是所有折扣奖励的总和，即在不同时刻得到的奖励添加对应的折扣因子再求和：

$$\text{discounted return}=0+\gamma0+\gamma^20+\gamma^31+\gamma^41+\gamma^51+\ldots,\tag{1.3}$$

在这里$\gamma \in (0,1)$被叫做**折扣因子** (discount rate)。当$\gamma \in (0,1)$，式$(1.3)$可以被计算为:

$$\text{discounted return}=\gamma^3(1+\gamma+\gamma^2+\ldots)=\gamma^3\frac{1}{1-\gamma}.$$

折扣因子的引入有以下用途。第一，它允许无限长的轨迹出现，而不用担心回报会发散到无穷。第二，折扣因子可以用来调整对近期或远期回报的重视程度。具体来说，如果$\gamma$接近$0$，那么智能体就会更重视在近期获得的回报。由此产生的政策将是短视的。如果$\gamma$接近$1$，那么智能体会更重视远期的回报，由此产生的政策更具有远见，例如敢于承担近期内获得负面回报的风险来获得更大的未来奖励。这些结论将在第$3.5$节中加以论证。

当执行一个策略与环境进行交互时，智能体从初始状态开始到**终止状态** (terminal state)停止的过程称为一个**回合** (episode)。

回合和轨迹在概念上非常相似，回合通常被认为是一条有限的长的轨迹。如果一个任务最多有有限步，那么这样的任务称为**回合制任务** (episodic task)。如果一个任务没有终止状态，则意味着智能体与环境的交互不会停止，这类任务称为**持续性任务** (continuing tasks)。为了在数学上将两类任务统一起来，我们需要合理定义智能体到达终止状态后的状态和行动等元素。具体来说，有以下两种方法，将回合制任务转化为持续性任务。

- 第一，我们可以将终端状态视为一种特殊状态，即可以专门设计其行动空间或状态转移，从而使智能体永远停留在此状态。这种状态被称为**吸收状态** (absorbing state)，这意味着智能体只要达到这种状态便永会一直停留在该状态。例如，对于目标状态 $s_9$，我们可以指定$\mathcal{A}(s_{9})=\{a_{5}\}$或设置$\mathcal{A}(s_9)=\{a_1,\ldots,a_5\}$，对于所有 $i=1,\cdots,5$，有$p(s9|s9,a_i)= 1$。

- 第二，我们可以将终止状态视为一种普通状态，将其行动空间设置为与其他状态相同，此时智能体可能会离开该状态并再次回来。由于每次到达$s_9$都能获得$r=1$的正奖励，智能体最终会学会永远停留在$s_9$以获得更多的奖励。值得注意的是，将回合制任务转换为持续性任务需要使用折扣因子，以避免回报趋于无穷。

在本书中，我们将考虑第二种情况，即目标状态被视为正常的状态，其行动空间为$\mathcal{A}(s_9)=\{a_1,\cdots,a_5\}$。因为这是一种更加一般化的情况，我们需要让智能体学习到在到达这个状态之后能够保持原地不动。