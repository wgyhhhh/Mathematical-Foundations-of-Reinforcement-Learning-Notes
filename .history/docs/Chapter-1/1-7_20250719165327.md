## 马尔科夫决策过程(Markov decision processes)

本章前几节通过实例说明了强化学习的一些基本概念。本节将在马尔可夫决策过程(MDP)的框架下，以更正式的方式介绍这些概念。

MDP是描述随机动态系统的一般框架。MDP 的关键要素如下：

1. 集合:
     - 状态空间:状态的结合，记为$\mathcal{S}$。
     - 行动空间:行动的集合，记为$\mathcal{A}(s)$，其中$s \in \mathcal{S}$。
     - 奖励集合:与每个状态-行动对$(s,a)$相关联的奖励集合，用$R(s，a)$表示。
2. 模型:
     - 状态转移概率: 在状态$s$下，当采取行动$a$时，转变为状态$s'$的概率为$p(s'|s,a)$。对于任意$(s,a)$有$\sum_{s^{\prime}\in\mathcal{S}}p(s^{\prime}|s,a)=1$。
     -  奖励概率: 在状态$s$下，当采取行动$a$时，获得奖励$r$的概率为$p(r|s,a)$。对于任意$(s,a)$有$\sum_{r\in\mathcal{R}(s,a)}p(r|s,a)=1$
3. 策略: 在状态$s$下，采取行动$a$的概率为$\pi(a|s).$对于任意$s \in \mathcal{S}$有$\sum_{r\in\mathcal{R}(s,a)}p(r|s,a)=1$。
4. 马尔可夫性质: 马尔可夫特性是指随机过程的无记忆特性。在数学上，它意味着

$$p(s_{t+1}|s_t,a_t,s_{t-1},a_{t-1},\ldots,s_0,a_0)=p(s_{t+1}|s_t,a_t),$$

$$p(r_{t+1}|s_t,a_t,s_{t-1},a_{t-1},\ldots,s_0,a_0)=p(r_{t+1}|s_t,a_t),\tag{1.4}$$