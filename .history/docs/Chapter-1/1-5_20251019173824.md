## 奖励(Reward)

**奖励** (Reward)是强化学习中最重要的概念之一。

在一个状态下执行一个行动后，智能体会从环境反馈中获得奖励$r$，其是状态$s$和行动$a$的函数，可以表示为$r(s,a)$，其数值可以为正数、负数或零。不同的奖励值对于智能体最终学习到的策略具有不同的影响。通常来说，正奖励表示鼓励智能体采取相应行动；负奖励不鼓励智能体采取该行动。

在所用的网格世界例子中，奖励可以设计如下:

- 如果智能体尝试跃出四周边界，则$r_\text{boundary}=-1$
  
- 如果智能体尝试进入禁区，则$r_\text{forbidden}=-1$
  
- 如果智能体到达目标区域，则$r_\text{target}=+1$
  
- 其他情况下，智能体获得的奖励为$r_\text{other}=0$

读者应该注意目标状态$s_9$，当到达该状态之后，它也许会持续执行策略，进而继续获得相应奖励。例如，如果智能体在状态$s_9$采取行动$a_5$ (静止不动)，下一个状态还是$s_9$，此时会获得奖励$r_\text{target}=+1$。如果智能体在状态$s_9$执行行动$a_2$，下一个状态也是$s_9$，但奖励是$r_\text{boundary}=+1$。

奖励可以被理解为一种人机交互 (human-machine interface)的重要手段，我们用它来引导智能体按照我们的期望行事。例如，通过上面奖励设置，可以让智能体避免越出边界或踏入禁区，力争进入目标区域。设计合适的奖励是强化学习的重要一步。不过对于复杂的任务来说，这一环节并不简单，因为它总是需要用户对给定问题有很好的理解。尽管如此，设计奖励可能仍然比使用其他专业工具来设计策略容易得多，这也可能是为什么强化学习受众广的原因。

奖励的过程可以直观地表示为一个表格，如$1.3$所示。表格的每一行对应一个状态，每列对应一个行动。表中每个单元格中的值表示在该状态下采取该行动所能获得的奖励。初学者可能会有这样一个问题：如果给定了奖励表格，我们是否能通过简单地选择奖励最大的行动来找到好的策略吗？答案是否定的。这是因为这些奖励都是**即时奖励** (immediate reward)，即在采取一个行动后立即获得的奖励。如果要确定一个好的政策，那么必须考虑更长远的**总奖励** (total reward)(更多信息请参见第$1.6$节)。具有最大即时奖励的行动不一定会带来最大的总奖励。

表格表示法虽然直观，但只能描述确定性的奖励过程。为了描述更加一般化的奖励过程，我们可以使用条件概率$p(r|s,a)$来描述在状态$s$采取行动$a$得到奖励$r$的概率。例如，对于状态$s_1$，有:

$$p(r=-1|s_{1},a_{1})=1,\quad p(r\neq-1|s_{1},a_{1})=0.$$

 ![](../img/01/7.png)
 >  表1.3: 奖励的表格表示法。

这意味着，当在状态$s_1$处采取行动$a_1$，智能体肯定会获得$r=-1$，而得到其他奖励值的概率为$0$。这个奖励是确定性的，因此既可以用表格也可以使用条件概率来描述。然而如果奖励过程是随机的，那么表格表示法也将不再适用。例如$p(r=-1\mid s_1,a_1)=0.5,p(r=-2\mid s_1,a_1)=0.5$，即各有$0.5$的概率获得$-1$或$-2$的奖励。值得强调的是，本书中的网格世界考虑的只是确定性的奖励过程。给出一个简单的例子帮助读者理解奖励为什么可能是随机的，例如，一个学习努力的学生，他/她会收到一个正向的奖励(例如在考试中取得更高的成绩)，但是奖励的值是不确定的，可能是100分，也可能是90分。