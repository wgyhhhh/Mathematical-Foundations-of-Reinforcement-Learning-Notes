## 轨迹、回报、回合(Trajectories, returns, episodes)

 ![](../img/01/8.png)
 > 图1.6: 根据两种策略所获得轨迹，轨迹在图中用红色虚线标出。

轨迹(trajectory)是一个状态-行动-奖励链。例如，给定一个图1.6所示的策略，智能体将会根据以下所示的轨迹进行移动: 

$$s_1\xrightarrow{a_2}s_2\xrightarrow{a_3}s_5\xrightarrow{a_3}s_8\xrightarrow{a_2}s_9.$$

这条轨迹的回报(return)被定义为沿轨迹收集的所有奖励的总和：

$$\mathrm{return}_1=0+0+0+1=1.$$

回报也称为总回报(total rewards)或累计回报(cumulative rewards)。

回报可以被用来评估策略。例如，我们可以通过比较图1.6两个策略的回报。具体而言，从$s_1$开始，左侧策略获得的回报为$1$，计算结果如上。对于右策略，从$s_1$开始，对应的轨迹如下: 

$$s_1\xrightarrow{a_3}s_4\xrightarrow{a_3}s_7\xrightarrow{a_2}s_8\xrightarrow{a_2}s_9.$$

对应的回报为:

$$\mathrm{return}_2=0-1+0+1=0.$$

$\mathrm{return}_1$和$\mathrm{return}_2$表明左边的策略比右边策略更好。这一数学结论与直觉是一致的，即右边的策略更糟糕，因为它穿过了一个禁区。

回报包括即时回报(immediate reward)和未来回报(future reward)。在这里，即时回报是指在初始状态下采取行动后获得的回报；未来回报是指离开初始状态后获得的回报。即时回报有可能是负数，而未来回报有可能是正数。因此应根据回报（即总回报）而不是即时回报来决定采取何种行动，以避免短视决策。

$return_1$是针对有限长轨迹定义的。对于无限长的轨迹，返回也可以定义。例如，图1.6中的轨迹在到达$s_9$后停止。由于策略对$s_9$已定义明确，因此在智能体到达$s_9$后，进程不必停止。我们可以设计一种策略，使代理在到达$s_9$后保持不动。那么，该策略将产生以下无限长的轨迹：

$$s_{1}\overset{a_{2}}{\operatorname*{\longrightarrow}}s_{2}\overset{a_{3}}{\operatorname*{\longrightarrow}}s_{5}\overset{a_{3}}{\operatorname*{\longrightarrow}}s_{8}\overset{a_{2}}{\operatorname*{\longrightarrow}}s_{9}\overset{a_{5}}{\operatorname*{\longrightarrow}}s_{9}\overset{a_{5}}{\operatorname*{\longrightarrow}}s_{9}\ldots$$

这条轨迹的回报之和为:

$$\mathrm{return}=0+0+0+1+1+1+\cdots=\infty,$$

但不幸的是，它是发散的。因此，我们必须引入无限长轨迹的贴现回报(discounted return)概念。具体来说，贴现回报是贴现回报的总和：

$$\text{discounted return}=0+\gamma0+\gamma^20+\gamma^31+\gamma^41+\gamma^51+\ldots,\tag{1.3}$$

在这里$\gamma \in (0,1)$被叫做贴现率(discount rate)。当$\gamma \in (0,1)$，上式的





