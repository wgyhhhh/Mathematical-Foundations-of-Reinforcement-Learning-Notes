---
title: 1.7 马尔科夫决策过程(Markov decision processes)
comments: true  # 开启评论
---
本章前几节通过例子直观介绍了强化学习的一些基本概念。本节将在**马尔可夫决策过程** (MDP)的框架下，以更加正式的方式介绍这些概念。

马尔可夫决策过程是描述随机动态系统的一般框架，其并不仅仅局限于强化学习。马尔可夫决策过程的涉及以下关键要素：

1. 集合:
     - 状态空间:状态的结合，记为$\mathcal{S}$。
     - 行动空间:行动的集合，记为$\mathcal{A}(s)$，其中$s \in \mathcal{S}$。
     - 奖励集合:与每个状态-行动对$(s,a)$相关联的奖励集合，用$\mathcal{R}(s，a)$表示。
2. 模型:
     - 状态转移概率: 在状态$s$采取行动$a$时，智能体转变为状态$s'$的概率为$p(s'|s,a)$。对于任意$(s,a)$有$\sum_{s^{\prime}\in\mathcal{S}}p(s^{\prime}\mid s,a)=1$。
     -  奖励概率: 在状态$s$下，当采取行动$a$时，智能体获得奖励$r$的概率为$p(r|s,a)$。对于任意$(s,a)$有$\sum_{r\in\mathcal{R}(s,a)}p(r|s,a)=1$
3. 策略: 在状态$s$，智能体采取行动$a$的概率为$\pi(a|s).$对于任意$s \in \mathcal{S}$有$\sum_{a\in\mathcal{A}(s)}p(a\mid s)=1$。
4. 马尔可夫性质: **马尔可夫性质** (Markov property)是指随机过程的无记忆性质。在数学上表示为：

$$p(s_{t+1}|s_t,a_t,s_{t-1},a_{t-1},\ldots,s_0,a_0)=p(s_{t+1}|s_t,a_t),$$

$$p(r_{t+1}|s_t,a_t,s_{t-1},a_{t-1},\ldots,s_0,a_0)=p(r_{t+1}|s_t,a_t),\tag{1.4}$$

其中$t$代表当前时刻，$t+1$代表下一个时刻。式子(1.4)表明下一个状态和奖励仅依赖于当前时刻的状态和行动，而与之前的状态和行动无关。马尔可夫特性对于推导MDP的贝尔曼方程非常重要，在下章我们会详细讨论。

在马尔科夫决策过程中，$p(s'|s,a)$和$p(r|s,a)$被称为**模型** (model)或**动态** (dynamics)。模型可以是**平稳的** (stationary)，也可以是**非平稳的** (nonstationary)：平稳模型不会随时间变化；而非平稳模型会随时间变化。例如，在网格世界的例子中，如果一个禁区时而出现时而消失，那么所对应的状态转移或者奖励就会随时间而变化，此时系统是非平稳的，本书只考虑平稳的情况。

读者可能听说过**马尔可夫过程**(Markov process, MP)。“马尔可夫决策过程”和“马尔可夫过程”有什么区别呢？答案是：一旦在马尔可夫决策过程中的策略被确定下来了，马尔可夫决策过程就退化成了一个马尔可夫过程。例如，图$1.7$中的网格世界示例可以被抽象成一个马尔可夫过程。在本书中主要考虑有限的马尔科夫决策过程，即状态和行动的数量都是有限的。“马尔可夫过程 ”和 “马尔可夫链 ”这两个术语可以互换使用。

 ![](../img/01/9.png)
 > 图1.7: 将网格世界示例抽象为马尔可夫过程。在这里，圆圈代表状态，带箭头的链接代表状态转移。

强化学习涉及智能体与环境的交互。智能体之外的一切都被视为环境。第一，智能体可以理解并感知当前的状态。第二，智能体可以了解在什么状态应该采取什么行动。第三，智能体能够执行策略所指示的动作，从而改变状态获得奖励。
---