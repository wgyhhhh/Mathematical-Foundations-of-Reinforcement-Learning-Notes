---
title: 9.3 目标函数的梯度
comments: true  # 开启评论
---
根据上一节介绍的指标，我们可以使用基于梯度的方法来最大化它们。要做到这一点，我们需要首先计算这些指标的梯度。本章最重要的理论结果是下面的定理。

!!! info
    **定理9.1**(策略梯度定理). 目标函数 $J(\theta)$的梯度为

    $$\nabla_\theta J(\theta) = \sum_{s\in\mathcal{S}} \eta(s) \sum_{a\in\mathcal{A}} \nabla_\theta \pi(a|s, \theta) q_\pi(s, a),\tag{9.8}$$

    其中$\eta$为稳态分布，$\nabla_\theta \pi$表示策略$\pi$对参数$\theta$的梯度。此外，$(9.8)$式可简化为期望形式：

    $$\nabla_\theta J(\theta) = \mathbb{E}_{S\sim\eta,A\sim\pi(S,\theta)} \left[ \nabla_\theta \ln \pi(A|S, \theta) q_\pi(S, A) \right],\tag{9.8}$$

    式中$\ln$为自然对数。

关于定理9.1的重要说明如下。

- 需注意的是，定理$9.1$是对定理$9.2$,定理$9.3$及定理$9.5$结果的总结。这三个定理分别处理涉及不同度量指标与贴现/非贴现情形的不同场景。这些场景下的梯度表达式均具有相似形式，因此统一归纳于定理$9.1$中。定理$9.1$未给出$J(\theta)$与$\eta$的具体表达式，其详细定义可参见定理$9.2$,定理$9.3$和定理$9.5$。特别地，$J(\theta)$可能是$\bar{v}_0^\pi$,$\bar{v}^\pi$或$\bar{r}^\pi$。式$(9.8)$中的等式可能转化为严格等式或近似关系。分布$\eta$在不同场景下也存在差异。

    梯度推导是策略梯度方法中最复杂的环节。对于多数读者而言，掌握定理$9.1$的结论即可，无需了解证明过程。本节后续推导涉及密集的数学运算，建议读者根据兴趣选择性研读。

- 式$(9.9)$比式$(9.8)$更具优势，因其表述为期望形式。我们将在第$9.4$节证明：该真实梯度可通过随机梯度进行近似。

    为什么$(9.8)$式可以表示为$(9.9)$式？证明如下。根据期望的定义，$(9.8)$式可改写为

    $$\begin{aligned}\nabla_{\theta}J(\theta)&=\sum_{s\in\mathcal{S}}\eta(s)\sum_{a\in\mathcal{A}}\nabla_{\theta}\pi(a|s,\theta)q_{\pi}(s,a)\\&=\mathbb{E}_{S\sim\eta}\left[\sum_{a\in\mathcal{A}}\nabla_{\theta}\pi(a|S,\theta)q_{\pi}(S,a)\right].\end{aligned}\tag{9.10}$$

    此外，$\ln \pi(a|s, \theta)$的梯度为

    $$\nabla_\theta\ln\pi(a|s,\theta)=\frac{\nabla_\theta\pi(a|s,\theta)}{\pi(a|s,\theta)}.$$

    由此可得

    $$\nabla_\theta\pi(a|s,\theta)=\pi(a|s,\theta)\nabla_\theta\ln\pi(a|s,\theta)\tag{9.11}$$

    将式$(9.11)$代入式$(9.10)$可得

    $$\begin{aligned}\nabla_{\theta}J(\theta)&=\mathbb{E}\left[\sum_{a\in\mathcal{A}}\pi(a|S,\theta)\nabla_{\theta}\ln\pi(a|S,\theta)q_{\pi}(S,a)\right]\\&=\mathbb{E}_{S\sim\eta,A\sim\pi(S,\theta)}\left[\nabla_{\theta}\ln\pi(A|S,\theta)q_{\pi}(S,A)\right].\end{aligned}$$

- 值得注意的是，为确保$\ln \pi(a|s, \theta)$有效，策略函数$\pi(a|s, \theta)$对所有状态-行动对$(s, a)$必须保持正值。这一特性可通过softmax函数实现：

    $$\pi(a|s,\theta)=\frac{e^{h(s,a,\theta)}}{\sum_{a^{\prime}\in\mathcal{A}}e^{h(s,a^{\prime},\theta)}},\quad a\in\mathcal{A},\tag{9.12}$$

    其中$h(s, a, \theta)$是表征在状态$s$下选择行动$a$偏好的函数。式$(9.12)$中的策略满足$\pi(a|s, \theta) \in (0,1)$且对任意$s \in \mathcal{S}$有$\sum_{a \in \mathcal{A}} \pi(a|s, \theta) =1$。该策略可通过神经网络实现：网络输入为状态$s$，输出层采用 softmax层，使得网络对所有$a$输出$\pi(a|s, \theta)$且输出总和为$1$。具体结构示意图见图$9.2(b)$。

    由于对所有行动$a$均有$\pi(a|s, \theta) >0$，该策略是随机且具有探索性的。策略本身并不直接指定应采取的动作，而是需要根据策略的概率分布生成动作。

### 9.3.1 推导策略梯度: 有折现的情况

接下来推导折现因子$\gamma\in(0,1)$情形下的指标梯度。折现情况下的状态值与动作值定义为:

$$\begin{aligned}v_{\pi}(s)&=\mathbb{E}[R_{t+1}+\gamma R_{t+2}+\gamma^{2}R_{t+3}+\ldots|S_{t}=s],\\q_{\pi}(s,a)&=\mathbb{E}[R_{t+1}+\gamma R_{t+2}+\gamma^{2}R_{t+3}+\ldots|S_{t}=s,A_{t}=a].\end{aligned}$$

状态值满足$v_\pi(s) = \sum_{a\in A} \pi(a|s, \theta)q_\pi(s, a)$，且该状态值服从贝尔曼方程。

首先，我们证明$\bar{v}_\pi(\theta)$与$\bar{r}_\pi(\theta)$是等价度量。

!!! info
    **引理9.1**.($\bar{v}_\pi(\theta)$与 $\bar{r}_\pi(\theta)$的等价性)。在折扣因子 $\gamma \in (0,1)$的情形下，成立关系式
    
    $$\bar{r}_\pi = (1 - \gamma)\bar{v}_\pi. \quad (9.13)$$
    
    **证明**：注意到 $\bar{v}_\pi(\theta) = d_\pi^T v_\pi$且 $\bar{r}_\pi(\theta) = d_\pi^T r_\pi$，其中 $v_\pi$和 $r_\pi$满足Bellman方程$v_\pi = r_\pi + \gamma P_\pi v_\pi$。将Bellman方程两边同乘$d_\pi^T$可得
    
    $$\bar{v}_\pi = \bar{r}_\pi + \gamma d_\pi^T P_\pi v_\pi = \bar{r}_\pi + \gamma d_\pi^T v_\pi = \bar{r}_\pi + \gamma \bar{v}_\pi,$$由此即得$(9.13)$。

其次，以下引理给出任意状态$s$下$v_\pi(s)$的梯度。

!!! info 
    **引理9.2**.($v_\pi(s)$的梯度)。在折现情形下，对于任意状态$s \in \mathcal{S}$，有：
$$\nabla_\theta v_\pi(s) = \sum_{s' \in \mathcal{S}} \Pr\nolimits_\pi(s'|s) \sum_{a \in \mathcal{A}} \nabla_\theta \pi(a|s', \theta) q_\pi(s', a),\tag{9.14}$$

其中

$$\Pr\nolimits_\pi(s'|s) \doteq \sum_{k=0}^\infty \gamma^k [P_\pi^k]_{ss'} = \left[ (I_n - \gamma P_\pi)^{-1} \right]_{ss'}$$

表示策略$\pi$下从$s$转移到$s'$的折现总概率。此处$[\cdot]_{ss'}$表示矩阵第$s$行第$s'$列元素，$[P_\pi^k]_{ss'}$为策略$\pi$下经$k$步从$s$转移到$s'$的概率。

根据引理$9.2$的结果，我们可以推导出$\bar{v}_0^\pi$的梯度。

!!! info
    **定理9.2**.(折扣情形下的$\bar{v}_0^\pi$梯度)。在折现因子$\gamma \in (0,1)$的情形下，$\bar{v}_0^\pi = \mathbf{d}_0^\mathrm{T} \mathbf{v}^\pi$的梯度为
    
    $$\nabla_\theta \bar{v}_0^\pi = \mathbb{E} \left[ \nabla_\theta \ln \pi(A|S, \theta) q_\pi(S, A) \right],$$

    其中状态$S \sim \rho^\pi$且行动$A \sim \pi(S, \theta)$。此处状态分布$\rho^\pi$定义为
    
    $$\rho^\pi(s) = \sum_{s' \in \mathcal{S}} d_0(s') \mathrm{Pr}^\pi(s|s'), \quad s \in \mathcal{S},\tag{9.19}$$


    其中$\mathrm{Pr}^\pi(s|s') = \sum_{k=0}^\infty \gamma^k [P_\pi^k]_{s's} = [(I - \gamma P_\pi)^{-1}]_{s's}$在策略$\pi$下从状态$s'$转移到状态$s$的折现总概率。

根据引理$9.1$和引理$9.2$，我们可以推导出$\bar{r}_\pi$和$\bar{v}_\pi$的梯度。

!!! info
    **定理9.3**.(折扣情形下的$\bar{r}_\pi$与$\bar{v}_\pi$梯度)。在折x现因子$\gamma \in (0,1)$的情形下，$\bar{r}_\pi$和$\bar{v}_\pi$的梯度满足：

    $$\nabla_\theta \bar{r}_\pi = (1 - \gamma) \nabla_\theta \bar{v}_\pi \approx \sum_{s \in \mathcal{S}} d_\pi(s) \sum_{a \in \mathcal{A}} \nabla_\theta \pi(a|s, \theta) q_\pi(s, a) = \mathbb{E} \left[ \nabla_\theta \ln \pi(A|S, \theta) q_\pi(S, A) \right],$$

    其中$S \sim d_\pi$且$A \sim \pi(S, \theta)$。当$\gamma$越接近1时，该近似结果越精确。

### 9.3.2 推导策略梯度: 无折现的情况

接下来我们将展示如何计算无折扣情形(即$\gamma =1$)下各指标的梯度。读者可能会疑惑，为何本书此前仅讨论折现情形，却突然开始分析无折现情形。事实上，平均奖励$\bar{r}_\pi$的定义同时适用于折现与无折现情形。虽然折现情形下 $\bar{r}_\pi$的梯度是近似解，但我们将看到其在无折现情形下的梯度表达式更为简洁优美。

#### 状态值与泊松方程

在无折扣情况下，需重新定义状态值与动作值。由于奖励的无折扣总和$\mathbb{E}[R_{t+1} + R_{t+2} + R_{t+3} + \dots |S_t = s]$可能发散，状态值与动作值需采用特殊方式定义[64]：

$$\begin{aligned}v_{\pi}(s)&\doteq\mathbb{E}[(R_{t+1}-\bar{r}_\pi)+(R_{t+2}-\bar{r}_\pi)+(R_{t+3}-\bar{r}_\pi)+\ldots|S_t=s],\\q_{\pi}(s,a)&\doteq\mathbb{E}[(R_{t+1}-\bar{r}_\pi)+(R_{t+2}-\bar{r}_\pi)+(R_{t+3}-\bar{r}_\pi)+\ldots|S_t=s,A_t=a],\end{aligned}$$

其中$\bar{r}_\pi$表示平均奖励，其值在给定策略$\pi$时确定。文献中对$v_\pi(s)$有不同称谓，如"差分奖励"[65]或"偏差"[2,第8.2.1节]。可以验证，上述定义的状态值函数满足如下类贝尔曼方程：

$$v_\pi(s)=\sum_a\pi(a|s,\theta)\left[\sum_rp(r|s,a)(r-\bar{r}_\pi)+\sum_{s^{\prime}}p(s^{\prime}|s,a)v_\pi(s^{\prime})\right].\tag{9.22}$$

由于$v_\pi(s) = \sum_{a \in \mathcal{A}} \pi(a|s, \theta) q_\pi(s, a)$，可得$q_\pi(s, a) = \sum_{r} p(r|s, a)(r - \bar{r}_\pi) + \sum_{s'} p(s'|s, a)v_\pi(s')$。式 (9.22)的矩阵-向量形式为

$$v_\pi = r_\pi - \bar{r}_\pi \mathbf{1}_n + P_\pi v_\pi,\tag{9.23}$$

其中$\mathbf{1}_n = [1, \ldots,1]^T \in \mathbb{R}^n$。方程$(9.23)$与贝尔曼方程类似，其特定名称为泊松方程[65,67]。

如何从泊松方程求解$v_\pi$？答案由以下定理给出。

!!! info
    **定理9.4**(泊松方程的解)。令$$v^*_\pi = (I_n - P_\pi +1_n d^T_\pi)^{-1} r_\pi.\tag{9.24}$$

    则$v^*_\pi$是方程$(9.23)$中泊松方程的一个解。此外，泊松方程的任意解均可表示为以下形式：
    $$v_\pi = v^*_\pi + c1_n,$$

    其中$c \in \mathbb{R}$。

该定理表明泊松方程的解可能不唯一。

#### 梯度推导

虽然在无折现情形下 $v_\pi$的值不唯一(如定理$9.4$所示)，但 $\bar{r}_\pi$的值具有唯一性。特别地，根据泊松方程可得：

$$\bar{r}_\pi \mathbf{1}_n = r_\pi + (P_\pi - I_n) v_\pi = r_\pi + (P_\pi - I_n)(v^*_\pi + c\mathbf{1}_n) = r_\pi + (P_\pi - I_n) v^*_\pi$$

值得注意的是，未定值$c$被消去，因此$\bar{r}_\pi$具有唯一性。这使得我们能够计算无折现情形下$\bar{r}_\pi$的梯度。此外，由于$v_\pi$不唯一，$\bar{v}_\pi$也不唯一。本文不研究无折现情形下$\bar{v}_\pi$的梯度。对于感兴趣的读者，需要说明的是：我们可以通过增加约束条件从泊松方程中唯一求解$v_\pi$。例如，假设存在常返状态时，该常返状态的状态值可被确定[65第II节]，从而确定$c$值。还存在其他唯一确定$v_\pi$的方法，具体可参见文献[2]中的方程(8.6.5)-(8.6.7)。

无折现情形下$\bar{r}_\pi$的梯度推导如下。

!!! info
    **定理9.5**(无折现情形下$\bar{r}_\pi$的梯度)。

    平均奖励$\bar{r}_\pi$的梯度为
    
    $$\nabla_\theta \bar{r}_\pi = \sum_{s \in \mathcal{S}} d_\pi(s) \sum_{a \in \mathcal{A}} \nabla_\theta \pi(a|s, \theta) q_\pi(s, a) = \mathbb{E} \left[ \nabla_\theta \ln \pi(A|S, \theta) q_\pi(S, A) \right],\tag{9.28}$$

    其中状态$S \sim d_\pi$且行动$A \sim \pi(S, \theta)$。

与定理$9.3$描述的折扣情形相比，非折扣情形下的$\bar{r}_\pi$梯度具有更简洁的数学表达——其严格满足式$(9.28)$，且状态转移矩阵$S$服从稳态分布。

---