---
title: 9.2 目标函数: 定义最优策略
comments: true  # 开启评论
---
若策略由函数表示，则定义最优策略的指标可分为两类：一类基于状态值，另一类基于即时奖励。

#### **指标1：平均状态值(Average state value)**

第一个指标是平均状态值，简称为平均值。其定义为

$$\bar{v}_\pi=\sum_{s\in\mathcal{S}}d(s)v_\pi(s),$$

其中$d(s)$表示状态$s$的权重。对于任意$s \in S$，满足$d(s) \geq0$且$\sum_{s\in S} d(s) =1$。因此，可将$d(s)$解释为$s$的概率分布。此时该指标可表示为

$$\bar{v}_\pi=\mathbb{E}_{S\sim d}[v_\pi(S)].$$

如何选择$d$的概率分布？这是一个关键问题。存在以下两种情况。

- 最简单的情形是$d$与策略$\pi$无关。此时，我们特地将$d$记为$d_0$，并将$\bar{v}_\pi$记为$\bar{v}^0_\pi$以表明该分布与策略无关。一种处理方式是赋予所有状态同等重要性，即设定$d_0(s) =1/|\mathcal{S}|$；另一种情形是当我们仅关注特定状态$s_0$时（例如智能体总是从$s_0$开始），此时可设计

    $$d_0(s_0)=1,\quad d_0(s\neq s_0)=0.$$

- 第二种情况是$d$依赖于策略$\pi$。此时通常选择$d$为$d_\pi$，即$\pi$下的稳态分布。$d^\pi$的一个基本性质是满足

    $$d_\pi^TP_\pi=d_\pi^T,$$

    其中$P_\pi$为状态转移概率矩阵。关于平稳分布的更多信息可参阅Box $8.1$。

    选择$d^\pi$的解释如下：平稳分布反映了马尔可夫决策过程在给定策略下的长期行为。若某一状态在长期中被频繁访问，则其重要性更高，应赋予更大权重；若某状态极少被访问，则其重要性较低，应赋予较小权重。

顾名思义，$\bar{v}_\pi$是状态值的加权平均。不同的$\theta$取值会导致$\bar{v}_\pi$取值的差异。我们的最终目标是找到最优策略（即最优参数$\theta$）以使$\bar{v}_\pi$最大化。

接下来我们引入$\bar{v}_\pi$的另外两种重要等价表达式。


- 假设智能体通过遵循给定策略 $\pi(\theta)$收集奖励序列 $\{R_{t+1}\}_{t=0}^\infty$。读者在文献中常会见到如下度量指标：

    $$J(\theta)=\lim_{n\to\infty}\mathbb{E}\left[\sum_{t=0}^n\gamma^tR_{t+1}\right]=\mathbb{E}\left[\sum_{t=0}^\infty\gamma^tR_{t+1}\right].\tag{9.1}$$

    该度量指标初看可能不易理解。实际上，它等于 $\bar{v}_\pi$。为证明这一点，我们有

    $$\begin{aligned}\mathbb{E}\left[\sum_{t=0}^{\infty}\gamma^{t}R_{t+1}\right]&=\sum_{s\in\mathcal{S}}d(s)\mathbb{E}\left[\sum_{t=0}^{\infty}\gamma^{t}R_{t+1}|S_{0}=s\right]\\&=\sum_{s\in\mathcal{S}}d(s)v_{\pi}(s)\\&=\bar{v}_{\pi}.\end{aligned}$$

    上式中的第一个等式由全期望定律得出；第二个等式则基于状态值的定义。

- 度量$\bar{v}_\pi$也可以表示为两个向量的内积。具体而言，设

    $$v_{\pi}=[\ldots,v_{\pi}(s),\ldots]^{T}\in\mathbb{R}^{|\mathcal{S}|},d=[\ldots,d(s),\ldots]^{T}\in\mathbb{R}^{|\mathcal{S}|}.$$

    于是我们得到

    $$\bar{v}_\pi=d^Tv_\pi.$$

    该表达式在分析其梯度时将会非常有用。

#### **指标2：平均奖励(Average reward)**

第二个指标是单步平均奖励(one-step reward，简称平均奖励)[2,64,65]，其定义为

$$\begin{gathered}\bar{r}_{\pi}\doteq\sum_{s\in\mathcal{S}}d_{\pi}(s)r_{\pi}(s)\\=\mathbb{E}_{S\sim d_{\pi}}[r_{\pi}(S)],\end{gathered}\tag{9.2}$$

其中$d_\pi$为平稳分布，且

$$r_\pi(s)\doteq\sum_{a\in\mathcal{A}}\pi(a|s,\theta)r(s,a)=\mathbb{E}_{A\sim\pi(s,\theta)}[r(s,A)|s]\tag{9.3}$$

这是即时奖励的期望值。其中，$r(s, a) = \mathbb{E}[R|s, a] = \sum rp(r|s, a)$。

接下来我们给出$\bar{r}_\pi$的另外两种重要等价表达式。

- 假设智能体通过遵循给定策略$\pi(\theta)$收集奖励序列$\{R_{t+1}\}_{t=0}^\infty$。文献中常见的一个评价指标可表示为
    
    $$J(\theta)=\lim_{n\to\infty}\frac{1}{n}\mathbb{E}\left[\sum_{t=0}^{n-1}R_{t+1}\right].\tag{9.4}$$

    乍看之下，这个度量的解释可能并不直观。实际上，它等于$\bar{r}_\pi$：

    $$\lim_{n\to\infty}\frac{1}{n}\mathbb{E}\left[\sum_{t=0}^{n-1}R_{t+1}\right]=\sum_{s\in\mathcal{S}}d_{\pi}(s)r_{\pi}(s)=\bar{r}_{\pi}.\tag{9.5}$$

    $(9.5)$式的证明见Box 9.1。

- 式$(9.2)$中的平均奖励$\bar{r}_\pi$也可表示为两个向量的内积。具体而言，设

    $$r_{\pi}=[\ldots,r_{\pi}(s),\ldots]^{T}\in\mathbb{R}^{|S|},d_{\pi}=[\ldots,d_{\pi}(s),\ldots]^{T}\in\mathbb{R}^{|S|},$$

    其中$r_\pi(s)$由式$(9.3)$定义。显然，

    $$\bar{r}_\pi=\sum_{s\in\mathcal{S}}d_\pi(s)r_\pi(s)=d_\pi^Tr_\pi.$$

    该表达式在推导其梯度时将非常有用。

#### **若干说明**

 ![](../img/09/1.png)

 > 表$9.2$：$\bar{v}_\pi$与$\bar{r}_\pi$不同但等价表达式的汇总。

截至目前，我们已经介绍了两种性能指标$\bar{v}_\pi$和$\bar{r}_\pi$。每种指标均有若干形式不同但等价的表达式，其总结见表$9.2$。我们有时用$\bar{v}_\pi$特指状态分布为平稳分布$d_\pi$的情形，并用$\bar{v}^0_\pi$表示$d_0$独立于$\pi$的情形。关于这些指标的补充说明如下：

- 所有这些指标都是$\pi$的函数。由于$\pi$由 $\theta$参数化，这些指标也是$\theta$的函数。换言之，不同的$\theta$值会生成不同的度量指标值。因此，我们可以通过搜索参数$\theta$的最优值来最大化这些评价指标。这正是策略梯度方法的基本思想。

- 在折扣因子$\gamma<1$的情况下，两个指标$\bar{v}_\pi$与$\bar{r}_\pi$是等价(而非相等)的。具体而言，可以证明：

    $$\bar{r}_{\pi}=(1-\gamma)\bar{v}_{\pi}.$$
    
    上述方程表明这两个指标可以同时最大化。该方程的证明将在后续引理9.1中给出。

!!! note
    $\bar{r}_\pi$似乎看起来更加短视，因为他只考虑即时奖励，但是$\bar{v}_\pi$考虑整个步骤的总回报。

---