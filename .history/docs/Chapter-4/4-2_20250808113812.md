## 策略迭代

本节介绍另一种重要的算法：策略迭代。与值迭代不同，策略迭代并不是直接求解贝尔曼最优公式。然而，它与值迭代有着密切的关系。策略迭代的思想非常重要，其广泛应用于强化学习算法中。

### 4.2.1 算法分析

策略迭代是一种迭代算法。每次迭代包含两个步骤。

- 第一步是策略评估(policy evaluation)步骤。顾名思义，这一步通过计算相应的状态值来评估给定的策略。即求解以下贝尔曼方程：

    $$v_{\pi_k}=r_{\pi_k}+\gamma P_{\pi_k}v_{\pi_k},\tag{4.3}$$
    
    其中，$\pi_k$是上一次迭代中得到的策略，$v_{\pi_k}$是待计算的状态值。$r_{\pi_k}$和$P_{\pi_k}$的值可从系统模型中获得。

- 第二个是策略改进(policy improvement)步骤。顾名思义，这一步用于改进策略。具体而言，一旦在第一步中计算出$v_{\pi_k}$，就可以得到一个新的策略$\pi_{k+1}$，如下所示：

    $$\pi_{k+1}=\arg\max_\pi(r_\pi+\gamma P_\pi v_{\pi_k}).$$

在上述算法描述之后，自然会引出三个问题。

- 在策略评估步骤中，如何求解状态值 $v_{\pi_k}$？ 
- 在策略改进步骤中，为什么新策略$π_{k+1}$比$\pi_k$更优？ 
-  为什么该算法最终能够收敛到最优策略？
  
接下来，我们将逐一回答这些问题。

#### 在策略评估步骤中，如何求解状态值 $v_{\pi_k}$？ 

我们在第$2$章中介绍了两种求解式$(4.3)$中的贝尔曼公式的方法。接下来我们将简要回顾这两种方法。第一种方法是封闭式解：$v_{\pi_k}=(I-\gamma P_{\pi_k})^{-1}r_{\pi_k}$。这种封闭形式的解在理论分析中很有用，但由于计算逆矩阵，因此实现起来效率较低。第二种方法是一种易于实现的迭代算法：

$$v_{\pi_k}^{(j+1)}=r_{\pi_k}+\gamma P_{\pi_k}v_{\pi_k}^{(j)},\quad j=0,1,2,...,\tag{4.4}$$

其中，$v^{(j)}_{π_k}$表示$v_{\pi_k}$的第$j$次估计。从任意初始猜测$v^{(0)}_{\pi_k}$开始，可以保证当$j \rightarrow \infty$时，$v^{(j)}_{\pi_k}\rightarrow v_{\pi_k}$。具体细节请参见第$2.7$节。

有趣的是，策略迭代是一个迭代算法，其中嵌入了另一个迭代算法$(4.4)$，该算法在策略评估步骤中执行。从理论上讲，这个嵌入的迭代算法需要无限次的步骤(即 $j \to \infty$)才能收敛到真实的状态值 $v_{\pi_k}$。然而，这是不可能实现的。在实际操作中，迭代过程会在满足某个标准时终止。例如，终止标准可以是 $\| v_{\pi_k}^{(j+1)} - v_{\pi_k}^{(j)}\|$小于预设的阈值，或者$j$超过预设的值。如果我们不进行无限次的迭代，我们只能得到一个不精确的$v_{\pi_k}$值，这个值将在随后的策略改进步骤中使用。这会造成问题吗？答案是否定的。原因将在我们后面介绍的截断策略迭代算法($4.3$节)中变得清晰。

#### 在策略改进步骤中，为什么新策略$π_{k+1}$比$\pi_k$更优？ 

策略改进步骤可以改进给定的策略，如下所示。

!!! note
    **引理4.1**(策略改进). 若 $\pi_{k+1} = \arg\max_\pi(r_\pi + \gamma P_\pi v_\pi^k)$，则$v_{\pi_{k+1}} \geq v_{\pi_k}$。

这里，$v_{\pi_{k+1}} 
\geq v_{\pi_k}$表示对于所有状态$s$，有$v_{\pi_{k+1}}(s) ≥ v_{\pi_k}(s)$。该引理的证明见方框 4.1。

#### 为什么策略迭代算法最终能够找到最优策略？

策略迭代算法生成两个序列。第一个是策略序列：$\{\pi_0,\pi_1,...,\pi_k, ...\}$。第二个是状态值序列：$\{v_{\pi_0}, v_{\pi_1},...,v_{\pi)k},...\}$。假设$v^*$是最佳状态值，则对于所有$k$，有$v_{\pi_k}\leq v^*$。由于根据引理4.1，策略不断改进，因此我们知道

$$v_{\pi_0}\leq v_{\pi_1}\leq v_{\pi_2}\leq\cdots\leq v_{\pi_k}\leq\cdots\leq v^*.$$

由于$v_{\pi_k}$是非递减的，并且始终被$v^∗$上界所限制，因此根据单调收敛定理[12](附录 C)，当$k\rightarrow\infty$ 时，$v_{\pi_k}$收敛到一个常数值，记为 $v_\infty$。以下分析表明$v_\infty=v^*$。

!!! note
    **定理4.1**. (策略迭代的收敛性)由策略迭代算法生成的状态值序列$\{v_{\pi_k}\}_{k=0}^{\infty}$收敛到最优状态值$v^∗$。因此，策略序列$\{\pi_k\}^\infty_{k=0}$收敛到一个最优策略。

该定理的证明见方框$4.2$。该证明不仅表明了策略迭代算法的收敛性，还揭示了策略迭代算法与值迭代算法之间的关系。从直观上讲，如果两种算法均从相同的初始猜测开始，由于策略评估步骤中嵌入了额外的迭代，策略迭代的收敛速度将比值迭代更快。当我们将在第$4.3$节中介绍截断策略迭代算法时，这一点将变得更加清晰。

### 4.2.2 逐元素形式与实现

为了实现策略迭代算法，我们需要研究其逐元素形式。

- 首先，策略评估步骤通过(4.4)中的迭代算法来求解$v_{\pi_k} = r_{\pi_k} + \gamma P_{\pi_k}v_{\pi_k}$中的$v_{\pi_k}$，该算法的逐元素形式为

    $$v_{\pi_{k}}^{(j+1)}(s)=\sum_{a}\pi_{k}(a|s)\left(\sum_{r}p(r|s,a)r+\gamma\sum_{s^{\prime}}p(s^{\prime}|s,a)v_{\pi_{k}}^{(j)}(s^{\prime})\right),\quad s\in\mathcal{S},$$

    在这里$j=0,1,2,...$

- 其次，策略改进步骤求解$\pi_{k+1} = \arg \max_\pi(r_\pi + \gamma P_\pi v_{\pi_k})$。该方程的元素形式为

    $$\pi_{k+1}(s)=\arg\max_{\pi}\sum_{a}\pi(a|s)\underbrace{\left(\sum_{r}p(r|s,a)r+\gamma\sum_{s^{\prime}}p(s^{\prime}|s,a)v_{\pi_{k}}(s^{\prime})\right)}_{q_{\pi_{k}}(s,a)},\quad s\in\mathcal{S},$$

    其中 $q_{\pi_k}(s,a)$是策略$\pi_k$下的行动值。令$a^*_k(s) =\arg\max_a q_{\pi_k}(s,a)$。那么，Greedy最优策略为

    $$\pi_{k+1}(a|s)=\left\{\begin{array}{ll}1,&a=a_k^*(s),\\0,&a\neq a_k^*(s).\end{array}\right.$$

具体实现细节在算法$4.2$中总结。