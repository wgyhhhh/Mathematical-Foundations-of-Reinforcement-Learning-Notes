## 策略迭代

本节介绍另一种重要的算法：策略迭代。与值迭代不同，策略迭代并不是直接求解贝尔曼最优公式。然而，它与值迭代有着密切的关系。策略迭代的思想非常重要，其广泛应用于强化学习算法中。

### 4.2.1 算法分析

策略迭代是一种迭代算法。每次迭代包含两个步骤。

- 第一步是策略评估(policy evaluation)步骤。顾名思义，这一步通过计算相应的状态值来评估给定的策略。即求解以下贝尔曼方程：

    $$v_{\pi_k}=r_{\pi_k}+\gamma P_{\pi_k}v_{\pi_k},\tag{4.3}$$
    
    其中，$\pi_k$是上一次迭代中得到的策略，$v_{\pi_k}$是待计算的状态值。$r_{\pi_k}$和$P_{\pi_k}$的值可从系统模型中获得。

- 第二个是策略改进(policy improvement)步骤。顾名思义，这一步用于改进策略。具体而言，一旦在第一步中计算出$v_{\pi_k}$，就可以得到一个新的策略$\pi_{k+1}$，如下所示：

    $$\pi_{k+1}=\arg\max_\pi(r_\pi+\gamma P_\pi v_{\pi_k}).$$

在上述算法描述之后，自然会引出三个问题。

- 在策略评估步骤中，如何求解状态值 $v_{\pi_k}$？ 
- 在策略改进步骤中，为什么新策略$π_{k+1}$比$\pi_k$更优？ 
-  为什么该算法最终能够收敛到最优策略？
  
接下来，我们将逐一回答这些问题。

#### 在策略评估步骤中，如何求解状态值 $v_{\pi_k}$？ 

我们在第$2$章中介绍了两种求解式$(4.3)$中的贝尔曼公式的方法。接下来我们将简要回顾这两种方法。第一种方法是封闭式解：$v_{\pi_k}=(I-\gamma P_{\pi_k})^{-1}r_{\pi_k}$。这种封闭形式的解在理论分析中很有用，但由于计算逆矩阵，因此实现起来效率较低。第二种方法是一种易于实现的迭代算法：

$$v_{\pi_k}^{(j+1)}=r_{\pi_k}+\gamma P_{\pi_k}v_{\pi_k}^{(j)},\quad j=0,1,2,...,\tag{4.4}$$

其中，$v^{(j)}_{π_k}$表示$v_{\pi_k}$的第$j$次估计。从任意初始猜测$v^{(0)}_{\pi_k}$开始，可以保证当$j \rightarrow \infty$时，$v^{(j)}_{\pi_k}\rightarrow v_{\pi_k}$。具体细节请参见第$2.7$节。

有趣的是，策略迭代是一个迭代算法，其中嵌入了另一个迭代算法$(4.4)$，该算法在策略评估步骤中执行。从理论上讲，这个嵌入的迭代算法需要无限次的步骤(即 $j \to \infty$)才能收敛到真实的状态值 $v_{\pi_k}$。然而，这是不可能实现的。在实际操作中，迭代过程会在满足某个标准时终止。例如，终止标准可以是 $\| v_{\pi_k}^{(j+1)} - v_{\pi_k}^{(j)}\|$小于预设的阈值，或者$j$超过预设的值。如果我们不进行无限次的迭代，我们只能得到一个不精确的$v_{\pi_k}$值，这个值将在随后的策略改进步骤中使用。这会造成问题吗？答案是否定的。原因将在我们后面介绍的截断策略迭代算法($4.3$节)中变得清晰。