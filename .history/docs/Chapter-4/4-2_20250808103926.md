## 策略迭代

本节介绍另一种重要的算法：策略迭代。与值迭代不同，策略迭代并不是直接求解贝尔曼最优公式。然而，它与值迭代有着密切的关系。策略迭代的思想非常重要，其广泛应用于强化学习算法中。

### 4.2.1 算法分析

策略迭代是一种迭代算法。每次迭代包含两个步骤。

- 第一步是策略评估(policy evaluation)步骤。顾名思义，这一步通过计算相应的状态值来评估给定的策略。即求解以下贝尔曼方程：

    $$v_{\pi_k}=r_{\pi_k}+\gamma P_{\pi_k}v_{\pi_k},\tag{4.3}$$
    
    其中，$\pi_k$是上一次迭代中得到的策略，$v_{\pi_k}$是待计算的状态值。$r_{\pi_k}$和$P_{\pi_k}$的值可从系统模型中获得。

- 第二个是策略改进(policy improvement)步骤。顾名思义，这一步用于改进策略。具体而言，一旦在第一步中计算出$v_{\pi_k}$，就可以得到一个新的策略$\pi_{k+1}$，如下所示：

    $$\pi_{k+1}=\arg\max_\pi(r_\pi+\gamma P_\pi v_{\pi_k}).$$

在上述算法描述之后，自然会引出三个问题。

- 在策略评估步骤中，如何求解状态值 $v_{\pi_k}$？ 
- 在策略改进步骤中，为什么新策略$π_{k+1}$比$\pi_k$更优？ 
-  为什么该算法最终能够收敛到最优策略？
  
接下来，我们将逐一回答这些问题。

#### 在策略评估步骤中，如何求解状态值 $v_{\pi_k}$？ 

我们在第$2$章中介绍了两种求解式(4.3)中的贝尔曼方程的方法。接下来我们将简要回顾这两种方法。第一种方法是闭式解：