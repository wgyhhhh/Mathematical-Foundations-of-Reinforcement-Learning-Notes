---
title: 总结
comments: true  # 开启评论
---
本章第一次介绍了三种可用于寻找最优策略的算法。

- 值迭代(Value Iteration)：该算法就是求解贝尔曼最优方程的算法。它的每次迭代包含为两个步骤：值更新和策略更新。

- 策略迭代(Policy Iteration)：该算法每次迭代也包含两个步骤：策略评估和策略改进。

- 截断策略迭代(Truncated Policy Iteration)：值迭代和策略迭代算法可被视为截断策略迭代算法的两个极端情况。
  
这三种算法的共同特点是是每一轮迭代都包含两个步骤：一个是用于值的更新，另一个步骤关于策略的更新。在值与策略更新之间不断切换的思想在强化学习中非常普遍，这种理念也被称为**广义策略迭代**(Generalized Policy Iteration)[3](https://web.stanford.edu/class/psych209/Readings/SuttonBartoIPRLBook2ndEd.pdf)。

最后，本章介绍的算法都需要事先知道系统模型。从第$5$章开始，我们将学习无模型的强化学习算法。届时我们将看到无模型的算法可以通过对本章介绍的有模型的算法进行简单修改得到，因此本章的内容十分重要。

---