## 4.1 值迭代

本节介绍值迭代算法。该算法正是上一章(定理$3.3$)中所介绍的，由压缩映射定理提出的用于求解贝尔曼最优公式的算法。具体而言，该算法是

$$v_{k+1}=\max_{\pi\in\Pi}(r_{\pi}+\gamma P_{\pi}v_{k}),\quad k=0,1,2,\ldots$$

定理$3.3$保证了当$k$趋于无穷时，$v_k$和$\pi_k$分别收敛到最优状态值和最优策略。

该算法是迭代式的，每一步迭代包含两个步骤。

- 每次迭代的第一步是策略更新步骤(policy update)。从数学上讲，其目标是找到一个能够解决以下优化问题的策略：

    $$\pi_{k+1}=\arg\max_\pi(r_\pi+\gamma P_\pi v_k),$$
    
    其中$v_k$是在前一次迭代中获得的。

- 第二步称为值更新步骤(value update)。在数学上，它通过以下方式计算一个新的值$v_{k+1}$：

    $$v_{k+1}=r_{\pi_{k+1}}+\gamma P_{\pi_{k+1}}v_{k},\tag{4.1}$$
    
    其中$v_{k+1}$将在下一次迭代中使用。

上面介绍的值迭代算法采用的是矩阵向量形式。为了实现该算法，我们需要进一步考察其逐元素形式。虽然矩阵向量形式有助于理解算法的核心思想，但逐元素形式对于实现细节是必要的。

### 4.1.1 逐元素形式与实现

考虑时间步$k$和状态$s$。

- 首先，策略更新步骤$\pi_{k+1}=\arg\max_\pi(r_\pi+\gamma P_\pi v_k)$的逐元素形式是

    $$\pi_{k+1}(s)=\arg\max_{\pi}\sum_{a}\pi(a|s)\underbrace{\left(\sum_{r}p(r|s,a)r+\gamma\sum_{s^{\prime}}p(s^{\prime}|s,a)v_{k}(s^{\prime})\right)}_{q_{k}(s,a)},\quad s\in\mathcal{S}.$$

    我们在第$3.3.1$节中表明，能够解决上述优化问题的最优策略是

    $$\pi_{k+1}(a|s)=\left\{\begin{array}{ll}1,&a=a_k^*(s),\\0,&a\neq a_k^*(s),\end{array}\right.\tag{4.2}$$
    