{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 网格世界强化学习环境\n",
    "\n",
    "这是一个适合在Jupyter notebook中运行的网格世界环境实现。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 导入必要的库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 定义环境参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================== 用户设置 ====================\n",
    "# 指定网格世界的列数和行数\n",
    "ENV_SIZE = (5, 5)   \n",
    "\n",
    "# 指定起始状态\n",
    "START_STATE = (2, 2)\n",
    "\n",
    "# 指定目标状态\n",
    "TARGET_STATE = (4, 4)\n",
    "\n",
    "# 指定禁止状态\n",
    "FORBIDDEN_STATES = [(2, 1), (3, 3), (1, 3)]\n",
    "\n",
    "# 指定到达目标的奖励\n",
    "REWARD_TARGET = 10\n",
    "\n",
    "# 指定进入禁止区域的奖励\n",
    "REWARD_FORBIDDEN = -5\n",
    "\n",
    "# 指定每步的奖励\n",
    "REWARD_STEP = -1\n",
    "\n",
    "# ==================== 高级设置 ====================\n",
    "# 动作空间: 下, 右, 上, 左, 停留\n",
    "ACTION_SPACE = [(0, 1), (1, 0), (0, -1), (-1, 0), (0, 0)]\n",
    "\n",
    "# 调试模式\n",
    "DEBUG = False\n",
    "\n",
    "# 动画间隔\n",
    "ANIMATION_INTERVAL = 0.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 定义GridWorld类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GridWorld():\n",
    "\n",
    "    def __init__(self, env_size=ENV_SIZE, \n",
    "                 start_state=START_STATE, \n",
    "                 target_state=TARGET_STATE, \n",
    "                 forbidden_states=FORBIDDEN_STATES):\n",
    "\n",
    "        self.env_size = env_size\n",
    "        self.num_states = env_size[0] * env_size[1]\n",
    "        self.start_state = start_state\n",
    "        self.target_state = target_state\n",
    "        self.forbidden_states = forbidden_states\n",
    "\n",
    "        self.agent_state = start_state\n",
    "        self.action_space = ACTION_SPACE          \n",
    "        self.reward_target = REWARD_TARGET\n",
    "        self.reward_forbidden = REWARD_FORBIDDEN\n",
    "        self.reward_step = REWARD_STEP\n",
    "\n",
    "        self.canvas = None\n",
    "        self.animation_interval = ANIMATION_INTERVAL\n",
    "\n",
    "        self.color_forbid = (0.9290,0.6940,0.125)\n",
    "        self.color_target = (0.3010,0.7450,0.9330)\n",
    "        self.color_policy = (0.4660,0.6740,0.1880)\n",
    "        self.color_trajectory = (0, 1, 0)\n",
    "        self.color_agent = (0,0,1)\n",
    "\n",
    "    def reset(self):\n",
    "        self.agent_state = self.start_state\n",
    "        self.traj = [self.agent_state] \n",
    "        return self.agent_state, {}\n",
    "\n",
    "    def step(self, action):\n",
    "        assert action in self.action_space, \"Invalid action\"\n",
    "\n",
    "        next_state, reward  = self._get_next_state_and_reward(self.agent_state, action)\n",
    "        done = self._is_done(next_state)\n",
    "\n",
    "        x_store = next_state[0] + 0.03 * np.random.randn()\n",
    "        y_store = next_state[1] + 0.03 * np.random.randn()\n",
    "        state_store = tuple(np.array((x_store,  y_store)) + 0.2 * np.array(action))\n",
    "        state_store_2 = (next_state[0], next_state[1])\n",
    "\n",
    "        self.agent_state = next_state\n",
    "\n",
    "        self.traj.append(state_store)   \n",
    "        self.traj.append(state_store_2)\n",
    "        return self.agent_state, reward, done, {}   \n",
    "    \n",
    "    def _get_next_state_and_reward(self, state, action):\n",
    "        x, y = state\n",
    "        new_state = tuple(np.array(state) + np.array(action))\n",
    "        if y + 1 > self.env_size[1] - 1 and action == (0,1):    # down\n",
    "            y = self.env_size[1] - 1\n",
    "            reward = self.reward_forbidden  \n",
    "        elif x + 1 > self.env_size[0] - 1 and action == (1,0):  # right\n",
    "            x = self.env_size[0] - 1\n",
    "            reward = self.reward_forbidden  \n",
    "        elif y - 1 < 0 and action == (0,-1):   # up\n",
    "            y = 0\n",
    "            reward = self.reward_forbidden  \n",
    "        elif x - 1 < 0 and action == (-1, 0):  # left\n",
    "            x = 0\n",
    "            reward = self.reward_forbidden \n",
    "        elif new_state == self.target_state:  # stay\n",
    "            x, y = self.target_state\n",
    "            reward = self.reward_target\n",
    "        elif new_state in self.forbidden_states:  # stay\n",
    "            x, y = state\n",
    "            reward = self.reward_forbidden        \n",
    "        else:\n",
    "            x, y = new_state\n",
    "            reward = self.reward_step\n",
    "            \n",
    "        return (x, y), reward\n",
    "        \n",
    "    def _is_done(self, state):\n",
    "        return state == self.target_state\n",
    "    \n",
    "    def render(self, animation_interval=ANIMATION_INTERVAL):\n",
    "        if self.canvas is None:\n",
    "            plt.ion()                             \n",
    "            self.canvas, self.ax = plt.subplots()   \n",
    "            self.ax.set_xlim(-0.5, self.env_size[0] - 0.5)\n",
    "            self.ax.set_ylim(-0.5, self.env_size[1] - 0.5)\n",
    "            self.ax.xaxis.set_ticks(np.arange(-0.5, self.env_size[0], 1))     \n",
    "            self.ax.yaxis.set_ticks(np.arange(-0.5, self.env_size[1], 1))     \n",
    "            self.ax.grid(True, linestyle=\"-\", color=\"gray\", linewidth=\"1\", axis='both')          \n",
    "            self.ax.set_aspect('equal')\n",
    "            self.ax.invert_yaxis()                           \n",
    "            self.ax.xaxis.set_ticks_position('top')           \n",
    "            \n",
    "            idx_labels_x = [i for i in range(self.env_size[0])]\n",
    "            idx_labels_y = [i for i in range(self.env_size[1])]\n",
    "            for lb in idx_labels_x:\n",
    "                self.ax.text(lb, -0.75, str(lb+1), size=10, ha='center', va='center', color='black')           \n",
    "            for lb in idx_labels_y:\n",
    "                self.ax.text(-0.75, lb, str(lb+1), size=10, ha='center', va='center', color='black')\n",
    "            self.ax.tick_params(bottom=False, left=False, right=False, top=False, labelbottom=False, labelleft=False,labeltop=False)   \n",
    "\n",
    "            self.target_rect = patches.Rectangle( (self.target_state[0]-0.5, self.target_state[1]-0.5), 1, 1, linewidth=1, edgecolor=self.color_target, facecolor=self.color_target)\n",
    "            self.ax.add_patch(self.target_rect)     \n",
    "\n",
    "            for forbidden_state in self.forbidden_states:\n",
    "                rect = patches.Rectangle((forbidden_state[0]-0.5, forbidden_state[1]-0.5), 1, 1, linewidth=1, edgecolor=self.color_forbid, facecolor=self.color_forbid)\n",
    "                self.ax.add_patch(rect)\n",
    "\n",
    "            self.agent_star, = self.ax.plot([], [], marker = '*', color=self.color_agent, markersize=20, linewidth=0.5) \n",
    "            self.traj_obj, = self.ax.plot([], [], color=self.color_trajectory, linewidth=0.5)\n",
    "\n",
    "        # self.agent_circle.center = (self.agent_state[0], self.agent_state[1])\n",
    "        self.agent_star.set_data([self.agent_state[0]],[self.agent_state[1]])       \n",
    "        traj_x, traj_y = zip(*self.traj)         \n",
    "        self.traj_obj.set_data(traj_x, traj_y)\n",
    "\n",
    "        plt.draw()\n",
    "        plt.pause(animation_interval)\n",
    "        if DEBUG:\n",
    "            input('press Enter to continue...')     \n",
    "\n",
    "    def add_policy(self, policy_matrix):                  \n",
    "        for state, state_action_group in enumerate(policy_matrix):    \n",
    "            x = state % self.env_size[0]\n",
    "            y = state // self.env_size[0]\n",
    "            for i, action_probability in enumerate(state_action_group):\n",
    "                if action_probability !=0:\n",
    "                    dx, dy = self.action_space[i]\n",
    "                    if (dx, dy) != (0,0):\n",
    "                        self.ax.add_patch(patches.FancyArrow(x, y, dx=(0.1+action_probability/2)*dx, dy=(0.1+action_probability/2)*dy, color=self.color_policy, width=0.001, head_width=0.05))\n",
    "                    else:\n",
    "                        self.ax.add_patch(patches.Circle((x, y), radius=0.07, facecolor=self.color_policy, edgecolor=self.color_policy, linewidth=1, fill=False))\n",
    "    \n",
    "    def add_state_values(self, values, precision=1):\n",
    "        '''\n",
    "            values: iterable\n",
    "        '''\n",
    "        values = np.round(values, precision)\n",
    "        for i, value in enumerate(values):\n",
    "            x = i % self.env_size[0]\n",
    "            y = i // self.env_size[0]\n",
    "            self.ax.text(x, y, str(value), ha='center', va='center', fontsize=10, color='black')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 基本使用示例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建环境\n",
    "env = GridWorld()\n",
    "state = env.reset()               \n",
    "\n",
    "# 随机策略运行\n",
    "for t in range(20):\n",
    "    env.render()\n",
    "    action = random.choice(env.action_space)\n",
    "    next_state, reward, done, info = env.step(action)\n",
    "    print(f\"Step: {t}, Action: {action}, State: {next_state}, Reward: {reward}, Done: {done}\")\n",
    "    if done:\n",
    "        print(\"到达目标！\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 添加策略和状态值可视化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 重置环境\n",
    "env = GridWorld()\n",
    "state = env.reset()\n",
    "\n",
    "# 添加随机策略\n",
    "policy_matrix = np.random.rand(env.num_states, len(env.action_space))                                            \n",
    "policy_matrix /= policy_matrix.sum(axis=1)[:, np.newaxis]  # 使每行元素和为1\n",
    "\n",
    "env.add_policy(policy_matrix)\n",
    "\n",
    "# 添加状态值\n",
    "values = np.random.uniform(0, 10, (env.num_states,))\n",
    "env.add_state_values(values)\n",
    "\n",
    "# 渲染环境\n",
    "env.render(animation_interval=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 自定义环境参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建自定义环境\n",
    "custom_env = GridWorld(\n",
    "    env_size=(6, 6),\n",
    "    start_state=(0, 0),\n",
    "    target_state=(5, 5),\n",
    "    forbidden_states=[(1, 1), (2, 2), (3, 3), (4, 4)]\n",
    ")\n",
    "\n",
    "state = custom_env.reset()\n",
    "\n",
    "# 运行几步\n",
    "for t in range(10):\n",
    "    custom_env.render()\n",
    "    action = random.choice(custom_env.action_space)\n",
    "    next_state, reward, done, info = custom_env.step(action)\n",
    "    print(f\"Step: {t}, Action: {action}, State: {next_state}, Reward: {reward}, Done: {done}\")\n",
    "    if done:\n",
    "        print(\"到达目标！\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 环境说明\n",
    "\n",
    "- **网格大小**: 5x5 (可自定义)\n",
    "- **起始位置**: (2,2) (可自定义)\n",
    "- **目标位置**: (4,4) (可自定义)\n",
    "- **禁止区域**: [(2,1), (3,3), (1,3)] (可自定义)\n",
    "- **动作空间**: 下(0,1), 右(1,0), 上(0,-1), 左(-1,0), 停留(0,0)\n",
    "- **奖励机制**: \n",
    "  - 到达目标: +10\n",
    "  - 进入禁止区域: -5\n",
    "  - 每走一步: -1\n",
    "\n",
    "这个环境适合用于演示强化学习算法，如值迭代、策略迭代、Q-learning等。"
