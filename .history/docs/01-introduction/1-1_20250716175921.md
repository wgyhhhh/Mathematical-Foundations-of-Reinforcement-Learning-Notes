## 1.1 为什么要创作这个文档

由于缺乏必要的技术工具以及可能有助于确保大语言模型（LLM）安全开发和部署的社会技术结构存在缺陷（[Bengio等，2023](https://arxiv.org/pdf/2310.17688v2)），当前快速发展的态势尤其令人担忧。本文系统梳理了两类核心挑战：一是开发有助于保障安全性的技术功能，二是理解并应对确保社会层面安全性时可能面临的社会技术难题。这项工作本质上是向机器学习研究者及相关领域学者发出的行动倡议。我们通过大量引用前沿文献、聚焦具有可行性的具体研究方向，并对每个挑战进行深入探讨，使其成为该领域新人的理想教育资料。同时，我们期望文中指出的诸多挑战能为当前从事LLM对齐与安全研究的实践者（包括来自社会科学、人文、法律、政策、风险分析、哲学等多学科背景的研究者）提供创新灵感。

已经有若干研究对人工智能安全的基础性问题进行了系统梳理与讨论()。然而，大型语言模型(LLMs)的出现标志着范式转变，在对齐性、安全性和可靠性方面带来了诸多现有研究尚未涉及的新颖挑战。其中，Kenton等人是唯一专门针对LLMs的研究，但其讨论范围较窄，仅聚焦于目标函数意外错误设定引发的问题。本文档基于上述工作，首次对LLMs对齐与安全相关挑战作出了迄今最全面、最细致的系统性分析。

我们重点阐述了大型语言模型(LLMs)安全性和对齐性领域的18项基础性挑战，并对每项挑战进行了深入探讨。这些被识别出的挑战具有基础性特性-如果无法克服它们，那么确保LLMs及其衍生系统的安全性与对齐性将极为困难。在本研究中，我们讨论了符合以下标准的基础性挑战：**1、非推测性**，**2、研究成熟度**，**3、危害相关性**。此外，我们提出了200多个具体研究问题以供进一步探索。每个问题均与特定基础性挑战相关联。这些研究问题具有较强开放性，其研究体量大致相当于一篇研究生学位论文，但其中多数问题可通过多角度切入，且存在更深入研究的可能性。