---
title: 总结
comments: true  # 开启评论
---

本章并没有介绍新的强化学习算法，而是介绍了 RM 算法和 SGD 算法的随机梯度算法。与许多其他求解方程的算法不同，RM 算法不需要知道目标函数的表达式。我们也证明了 SGD 算法实际上是一种特殊的 RM 算法。此外，本章频繁讨论的一个重要问题是期望值估计。现在我们知道了式$(6.4)$中的期望值估计也是一个特殊的 SGD 算法。我们将在第 7 节看到，时序差分学习算法具有相似的表达式，到那时大家就不会再困惑为什么序列分算算法会被设计成那个样子了。最后，“随机近似”这个名词最初由 Robbins 和 Monro 在 1951 年使用[[25](https://www.columbia.edu/~ww2040/8100F16/RM51.pdf)]。有关随机近似的更多信息可参见文献[[24](https://link.springer.com/book/10.1007/b101987)]。


---