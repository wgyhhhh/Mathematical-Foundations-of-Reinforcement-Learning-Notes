---
title: 7.1 状态值估计: 时序差分算法
comments: true  # 开启评论
---

本节将介绍最基础的TD算法，它可以估计一个给定策略的状态值。后面的章节会进一步推广这个TD算法从而得到更复杂的算法，因此本节的内容非常重要。
例如，本章介绍的所有算法都属于时序差分（TD）学习的范畴。然而，本节所述的时序差分学习特指一种用于估计状态值的经典算法。
!!! note
    本节核心在于用TD算法求解一个给定的策略$\pi$的状态值，之所以要求解状态值，是因为求解出来状态值后就相当于policy evaluation，之后和policy improvement结合，就可以寻找最优策略。

### 7.1.1 算法描述

!!! note
    TD算法基于数据而非模型，这些数据都是由一个给定的策略$\pi$产生的，TD算法就是要利用这些数据估计$\pi$所对应的状态值。

给定策略$\pi$，我们的目标是估计所有状态$s \in \mathcal{S}$对应的状态值 $v_\pi(s)$。假设我们拥有一些由$\pi$生成的经验样本$(s_0, r_1, s_1, \ldots, s_t, r_{t+1}, s_{t+1}, \ldots)$(也可写为$\{(s_t,r_{t+1},s_{t+1})\}_t$)，其中$t=0,1,2,\ldots$表示采样时刻。以下时序差分(TD)算法可利用这些样本来估计状态值：


$$v_{t+1}(s_t) = v_t(s_t) - \alpha_t(s_t) \left[ v_t(s_t) - \left( r_{t+1} + \gamma v_t(s_{t+1}) \right) \right],\tag{7.1}$$

$$v_{t+1}(s) = v_t(s)，s \neq s_t,\tag{7.2}$$

其中$t =0,1,2, \ldots$。此处$v_t(s_t)$表示时刻$t$时$v_\pi(s_t)$的估计值；$\alpha_t(s_t)$为状态$s_t$在时刻$t$的学习率。

在时刻$t$时，只有当时正在被访问的状态$s_t$的估计值会被更新(如式$(7.1)$所示);未访问状态的估计值保持不变(如式$(7.2)$所示)。通常情况下，式$(7.2)$会被省略，但是我们应该知道该式子的存在。该式可以帮助我们更好地理解TD算法，如果没有这个式子，TD算法在数学上也是不完整的。

!!! note
    回想一下，TD算法在数学上的作用是什么?答案就是它求解给定策略$\pi$的贝尔曼方程，但是之前的课程我们学了解析解和数值解，这里的TD算法与他的区别是什么呢？答案是显而易见的，之前的方法是基于模型的方法，而TD算法并不依赖于模型。

许多读者在第一次看到$(7.1)$中的TD算法时会问为什么它要设计成这个样子?实际上，该算法是一个用于求解贝尔曼方程的随机近似算法。要理解这一点，我们首先回顾状态值的定义：

$$v_\pi(s) = \mathbb{E}\left[ R_{t+1} + \gamma G_{t+1} \mid S_t = s \right], \quad s \in \mathcal{S}.\tag{7.3}$$

式子$(7.3)$可被重写为

$$v_\pi(s) = \mathbb{E}[R_{t+1} + \gamma v_\pi(S_{t+1})|S_t = s], s\in \mathcal{S}.\tag{7.4}$$



这是因为$\mathbb{E}[G_{t+1}|S_t = s] = \sum_a \pi(a|s) \sum_{s'} p(s'|s, a)v_\pi(s') = \mathbb{E}[v_\pi(S_{t+1})|S_t = s]$。式$(7.4)$是贝尔曼方程的另一种表达，它有时被称为**贝尔曼期望方程**(Bellman expecta-tion equation)。如果我们应用第6章介绍的罗宾斯-门罗算法来求解式$(7.4)$,相应的算法就是TD算法。感兴趣的读者可以参见Box 7.1。

!!! note
    建议读者读一下Box7.1，否则对于这个算法很可能云里雾里。

### 7.1.2 性质分析

下面讨论TD算法(7.1)的一些重要性质。
首先，我们先介绍TD算法中每一项的含义。具体如下所示：

$$\underbrace{v_{t+1}(s_t)}_{\text{new estimate}}=\underbrace{v_t(s_t)}_{\text{current estimate}}-\alpha_t(s_t)\left[\overbrace{v_t(s_t)-\left(\underbrace{r_{t+1}+\gamma v_t(s_{t+1})}_{\text{TD target }\tilde{v}_t}\right)}^{\text{TD error }\delta_t}\right],\tag{7.6}$$

在这里

$$\bar{v}_t = r_{t+1} + \gamma v_t(s_{t+1})$$

被称为**TD目标值(TD target)**，而

### 7.1.2 性质分析

$$\delta_t = v(s_t) - \bar{v}_t = v(s_t) - \left( r_{t+1} + \gamma v(s_{t+1}) \right)$$

称为**TD误差(TD error)**。可以看出，新估计值$v_{t+1}(s_t)$是当前估计值$v_t(s_t)$与时序差分误差$\delta_t$的组合。

- 为什么$\bar{v}_t$被叫做TD目标值

    是因为该算法在数学上就是让$v(s_t)$的值更加接近$\bar{v}_t$，即$\bar{v}_t$是$v(s_t)$的目标值。为了理解这一点，我们在(7.6)两边同时减去可得
    
    $$\begin{gathered}v_{t+1}(s_{t})-\bar{v}_{t}=\left[v_{t}(s_{t})-\bar{v}_{t}\right]-\alpha_{t}(s_{t})\left[v_{t}(s_{t})-\bar{v}_{t}\right]\\=[1-\alpha_{t}(s_{t})]\left[v_{t}(s_{t})-\bar{v}_{t}\right].\end{gathered}$$

    上述等式两边取绝对值可得
    
    $$|v_{t+1}(s_t) − \bar{v}_t| = |1 − \alpha_t(s_t)||v_t(s_t) − \bar{v}_t|.$$
    
    由于$\alpha_t(s_t)$是一个小的正数，因此有$0 <1 - \alpha_t(s_t) <1$。因此由上式可以推得
    
    $$|v_{t+1}(s_t) − \bar{v}_t| < |v_t(s_t) − \bar{v}_t|.$$
    
    上述不等式具有重要意义，因为它表明新值 $v_{t+1}(s_t)$比旧值 $v_t(s_t)$更接近 $\bar{v}_t$。因此，该算法在数学上会将 $v_t(s_t)$向 $\bar{v}_t$方向驱动。这正是 $\bar{v}_t$被称为 TD目标的原因。

- TD误差的解释是什么？

<<<<<<< HEAD
    首先，这一误差之所以被称为TD误差，是因为 $\delta_t = v_t(s_t) - (r_{t+1} + \gamma v_t(s_{t+1}))$反映了时间步 $t$与 $t +1$之间的差异。其次，当状态价值估计准确时，时序差分误差在期望意义上为零。具体而言，当 $v_t = v_\pi$时，时序差分误差的期望值为
=======
    首先，这一误差之所以被称为时序差分（temporal-difference），是因为 $\delta_t = v_t(s_t) - (r_{t+1} + \gamma v_t(s_{t+1}))$反映了时间步 $t$与 $t +1$之间的差异。其次，当状态价值估计准确时，时序差分误差在期望意义上为零。具体而言，当 $v_t = v_\pi$时，时序差分误差的期望值为
>>>>>>> 50b98123eef511e961e9c05b37b46f2a667dafb0
    
    $$\begin{aligned}\mathbb{E}[\delta_{t}|S_{t}=s_{t}]&=\mathbb{E}\left[v_{\pi}(S_{t})-(R_{t+1}+\gamma v_{\pi}(S_{t+1}))|S_{t}=s_{t}\right]\\&=v_{\pi}(s_{t})-\mathbb{E}\left[R_{t+1}+\gamma v_{\pi}(S_{t+1})|S_{t}=s_{t}\right]\\&=0.\quad\mathrm{(due~to~(7.3))}\end{aligned}$$
    
    因此，时序差分误差不仅反映两个时间步之间的差异，更重要的是反映了估计值$v_t$与真实状态值$v_\pi$之间的差异。

<<<<<<< HEAD
    在更抽象的层面上，TD误差可解释为**新息**(innovation)，它表示从经验样本 $(s_t, r_{t+1}, s_{t+1})$中获得的新信息。TD学习的核心思想是基于新获得的信息来修正当前对状态值的估计。新息是卡尔曼滤波[33,34]等众多估计问题中的基础概念。

其次，$(7.1)$中的 TD算法只能估计给定策略的状态值。为了寻找最优策略，我们仍需进一步计算行动值并执行策略改进，这将在第$7.2$节中介绍。尽管如此，本节介绍的TD算法对于理解本章其他算法具有基础性和重要性。

第三，虽然时序差分(TD)学习和蒙特卡洛(MC)学习均属于无模型方法，但二者各有哪些优劣？答案总结于表$7.1$。

 | **TD学习**            | **MC学习**            |
 |--------------------|----------------------|
 | **增量**: TD学习是增量的。它可以在接收到经验样本后立即更新状态/动作值。 | **非增量**: MC学习是非增量的。它必须等到一个回合(episode)完全收集完毕才能更新。这是因为它必须计算回合的折扣回报。 |
 | **持续任务**: 由于TD学习是增量的，它可以处理回合制和持续任务。持续任务可能没有终止状态。 | **回合任务**: 由于MC学习是非增量的，它只能处理回合任务，即回合在有限的步骤后终止。 |
 | **自助法**: TD学习采用自助法，因为状态/行动值的更新依赖于该值的前一估计。因此，TD学习需要初始值猜测。 | **非自助法**: MC学习不是自助法，因为它可以直接估计状态/行动值，而无需初始猜测。 |
 | **低估计方差**: TD的估计方差低于MC，因为它涉及较少的随机变量。例如，估计一个动作值 \( q_\pi(s_t, a_t) \)，Sarsa 仅需要三个随机变量的样本：\( R_{t+1} \), \( S_{t+1} \), \( A_{t+1} \)。 | **高估计方差**: MC的估计方差较高，因为涉及更多的随机变量。例如，为了估计一个行动值 \( q_\pi(s_t, a_t) \)，我们需要样本 \( R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \dots \)。假设每个回合的长度为 \( L \)，并且每个状态的动作数量相同，记作 \( |A| \)，那么每个回合有 \( |A|^L \) 种可能的序列。如果我们仅估计少数几个回合，估计方差高是很自然的。
=======
    在更抽象的层面上，时序差分（TD）误差可解释为**新息**(innovation)，它表示从经验样本 $(s_t, r_{t+1}, s_{t+1})$中获得的新信息。时序差分学习的核心思想是基于新获得的信息来修正当前对状态值的估计。新息是卡尔曼滤波[33,34]等众多估计问题中的基础概念。

其次，$(7.1)$中的 TD算法只能估计给定策略的状态值。为了寻找最优策略，我们仍需进一步计算动作值并执行策略改进，这将在第$7.2$节中介绍。尽管如此，本节介绍的 TD算法对于理解本章其他算法具有基础性和重要性。

第三，虽然时序差分（TD）学习和蒙特卡洛（MC）学习均属于无模型方法，但二者各有哪些优劣？答案总结于表$7.1$。

 | **TD学习**            | **MC学习**            |
 |--------------------|----------------------|
 | **增量**: TD学习是增量的。它可以在接收到经验样本后立即更新状态/动作值。 | **非增量**: MC学习是非增量的。它必须等到一个回合（episode）完全收集完毕才能更新。这是因为它必须计算回合的折扣回报。 |
 | **持续任务**: 由于TD学习是增量的，它可以处理回合制和持续任务。持续任务可能没有终止状态。 | **回合任务**: 由于MC学习是非增量的，它只能处理回合任务，即回合在有限的步骤后终止。 |
 | **自助法**: TD学习采用自助法，因为状态/动作值的更新依赖于该值的前一估计。因此，TD学习需要初始值猜测。 | **非自助法**: MC学习不是自助法，因为它可以直接估计状态/动作值，而无需初始猜测。 |
 | **低估计方差**: TD的估计方差低于MC，因为它涉及较少的随机变量。例如，估计一个动作值 \( q_n(s_t, a_t) \)，Sarsa 仅需要三个随机变量的样本：\( R_{t+1} \), \( S_{t+1} \), \( A_{t+1} \)。 | **高估计方差**: MC的估计方差较高，因为涉及更多的随机变量。例如，为了估计一个动作值 \( q_n(s_t, a_t) \)，我们需要样本 \( R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \dots \)。假设每个回合的长度为 \( L \)，并且每个状态的动作数量相同，记作 \( |A| \)，那么每个回合有 \( |A|^L \) 种可能的序列。如果我们仅估计少数几个回合，估计方差高是很自然的。
>>>>>>> 50b98123eef511e961e9c05b37b46f2a667dafb0

 > 表7.1: TD学习与MC学习的比较。

### 7.1.3收敛性分析

下面给出$(7.1)$中TD算法的收敛性分析。

!!! info
    **定理7.1**（时序差分学习的收敛性）。给定策略 $\pi$，若对任意状态 $s \in \mathcal{S}$满足 $\sum_t \alpha_t(s) = \infty$且 $\sum_t \alpha_t^2(s) < \infty$，则通过$(7.1)$式的时序差分算法，$v_t(s)$几乎必然收敛到 $v_\pi(s)$（当 $t \to \infty$时）。

以下给出关于 $\alpha_t$的若干说明。首先，对于所有 $s \in S$，必须满足条件 $\sum_t \alpha_t(s) = \infty$且 $\sum_t \alpha^2_t(s) < \infty$。需注意，在时刻 $t$，若状态 $s$被访问则 $\alpha_t(s) >0$，否则 $\alpha_t(s) =0$。条件 $\sum_t \alpha_t(s) = \infty$要求状态 $s$被无限次（或充分多次）访问，这需要通过探索性起始条件或探索性策略来实现，从而确保每个状态-动作对都可能被多次访问。其次，学习率 $\alpha_t$在实际总通常选取为较小的正常数。此时，条件$\sum_t \alpha^2_t(s) < \infty$不再成立。当$\alpha$为常数时，仍可证明该算法在期望意义下收敛[24,第1.5节]。
---