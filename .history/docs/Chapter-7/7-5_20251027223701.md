---
title: 7.5 时序差分算法的统一框架
comments: true  # 开启评论
---
截至目前，我们已经介绍了多种时序差分（TD）算法，例如Sarsa、n步Sarsa和Q-learning。本节将提出一个统一框架，用以兼容上述所有算法及蒙特卡洛(MC)学习方法。

特别地，时序差分算法（用于行动值估计)可统一表示为：

$$q_{t+1}(s_t,a_t)=q_t(s_t,a_t)-\alpha_t(s_t,a_t)[q_t(s_t,a_t)-\bar{q}_t],\tag{7.20}$$

<<<<<<< HEAD
=======
 ![](../img/07/6.png)
 >图$7.4$：通过Q-learning演示off-policy学习的示例。(a)和(b)分别显示了最优策略和最优状态值。(c)和(d)分别展示了行为策略和生成的轨迹。(e)和(f)分别呈现了估计策略与误差演变过程。(g)和(h)则显示了不同初始值条件下的对比情况。

 ![](../img/07/7.png)
 >图$7.5$：当行为策略不具备探索性时，Q-learning算法的性能会下降。左列各图展示了行为策略的分布特征。中间列各图呈现了对应行为策略下生成的轨迹序列，每个示例中的轨迹均包含$100,000$步。右列各图则显示了状态值估计值的均方根误差演变过程。

>>>>>>> 50b98123eef511e961e9c05b37b46f2a667dafb0
 ![](../img/07/8.png)
 >表7.2：时序差分算法的统一视角。其中BE和BOE分别表示贝尔曼方程(Bellman equation)和贝尔曼最优方程(Bellman optimality equation)。

 


其中 $\bar{q}_t$为TD目标值。不同的时序差分算法对应不同的$\bar{q}_t$表达式，具体总结见表7.2。蒙特卡洛学习算法可视为式$(7.20)$的特例：令 $\alpha_t(s_t, a_t) =1$时，式(7.20)即退化为 $q_{t+1}(s_t, a_t) = \bar{q}_t$。

算法$(7.20)$可视为求解统一方程$q(s, a) = E[\bar{q}_t|s, a]$的随机近似算法。该方程随 $\bar{q}_t$的不同具有不同表达形式，其对应关系总结于表$7.2$中。由此可见，除 Q-learning算法以求解贝尔曼最优方程为目标外，其余算法均致力于求解贝尔曼方程。
---