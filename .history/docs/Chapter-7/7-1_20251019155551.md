## 7.1 状态值估计: 时序查分算法

<<<<<<< HEAD
例如，本章介绍的所有算法都属于时序差分(TD)学习的范畴。然而，本节所述的时序差分学习特指一种用于估计状态值的经典算法。

!!! note
    本节核心在于用TD算法求解一个给定的策略$\pi$的状态值，之所以要求解状态值，是因为求解出来状态值后就相当于policy evaluation，之后和policy improvement结合，就可以寻找最优策略。

### 7.1.1算法描述

给定策略$\pi$，我们的目标是估计所有状态$s \in \mathcal{S}$对应的 $v_\pi(s)$。假设我们拥有遵循$\pi$生成的若干经验样本$(s_0, r_1, s_1, \ldots, s_t, r_{t+1}, s_{t+1}, \ldots)$(也可写为$\{(s_t,r_{t+1},s_{t+1})\}_t$)，其中$t$表示时间步。以下时序差分(TD)算法可利用这些样本来估计状态值：

!!! note
    TD算法基于数据而非模型，这些数据都是由一个给定的策略$\pi$产生的，TD算法就是要利用这些数据估计$\pi$所对应的状态值。
=======
例如，本章介绍的所有算法都属于时序差分（TD）学习的范畴。然而，本节所述的时序差分学习特指一种用于估计状态值的经典算法。

### 7.1.1算法描述

给定策略 $\pi$，我们的目标是估计所有状态 $s \in S$对应的 $v_\pi(s)$。假设我们拥有遵循 $\pi$生成的若干经验样本 $(s_0, r_1, s_1, \ldots, s_t, r_{t+1}, s_{t+1}, \ldots)$，其中 $t$表示时间步。以下时序差分（TD）算法可利用这些样本来估计状态值：
>>>>>>> 50b98123eef511e961e9c05b37b46f2a667dafb0

$$v_{t+1}(s_t) = v_t(s_t) - \alpha_t(s_t) \left[ v_t(s_t) - \left( r_{t+1} + \gamma v_t(s_{t+1}) \right) \right],\tag{7.1}$$

$$v_{t+1}(s) = v_t(s)，s \neq s_t,\tag{7.2}$$

<<<<<<< HEAD
其中$t =0,1,2, \ldots$。此处$v_t(s_t)$表示时刻$t$时$v_\pi(s_t)$的估计值；$\alpha_t(s_t)$为状态$s_t$在时刻$t$的学习率。

需要注意的是，在时刻$t$时，仅更新已访问状态$s_t$的值。未访问状态$s \neq s_t$的值保持不变，如式$(7.2)$所示。为简洁起见，式$(7.2)$常被省略，但应牢记这一公式，因为缺少该式会导致算法在数学上不完整。

!!! note
    回想一下，TD算法在数学上的作用是什么?答案就是它求解给定策略$\pi$的贝尔曼方程，但是之前的课程我们学了封闭解和迭代解，这里的TD算法与他的区别是什么呢？答案是显而易见的，之前的方法是基于模型的方法，而TD算法并不依赖于模型。

初次接触TD学习算法的读者可能会好奇其设计原理。实际上，该算法可视为求解贝尔曼方程的一种特殊随机近似算法。为理解这一点，首先回顾状态值的定义：

$$v_\pi(s) = \mathbb{E}\left[ R_{t+1} + \gamma G_{t+1} \mid S_t = s \right], \quad s \in \mathcal{S}.\tag{7.3}$$


由于

$$\mathbb{E}[G_{t+1}|S_{t}=s]=\sum_{a}\pi(a|s)\sum_{s^{\prime}}p(s^{\prime}|s,a)v_{\pi}(s^{\prime})=\mathbb{E}[v_{\pi}(S_{t+1})|S_{t}=s].$$

=======
其中 $t =0,1,2, \ldots$。此处 $v_t(s_t)$表示时刻 $t$时 $v_\pi(s_t)$的估计值；$\alpha_t(s_t)$为状态 $s_t$在时刻 $t$的学习率。

需要注意的是，在时刻 $t$时，仅更新已访问状态 $s_t$的值。未访问状态 $s \neq s_t$的值保持不变，如式$(7.2)$所示。为简洁起见，式$(7.2)$常被省略，但应牢记这一公式，因为缺少该式会导致算法在数学上不完整。

初次接触TD学习算法的读者可能会好奇其设计原理。实际上，该算法可视为求解贝尔曼方程的一种特殊随机逼近算法。为理解这一点，首先回顾状态值的定义：

$$v_\pi(s) = \mathbb{E}\left[ R_{t+1} + \gamma G_{t+1} \mid S_t = s \right], \quad s \in \mathcal{S}.\tag{7.3}$$

>>>>>>> 50b98123eef511e961e9c05b37b46f2a667dafb0
我们可以将式$(7.3)$改写为

$$v_\pi(s) = \mathbb{E}[R_{t+1} + \gamma v_\pi(S_{t+1})|S_t = s], s\in \mathcal{S}.\tag{7.4}$$

<<<<<<< HEAD
在这里$S^{t+1}$是下一个状态，方程$(7.4)$是贝尔曼方程的另一种表达形式，有时也被称为**贝尔曼期望方程**。

TD算法可通过将Robbins-Monro算法(第6章)应用于求解式$(7.4)$中的Bellman方程推导得出。感兴趣的读者可参阅Box 7.1了解具体细节。

!!! note
    建议读者读一下Box7.1，否则对于这个算法很可能云里雾里。

### 7.1.2 性质分析

TD算法的一些重要特性讨论如下。首先，我们更详细地考察TD算法的表达式。具体而言，$(7.1)$式可表述为

$$\underbrace{v_{t+1}(s_t)}_{\text{new estimate}}=\underbrace{v_t(s_t)}_{\text{current estimate}}-\alpha_t(s_t)\left[\overbrace{v_t(s_t)-\left(\underbrace{r_{t+1}+\gamma v_t(s_{t+1})}_{\text{TD target }\tilde{v}_t}\right)}^{\text{TD error }\delta_t}\right],\tag{7.6}$$
=======
这是因为 $\mathbb{E}[G_{t+1}|S_t = s] = \sum_a \pi(a|s) \sum_{s'} p(s'|s, a)v_\pi(s') = \mathbb{E}[v_\pi(S_{t+1})|S_t = s]$。方程$(7.4)$是贝尔曼方程的另一种表达形式，有时也被称为**贝尔曼期望方程**。

TD算法可通过将Robbins-Monro算法（第6章）应用于求解式$(7.4)$中的Bellman方程推导得出。感兴趣的读者可参阅Box 7.1了解具体细节。

### 7.1.2 性质分析

TD算法的一些重要特性讨论如下。首先，我们更详细地考察TD算法的表达式。具体而言，(7.1)式可表述为

$$v_{t+1}(s_t) = v_t(s_t) - \alpha_t(s_t) \left[ v_t(s_t) - \left( r_{t+1} + \gamma v_{t+1}(s_{t+1}) \right) \right]$$
>>>>>>> 50b98123eef511e961e9c05b37b46f2a667dafb0

在这里

$$\bar{v}_t = r_{t+1} + \gamma v_t(s_{t+1})$$

<<<<<<< HEAD
称为**TD目标值(TD target)**

$$\delta_t = v(s_t) - \bar{v}_t = v(s_t) - \left( r_{t+1} + \gamma v(s_{t+1}) \right)$$

称为**TD误差(TD error)**。可以看出，新估计值$v_{t+1}(s_t)$是当前估计值$v_t(s_t)$与时序差分误差$\delta_t$的加权组合。

- 为什么$\bar{v}_t$被叫做TD目标值

    这是因为算法驱使$v(s_t)$趋向于$\bar{v}_t$。为验证这一点，将 (7.6)式两边同时减去$\bar{v}_t$可得
=======
称为TD目标值(TD target)

$$\delta_t = v(s_t) - \tilde{v}_t = v(s_t) - \left( r_{t+1} + \gamma v(s_{t+1}) \right)$$

称为TD误差(TD error)。可以看出，新估计值$v_{t+1}(s_t)$是当前估计值$v_t(s_t)$与时序差分误差$\delta_t$的加权组合。

- 为什么$\bar{v}_t$被叫做TD目标值

    这是因为$\bar{v}_t$是算法试图驱使$v(s_t)$趋近的目标值。为验证这一点，将 (7.6)式两边同时减去$\bar{v}_t$可得
>>>>>>> 50b98123eef511e961e9c05b37b46f2a667dafb0
    
    $$\begin{gathered}v_{t+1}(s_{t})-\bar{v}_{t}=\left[v_{t}(s_{t})-\bar{v}_{t}\right]-\alpha_{t}(s_{t})\left[v_{t}(s_{t})-\bar{v}_{t}\right]\\=[1-\alpha_{t}(s_{t})]\left[v_{t}(s_{t})-\bar{v}_{t}\right].\end{gathered}$$
    
    对上述等式两边取绝对值可得
    
    $$|v_{t+1}(s_t) − \bar{v}_t| = |1 − \alpha_t(s_t)||v_t(s_t) − \bar{v}_t|.$$
    
    由于$\alpha_t(s_t)$是一个小的正数，因此有$0 <1 - \alpha_t(s_t) <1$。由此可得
    
    $$|v_{t+1}(s_t) − \bar{v}_t| < |v_t(s_t) − \bar{v}_t|.$$
    
    上述不等式具有重要意义，因为它表明新值 $v_{t+1}(s_t)$比旧值 $v_t(s_t)$更接近 $\bar{v}_t$。因此，该算法在数学上会将 $v_t(s_t)$向 $\bar{v}_t$方向驱动。这正是 $\bar{v}_t$被称为 TD目标的原因。

- TD误差的解释是什么？

<<<<<<< HEAD
    首先，这一误差之所以被称为TD误差，是因为 $\delta_t = v_t(s_t) - (r_{t+1} + \gamma v_t(s_{t+1}))$反映了时间步 $t$与 $t +1$之间的差异。其次，当状态价值估计准确时，时序差分误差在期望意义上为零。具体而言，当 $v_t = v_\pi$时，时序差分误差的期望值为
=======
    首先，这一误差之所以被称为时序差分（temporal-difference），是因为 $\delta_t = v_t(s_t) - (r_{t+1} + \gamma v_t(s_{t+1}))$反映了时间步 $t$与 $t +1$之间的差异。其次，当状态价值估计准确时，时序差分误差在期望意义上为零。具体而言，当 $v_t = v_\pi$时，时序差分误差的期望值为
>>>>>>> 50b98123eef511e961e9c05b37b46f2a667dafb0
    
    $$\begin{aligned}\mathbb{E}[\delta_{t}|S_{t}=s_{t}]&=\mathbb{E}\left[v_{\pi}(S_{t})-(R_{t+1}+\gamma v_{\pi}(S_{t+1}))|S_{t}=s_{t}\right]\\&=v_{\pi}(s_{t})-\mathbb{E}\left[R_{t+1}+\gamma v_{\pi}(S_{t+1})|S_{t}=s_{t}\right]\\&=0.\quad\mathrm{(due~to~(7.3))}\end{aligned}$$
    
    因此，时序差分误差不仅反映两个时间步之间的差异，更重要的是反映了估计值$v_t$与真实状态值$v_\pi$之间的差异。

<<<<<<< HEAD
    在更抽象的层面上，TD误差可解释为**新息**(innovation)，它表示从经验样本 $(s_t, r_{t+1}, s_{t+1})$中获得的新信息。TD学习的核心思想是基于新获得的信息来修正当前对状态值的估计。新息是卡尔曼滤波[33,34]等众多估计问题中的基础概念。

其次，$(7.1)$中的 TD算法只能估计给定策略的状态值。为了寻找最优策略，我们仍需进一步计算行动值并执行策略改进，这将在第$7.2$节中介绍。尽管如此，本节介绍的TD算法对于理解本章其他算法具有基础性和重要性。

第三，虽然时序差分(TD)学习和蒙特卡洛(MC)学习均属于无模型方法，但二者各有哪些优劣？答案总结于表$7.1$。

 | **TD学习**            | **MC学习**            |
 |--------------------|----------------------|
 | **增量**: TD学习是增量的。它可以在接收到经验样本后立即更新状态/动作值。 | **非增量**: MC学习是非增量的。它必须等到一个回合(episode)完全收集完毕才能更新。这是因为它必须计算回合的折扣回报。 |
 | **持续任务**: 由于TD学习是增量的，它可以处理回合制和持续任务。持续任务可能没有终止状态。 | **回合任务**: 由于MC学习是非增量的，它只能处理回合任务，即回合在有限的步骤后终止。 |
 | **自助法**: TD学习采用自助法，因为状态/行动值的更新依赖于该值的前一估计。因此，TD学习需要初始值猜测。 | **非自助法**: MC学习不是自助法，因为它可以直接估计状态/行动值，而无需初始猜测。 |
 | **低估计方差**: TD的估计方差低于MC，因为它涉及较少的随机变量。例如，估计一个动作值 \( q_\pi(s_t, a_t) \)，Sarsa 仅需要三个随机变量的样本：\( R_{t+1} \), \( S_{t+1} \), \( A_{t+1} \)。 | **高估计方差**: MC的估计方差较高，因为涉及更多的随机变量。例如，为了估计一个行动值 \( q_\pi(s_t, a_t) \)，我们需要样本 \( R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \dots \)。假设每个回合的长度为 \( L \)，并且每个状态的动作数量相同，记作 \( |A| \)，那么每个回合有 \( |A|^L \) 种可能的序列。如果我们仅估计少数几个回合，估计方差高是很自然的。
=======
    在更抽象的层面上，时序差分（TD）误差可解释为**新息**(innovation)，它表示从经验样本 $(s_t, r_{t+1}, s_{t+1})$中获得的新信息。时序差分学习的核心思想是基于新获得的信息来修正当前对状态值的估计。新息是卡尔曼滤波[33,34]等众多估计问题中的基础概念。

其次，$(7.1)$中的 TD算法只能估计给定策略的状态值。为了寻找最优策略，我们仍需进一步计算动作值并执行策略改进，这将在第$7.2$节中介绍。尽管如此，本节介绍的 TD算法对于理解本章其他算法具有基础性和重要性。

第三，虽然时序差分（TD）学习和蒙特卡洛（MC）学习均属于无模型方法，但二者各有哪些优劣？答案总结于表$7.1$。

 | **TD学习**            | **MC学习**            |
 |--------------------|----------------------|
 | **增量**: TD学习是增量的。它可以在接收到经验样本后立即更新状态/动作值。 | **非增量**: MC学习是非增量的。它必须等到一个回合（episode）完全收集完毕才能更新。这是因为它必须计算回合的折扣回报。 |
 | **持续任务**: 由于TD学习是增量的，它可以处理回合制和持续任务。持续任务可能没有终止状态。 | **回合任务**: 由于MC学习是非增量的，它只能处理回合任务，即回合在有限的步骤后终止。 |
 | **自助法**: TD学习采用自助法，因为状态/动作值的更新依赖于该值的前一估计。因此，TD学习需要初始值猜测。 | **非自助法**: MC学习不是自助法，因为它可以直接估计状态/动作值，而无需初始猜测。 |
 | **低估计方差**: TD的估计方差低于MC，因为它涉及较少的随机变量。例如，估计一个动作值 \( q_n(s_t, a_t) \)，Sarsa 仅需要三个随机变量的样本：\( R_{t+1} \), \( S_{t+1} \), \( A_{t+1} \)。 | **高估计方差**: MC的估计方差较高，因为涉及更多的随机变量。例如，为了估计一个动作值 \( q_n(s_t, a_t) \)，我们需要样本 \( R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \dots \)。假设每个回合的长度为 \( L \)，并且每个状态的动作数量相同，记作 \( |A| \)，那么每个回合有 \( |A|^L \) 种可能的序列。如果我们仅估计少数几个回合，估计方差高是很自然的。
>>>>>>> 50b98123eef511e961e9c05b37b46f2a667dafb0

 > 表7.1: TD学习与MC学习的比较。

### 7.1.3收敛性分析

下面给出$(7.1)$中TD算法的收敛性分析。

!!! info
    **定理7.1**（时序差分学习的收敛性）。给定策略 $\pi$，若对任意状态 $s \in \mathcal{S}$满足 $\sum_t \alpha_t(s) = \infty$且 $\sum_t \alpha_t^2(s) < \infty$，则通过$(7.1)$式的时序差分算法，$v_t(s)$几乎必然收敛到 $v_\pi(s)$（当 $t \to \infty$时）。

以下给出关于 $\alpha_t$的若干说明。首先，对于所有 $s \in S$，必须满足条件 $\sum_t \alpha_t(s) = \infty$且 $\sum_t \alpha^2_t(s) < \infty$。需注意，在时刻 $t$，若状态 $s$被访问则 $\alpha_t(s) >0$，否则 $\alpha_t(s) =0$。条件 $\sum_t \alpha_t(s) = \infty$要求状态 $s$被无限次（或充分多次）访问，这需要通过探索性起始条件或探索性策略来实现，从而确保每个状态-动作对都可能被多次访问。其次，学习率 $\alpha_t$在实际总通常选取为较小的正常数。此时，条件$\sum_t \alpha^2_t(s) < \infty$不再成立。当$\alpha$为常数时，仍可证明该算法在期望意义下收敛[24,第1.5节]。