---
title: 7.5 时序差分算法的统一框架
comments: true  # 开启评论
---
到目前为止，我们已经介绍了几个不同的TD算法，如Sarsa、n-Step Sarsa和Q-learning。下面介绍一个统一的框架来描述这些TD算法甚至蒙特卡洛算法。

具体来说，用于行动值估计的TD算法可以写成一个统一的表达式：

$$q_{t+1}(s_t,a_t)=q_t(s_t,a_t)-\alpha_t(s_t,a_t)[q_t(s_t,a_t)-\bar{q}_t],\tag{7.20}$$

 ![](../img/07/6.png)
 >图$7.4$：用于展示 Of-policy模式的 Q-learning的例子。图(a)和(b)展示了最优策略和最优状态值。图(c)和(d)展示了行为策略和生成的回合。图(e)和(f)展示了学习到的策略和估计误差的收敛过程。图(g)和(h)展示了具有不同初始值的情况。

 ![](../img/07/7.png)
 >图$7.5$：当行为策略探索性较弱时，学习的效果会下降。左列的图展示了不同的行为策略。中间列的图展示了由相应行为策略生成的回合，每个回合有100000步。右列的图展示了最优状态值估计误差的演变过程。

其中 $\bar{q}_t$为TD目标。只是不同的TD算法有不同的TD目标$\bar{q}_t$表达式，具体总结见表7.2。蒙特卡洛算法可视为式$(7.20)$的特例：如果设置$\alpha_t(s_t, a_t) =1$，那么式(7.20)就变成了$q_{t+1}(s_t, a_t) = \bar{q}_t$，这实际上就是蒙特卡洛算法。

算法$(7.20)$可视为求解统一方程$q(s, a) = E[\bar{q}_t|s, a]$的随机近似算法。这个方程具有不同表达形式，其对应关系见表$7.2$。可以看出，所有算法本质上都是求解贝尔曼方程，只有Q-learning是求解贝尔曼最优方程。


 ![](../img/07/8.png)
 >表7.2：时序差分算法的统一视角。其中BE和BOE分别表示贝尔曼方程(Bellman equation)和贝尔曼最优方程(Bellman optimality equation)。
---