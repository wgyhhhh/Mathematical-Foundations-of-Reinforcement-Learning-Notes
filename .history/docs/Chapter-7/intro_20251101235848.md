在第5章，我们介绍了全书第一类无需模型的强化学习算法：蒙特卡罗(Monte Carlo,MC)。在本章，我们将介绍全书第二类无需模型的强化学习算法：时序差分(temporal difference,TD)。与MC算法相比，TD算法最大的不同在于它是增量式的。许多人第一次看到TD算法时会有很多疑惑，例如这些算法为什么设计成这个样子。不过在学习了第6章的随机近似算法后，相信读者能更加轻松地掌握TD算法，这是因为TD算法本质上是求解贝尔曼方程或者贝尔曼最优方程的随机近似算法。

由于本章将介绍多种TD算法，为了帮助读者更好地学习，我们首先梳理这些算法之间的关系。

- 第7.1节介绍最基本也是最核心的TD算法。该算法可以估计一个给定策略的状态值。掌握这个算法对于学习后面的TD算法是非常有必要的。
  
- 第7.2节介绍Sarsa算法。该算法可以估计给定策略的动作值。实际上，将第7.1节的TD算法中的状态值替换为动作值，就可以得到Sarsa算法。
  
- 第7.3节介绍n-Step Sarsa算法，这是Sarsa算法的一种推广。我们将会看到Sarsa算法和MC算法是n-Step Sarsa算法的两个特殊情况。
  
- 第7.4节介绍Q-learning算法，这是经典的强化学习算法之一。Q-learning算法和Sarsa算法的区别在于：Sarsa算法是在求解一个给定策略的贝尔曼方程，而Q-learning算法是直接求解贝尔曼最优方程。

- 第7.5节总结本章介绍的所有TD算法，并提供一个统一的描述框架。
  
 ![](../img/chapter/chapter-7.png)
 > 图$7.1$: 本章在全书中的位置。