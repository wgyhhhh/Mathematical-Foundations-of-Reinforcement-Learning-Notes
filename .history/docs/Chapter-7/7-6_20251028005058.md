---
title: 总结
comments: true  # 开启评论
---
本章介绍了多种时序差分算法，所有这些算法都可以被视为求解贝尔曼方程或贝尔曼最优方程的随机近似算法。

本章介绍的TD算法，除了Q-learning外，都是用于评价某个给定策略的，即从一些经验样本中估计给定策略的状态/动作值，它们需要结合策略改进步骤才能得到最优策略。此外，这些算法是On-policy的，因为它们的目标策略和行为策略相同。

Q-learning与其他算法相比有一点特殊，因为它是Of-policy的，其目标策略可以与行为策略不同。Q-learning是Of-policy的根本原因是它旨在求解贝尔曼最优方程，而不是某一个给定策略的贝尔曼方程。

值得一提的是，有一些方法可以将On-policy算法转换为Offpolicy算法。重要性采样就是其中一个广泛使用的方法[[3](https://web.stanford.edu/class/psych209/Readings/SuttonBartoIPRLBook2ndEd.pdf),[40](https://searchworks.stanford.edu/view/1321952)],该方法将在第10章介绍。最后，TD算法有一些变体和扩展[[41](https://papers.nips.cc/paper/2010/hash/091d584fced301b442654dd8c23b3fc9-Abstract.html),[42](https://ojs.aaai.org/index.php/AAAI/article/view/10295),[43](https://jmlr.csail.mit.edu/papers/v15/dann14a.html),[44](https://www.annualreviews.org/content/journals/10.1146/annurev-statistics-031219-041220),[45](https://ieeexplore.ieee.org/document/8836506)]。例如，TD(X)方法提供了一个更加通用和统一的框架，更多信息可参见[[3](https://web.stanford.edu/class/psych209/Readings/SuttonBartoIPRLBook2ndEd.pdf),[20](https://link.springer.com/book/10.1007/978-3-031-01551-9),[46](https://link.springer.com/article/10.1007/BF00115009)]。

---