---
title: 7.3 行动值估计: n步Sarsa
comments: true  # 开启评论
---
本节介绍n步Sarsa算法——Sarsa的扩展形式。我们将看到，Sarsa与蒙特卡洛(MC)学习实际上是n步Sarsa的两个极端特例。

回顾行动值的定义为

$$q_\pi(s,a)=\mathbb{E}[G_t|S_t=s,A_t=a],\tag{7.16}$$

其中$G_t$为满足以下条件的折现回报：

$$G_{t}=R_{t+1}+\gamma R_{t+2}+\gamma^{2}R_{t+3}+\ldots.$$

事实上，$G_t$也可以分解为不同形式：

$$\begin{aligned}
\text{Sarsa} \leftarrow &G^{(1)}_t = R_{t+1} + \gamma q_{\pi}(S_{t+1}, A_{t+1}), \\
&G^{(2)}_t = R_{t+1} + \gamma R_{t+2} + \gamma^2 q_{\pi}(S_{t+2}, A_{t+2}), \\
&\vdots \\
\text{n-step Sarsa} \leftarrow &G^{(n)}_t = R_{t+1} + \gamma R_{t+2} + \cdots + \gamma^n q_{\pi}(S_{t+n}, A_{t+n}), \\
&\vdots \\
\text{MC} \leftarrow &G^{(\infty)}_t = R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \gamma^3 R_{t+4} + \cdots
\end{aligned}$$

<<<<<<< HEAD
需注意$G_t = G^{(1)}_t = G^{(2)}_t = G^{(n)}_t = G^{(\infty)}_t$，其中上标仅表示$G_t$的不同分解结构。

将$G^{(n)}_t$的不同分解形式代入$(7.16)$式中的$q_\pi(s, a)$会得到不同的算法。
=======
需注意 $G_t = G^{(1)}_t = G^{(2)}_t = G^{(n)}_t = G^{(\infty)}_t$，其中上标仅表示 $G_t$的不同分解结构。

将 $G^{(n)}_t$的不同分解形式代入 (7.16)式中的 $q_\pi(s, a)$会得到不同的算法。
>>>>>>> 50b98123eef511e961e9c05b37b46f2a667dafb0

- 当$n=1$，我们有
  
    $$q_\pi(s,a)=\mathbb{E}[G_t^{(1)}|s,a]=\mathbb{E}[R_{t+1}+\gamma q_\pi(S_{t+1},A_{t+1})|s,a].$$

    求解该方程的相应随机近似算法为

    $$q_{t+1}(s_t,a_t)=q_t(s_t,a_t)-\alpha_t(s_t,a_t)\left[q_t(s_t,a_t)-(r_{t+1}+\gamma q_t(s_{t+1},a_{t+1}))\right],$$

    即式$(7.12)$中的 Sarsa算法。

- 当$n=\infty$，我们有

    $$q_\pi(s,a)=\mathbb{E}[G_t^{(\infty)}|s,a]=\mathbb{E}[R_{t+1}+\gamma R_{t+2}+\gamma^2R_{t+3}+\ldots|s,a].$$

    求解该方程的对应算法为

    $$q_{t+1}(s_t,a_t)=g_t\doteq r_{t+1}+\gamma r_{t+2}+\gamma^2r_{t+3}+\ldots,$$

<<<<<<< HEAD
    其中$g_t$是$G_t$的一个采样样本。实际上，这是蒙特卡洛（MC）学习算法，该算法通过从$(s_t, a_t)$起始的完整回合的折现回报来近似估计$(s_t, a_t)$的行动值。
=======
    其中$g_t$是$G_t$的一个采样样本。实际上，这是蒙特卡洛（MC）学习算法，该算法通过从$(s_t, a_t)$起始的完整回合的折现回报来近似估计$(s_t, a_t)$的动作价值。
>>>>>>> 50b98123eef511e961e9c05b37b46f2a667dafb0

- 对于一般的n值，我们有
    
    $$q_\pi(s,a)=\mathbb{E}[G_t^{(n)}|s,a]=\mathbb{E}[R_{t+1}+\gamma R_{t+2}+\cdots+\gamma^nq_\pi(S_{t+n},A_{t+n})|s,a].$$

    求解上述方程的对应算法为

    $$\begin{aligned}q_{t+1}(s_{t},a_{t})&=q_{t}(s_{t},a_{t})\\&-\alpha_{t}(s_{t},a_{t})\left[q_{t}(s_{t},a_{t})-\left(r_{t+1}+\gamma r_{t+2}+\cdots+\gamma^{n}q_{t}(s_{t+n},a_{t+n})\right)\right].\end{aligned}\tag{7.17}$$

    该算法称为n步Sarsa。

<<<<<<< HEAD
综上所述，$n$步Sarsa是一种更通用的算法：当$n=1$时退化为单步Sarsa算法；当$n=\infty$(通过设定$\alpha_t=1$)时则演化为MC学习算法。
=======
综上所述，$n$步Sarsa是一种更通用的算法：当$n=1$时退化为单步Sarsa算法；当$n=\infty$（通过设定$\alpha_t=1$）时则演化为MC学习算法。
>>>>>>> 50b98123eef511e961e9c05b37b46f2a667dafb0

为实现$(7.17)$式中的n步Sarsa算法，我们需要经验样本$(s_t, a_t, r_{t+1}, s_{t+1}, a_{t+1}, ..., r_{t+n}, s_{t+n}, a_{t+n})$。由于时刻$t$尚未收集到$(r_{t+n}, s_{t+n}, a_{t+n})$，必须等到时刻$t+n$才能更新$(s_t, a_t)$的q值。因此，$(7.17)$式可改写为

$$\begin{aligned}q_{t+n}(s_{t},a_{t})&=q_{t+n-1}(s_{t},a_{t})\\&-\alpha_{t+n-1}(s_{t},a_{t})\left[q_{t+n-1}(s_{t},a_{t})-\left(r_{t+1}+\gamma r_{t+2}+\cdots+\gamma^{n}q_{t+n-1}(s_{t+n},a_{t+n})\right)\right],\end{aligned}$$

其中$q_{t+n}(s_t, a_t)$是时刻 $t+n$对$q_\pi(s_t, a_t)$的估计值。

由于n步Sarsa算法将Sarsa与蒙特卡洛(MC)学习作为两种极端情况包含其中，其性能介于两者之间便不足为奇。具体而言：

- 当$n$取值较大时，n步Sarsa接近MC学习，此时估计值具有较高的方差但偏差较小；

- 当$n$取值较小时，n步Sarsa则接近Sarsa算法，此时估计值偏差较大但方差较低。
  
需特别说明的是，本文所述的n步Sarsa算法仅用于策略评估。要学习最优策略，必须结合策略改进步骤。其实现方式与Sarsa算法类似，此处不再赘述。感兴趣的读者可参阅文献[3,第7章]对多步时序差分(TD)学习的详细分析。

---