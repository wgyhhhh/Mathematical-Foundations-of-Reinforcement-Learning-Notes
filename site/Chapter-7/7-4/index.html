
<!doctype html>
<html lang="zh" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
        <meta name="description" content="《强化学习的数学原理》的课程笔记">
      
      
        <meta name="author" content="Guangyu">
      
      
        <link rel="canonical" href="https://wgyhhhh.github.io/Mathematical-Foundations-of-Reinforcement-Learning-Notes/Chapter-7/7-4/">
      
      
        <link rel="prev" href="../7-3/">
      
      
        <link rel="next" href="../7-5/">
      
      <link rel="icon" href="../../img/favicon-96x96.png">
      <meta name="generator" content="mkdocs-1.6.1, mkdocs-material-9.1.18">
    
    
      
        <title>7.4-最优行动值估计:Q-Learning - 强化学习课程笔记</title>
      
    
    
      <link rel="stylesheet" href="../../assets/stylesheets/main.26e3688c.min.css">
      
        
        <link rel="stylesheet" href="../../assets/stylesheets/palette.ecc896b0.min.css">
      
      

    
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
      <link rel="stylesheet" href="../../css/misc.css">
    
      <link rel="stylesheet" href="../../stylesheets/extra.css">
    
    <script>__md_scope=new URL("../..",location),__md_hash=e=>[...e].reduce((e,_)=>(e<<5)-e+_.charCodeAt(0),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
    
  </head>
  
  
    
    
      
    
    
    
    
    <body dir="ltr" data-md-color-scheme="default" data-md-color-primary="black" data-md-color-accent="green">
  
    
    
      <script>var palette=__md_get("__palette");if(palette&&"object"==typeof palette.color)for(var key of Object.keys(palette.color))document.body.setAttribute("data-md-color-"+key,palette.color[key])</script>
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#74-q-learning" class="md-skip">
          跳转至
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
      <div data-md-color-scheme="default" data-md-component="outdated" hidden>
        
      </div>
    
    
      

<header class="md-header" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="页眉">
    <a href="../.." title="强化学习课程笔记" class="md-header__button md-logo" aria-label="强化学习课程笔记" data-md-component="logo">
      
  <img src="../../img/logo.jpg" alt="logo">

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3V6m0 5h18v2H3v-2m0 5h18v2H3v-2Z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            强化学习课程笔记
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              7.4-最优行动值估计:Q-Learning
            
          </span>
        </div>
      </div>
    </div>
    
      
        <form class="md-header__option" data-md-component="palette">
          
            
            
            
            <input class="md-option" data-md-color-media="(prefers-color-scheme: light)" data-md-color-scheme="default" data-md-color-primary="black" data-md-color-accent="green"  aria-label="明亮主题"  type="radio" name="__palette" id="__palette_1">
            
              <label class="md-header__button md-icon" title="明亮主题" for="__palette_2" hidden>
                <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 7a5 5 0 0 1 5 5 5 5 0 0 1-5 5 5 5 0 0 1-5-5 5 5 0 0 1 5-5m0 2a3 3 0 0 0-3 3 3 3 0 0 0 3 3 3 3 0 0 0 3-3 3 3 0 0 0-3-3m0-7 2.39 3.42C13.65 5.15 12.84 5 12 5c-.84 0-1.65.15-2.39.42L12 2M3.34 7l4.16-.35A7.2 7.2 0 0 0 5.94 8.5c-.44.74-.69 1.5-.83 2.29L3.34 7m.02 10 1.76-3.77a7.131 7.131 0 0 0 2.38 4.14L3.36 17M20.65 7l-1.77 3.79a7.023 7.023 0 0 0-2.38-4.15l4.15.36m-.01 10-4.14.36c.59-.51 1.12-1.14 1.54-1.86.42-.73.69-1.5.83-2.29L20.64 17M12 22l-2.41-3.44c.74.27 1.55.44 2.41.44.82 0 1.63-.17 2.37-.44L12 22Z"/></svg>
              </label>
            
          
            
            
            
            <input class="md-option" data-md-color-media="(prefers-color-scheme: dark)" data-md-color-scheme="slate" data-md-color-primary="black" data-md-color-accent="yellow"  aria-label="暗黑主题"  type="radio" name="__palette" id="__palette_2">
            
              <label class="md-header__button md-icon" title="暗黑主题" for="__palette_1" hidden>
                <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="m17.75 4.09-2.53 1.94.91 3.06-2.63-1.81-2.63 1.81.91-3.06-2.53-1.94L12.44 4l1.06-3 1.06 3 3.19.09m3.5 6.91-1.64 1.25.59 1.98-1.7-1.17-1.7 1.17.59-1.98L15.75 11l2.06-.05L18.5 9l.69 1.95 2.06.05m-2.28 4.95c.83-.08 1.72 1.1 1.19 1.85-.32.45-.66.87-1.08 1.27C15.17 23 8.84 23 4.94 19.07c-3.91-3.9-3.91-10.24 0-14.14.4-.4.82-.76 1.27-1.08.75-.53 1.93.36 1.85 1.19-.27 2.86.69 5.83 2.89 8.02a9.96 9.96 0 0 0 8.02 2.89m-1.64 2.02a12.08 12.08 0 0 1-7.8-3.47c-2.17-2.19-3.33-5-3.49-7.82-2.81 3.14-2.7 7.96.31 10.98 3.02 3.01 7.84 3.12 10.98.31Z"/></svg>
              </label>
            
          
        </form>
      
    
    
    
      <label class="md-header__button md-icon" for="__search">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"/></svg>
      </label>
      <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="搜索" placeholder="搜索" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"/></svg>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12Z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="查找">
        
          <a href="javascript:void(0)" class="md-search__icon md-icon" title="分享" aria-label="分享" data-clipboard data-clipboard-text="" data-md-component="search-share" tabindex="-1">
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M18 16.08c-.76 0-1.44.3-1.96.77L8.91 12.7c.05-.23.09-.46.09-.7 0-.24-.04-.47-.09-.7l7.05-4.11c.54.5 1.25.81 2.04.81a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3c0 .24.04.47.09.7L8.04 9.81C7.5 9.31 6.79 9 6 9a3 3 0 0 0-3 3 3 3 0 0 0 3 3c.79 0 1.5-.31 2.04-.81l7.12 4.15c-.05.21-.08.43-.08.66 0 1.61 1.31 2.91 2.92 2.91 1.61 0 2.92-1.3 2.92-2.91A2.92 2.92 0 0 0 18 16.08Z"/></svg>
          </a>
        
        <button type="reset" class="md-search__icon md-icon" title="清空当前内容" aria-label="清空当前内容" tabindex="-1">
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12 19 6.41Z"/></svg>
        </button>
      </nav>
      
        <div class="md-search__suggest" data-md-component="search-suggest"></div>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            正在初始化搜索引擎
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
    
    
      <div class="md-header__source">
        <a href="https://github.com/wgyhhhh/Mathematical-Foundations-of-Reinforcement-Learning-Notes" title="前往仓库" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 496 512"><!--! Font Awesome Free 6.4.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2023 Fonticons, Inc.--><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg>
  </div>
  <div class="md-source__repository">
    wgyhhhh/Mathematical-Foundations-of-Reinforcement-Learning-Notes
  </div>
</a>
      </div>
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
            
<nav class="md-tabs" aria-label="标签" data-md-component="tabs">
  <div class="md-grid">
    <ul class="md-tabs__list">
      
        
  
  


  
  
  
    <li class="md-tabs__item">
      <a href="../.." class="md-tabs__link">
        主页
      </a>
    </li>
  

      
        
  
  


  
  
  
    <li class="md-tabs__item">
      <a href="../../Box/intro/" class="md-tabs__link">
        Box
      </a>
    </li>
  

      
        
  
  


  
  
  
    <li class="md-tabs__item">
      <a href="../../Chapter-1/intro/" class="md-tabs__link">
        第一章
      </a>
    </li>
  

      
        
  
  


  
  
  
    <li class="md-tabs__item">
      <a href="../../Chapter-2/intro/" class="md-tabs__link">
        第二章
      </a>
    </li>
  

      
        
  
  


  
  
  
    <li class="md-tabs__item">
      <a href="../../Chapter-3/intro/" class="md-tabs__link">
        第三章
      </a>
    </li>
  

      
        
  
  


  
  
  
    <li class="md-tabs__item">
      <a href="../../Chapter-4/intro/" class="md-tabs__link">
        第四章
      </a>
    </li>
  

      
        
  
  


  
  
  
    <li class="md-tabs__item">
      <a href="../../Chapter-5/intro/" class="md-tabs__link">
        第五章
      </a>
    </li>
  

      
        
  
  


  
  
  
    <li class="md-tabs__item">
      <a href="../../Chapter-6/intro/" class="md-tabs__link">
        第六章
      </a>
    </li>
  

      
        
  
  
    
  


  
  
  
    <li class="md-tabs__item">
      <a href="../intro/" class="md-tabs__link md-tabs__link--active">
        第七章
      </a>
    </li>
  

      
        
  
  


  
  
  
    <li class="md-tabs__item">
      <a href="../../Chapter-8/intro/" class="md-tabs__link">
        第八章
      </a>
    </li>
  

      
        
  
  


  
  
  
    <li class="md-tabs__item">
      <a href="../../Chapter-9/intro/" class="md-tabs__link">
        第九章
      </a>
    </li>
  

      
        
  
  


  
  
  
    <li class="md-tabs__item">
      <a href="../../Chapter-10/intro/" class="md-tabs__link">
        第十章
      </a>
    </li>
  

      
        
  
  


  
  
  
    <li class="md-tabs__item">
      <a href="../../Appendix/1/" class="md-tabs__link">
        附录
      </a>
    </li>
  

      
    </ul>
  </div>
</nav>
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

  


<nav class="md-nav md-nav--primary md-nav--lifted" aria-label="导航栏" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../.." title="强化学习课程笔记" class="md-nav__button md-logo" aria-label="强化学习课程笔记" data-md-component="logo">
      
  <img src="../../img/logo.jpg" alt="logo">

    </a>
    强化学习课程笔记
  </label>
  
    <div class="md-nav__source">
      <a href="https://github.com/wgyhhhh/Mathematical-Foundations-of-Reinforcement-Learning-Notes" title="前往仓库" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 496 512"><!--! Font Awesome Free 6.4.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2023 Fonticons, Inc.--><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg>
  </div>
  <div class="md-source__repository">
    wgyhhhh/Mathematical-Foundations-of-Reinforcement-Learning-Notes
  </div>
</a>
    </div>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
      

  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
        
      
      <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_1" >
      
      
        
          
            
          
        
          
        
      
      
        
        
        <div class="md-nav__link md-nav__link--index ">
          <a href="../..">主页</a>
          
            <label for="__nav_1">
              <span class="md-nav__icon md-icon"></span>
            </label>
          
        </div>
      
      <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_1_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_1">
          <span class="md-nav__icon md-icon"></span>
          主页
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../Preface1/" class="md-nav__link">
        引言
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
      
      
      

  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
        
      
      <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_2" >
      
      
        
          
        
          
        
          
        
      
      
        <label class="md-nav__link" for="__nav_2" id="__nav_2_label" tabindex="0">
          Box
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_2_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_2">
          <span class="md-nav__icon md-icon"></span>
          Box
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../Box/intro/" class="md-nav__link">
        介绍
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../Box/Box-7-1/" class="md-nav__link">
        7.1:TD算法的推导
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../Box/Box-7-4/" class="md-nav__link">
        7.4:期望Sarsa算法
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
      
      
      

  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
        
      
      <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_3" >
      
      
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
      
      
        <label class="md-nav__link" for="__nav_3" id="__nav_3_label" tabindex="0">
          第一章
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_3_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_3">
          <span class="md-nav__icon md-icon"></span>
          第一章
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../Chapter-1/intro/" class="md-nav__link">
        介绍
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../Chapter-1/1-1/" class="md-nav__link">
        1.1-网格世界
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../Chapter-1/1-2/" class="md-nav__link">
        1.2-状态和行动
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../Chapter-1/1-3/" class="md-nav__link">
        1.3-状态转移
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../Chapter-1/1-4/" class="md-nav__link">
        1.4-策略
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../Chapter-1/1-5/" class="md-nav__link">
        1.5-奖励
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../Chapter-1/1-6/" class="md-nav__link">
        1.6-轨迹、回报、回合
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../Chapter-1/1-7/" class="md-nav__link">
        1.7-马尔科夫决策过程
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../Chapter-1/1-8/" class="md-nav__link">
        1.8-总结
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
      
      
      

  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
        
      
      <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_4" >
      
      
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
      
      
        <label class="md-nav__link" for="__nav_4" id="__nav_4_label" tabindex="0">
          第二章
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_4_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_4">
          <span class="md-nav__icon md-icon"></span>
          第二章
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../Chapter-2/intro/" class="md-nav__link">
        介绍
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../Chapter-2/2-1/" class="md-nav__link">
        2.1-为什么回报很重要?
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../Chapter-2/2-2/" class="md-nav__link">
        2.2-如何计算回报?
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../Chapter-2/2-3/" class="md-nav__link">
        2.3-状态值
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../Chapter-2/2-4/" class="md-nav__link">
        2.4-贝尔曼方程
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../Chapter-2/2-5/" class="md-nav__link">
        2.5-贝尔曼方程的例子
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../Chapter-2/2-6/" class="md-nav__link">
        2.6-贝尔曼方程的矩阵形式
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../Chapter-2/2-7/" class="md-nav__link">
        2.7-求解状态值
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../Chapter-2/2-8/" class="md-nav__link">
        2.8-行动值
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../Chapter-2/2-9/" class="md-nav__link">
        2.9-总结
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
      
      
      

  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
        
      
      <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_5" >
      
      
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
      
      
        <label class="md-nav__link" for="__nav_5" id="__nav_5_label" tabindex="0">
          第三章
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_5_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_5">
          <span class="md-nav__icon md-icon"></span>
          第三章
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../Chapter-3/intro/" class="md-nav__link">
        介绍
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../Chapter-3/3-1/" class="md-nav__link">
        3.1-如何改进策略
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../Chapter-3/3-2/" class="md-nav__link">
        3.2-最优状态值和最优策略
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../Chapter-3/3-3/" class="md-nav__link">
        3.3-贝尔曼最优公式
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../Chapter-3/3-4/" class="md-nav__link">
        3.4-从贝尔曼最优公式中求解最优策略
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../Chapter-3/3-5/" class="md-nav__link">
        3.5-影响最优策略的因素
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../Chapter-3/3-6/" class="md-nav__link">
        3.6-总结
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
      
      
      

  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
        
      
      <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_6" >
      
      
        
          
        
          
        
          
        
          
        
          
        
      
      
        <label class="md-nav__link" for="__nav_6" id="__nav_6_label" tabindex="0">
          第四章
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_6_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_6">
          <span class="md-nav__icon md-icon"></span>
          第四章
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../Chapter-4/intro/" class="md-nav__link">
        介绍
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../Chapter-4/4-1/" class="md-nav__link">
        4.1-值迭代
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../Chapter-4/4-2/" class="md-nav__link">
        4.2-策略迭代
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../Chapter-4/4-3/" class="md-nav__link">
        4.3-截断策略迭代
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../Chapter-4/4-4/" class="md-nav__link">
        4.4-总结
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
      
      
      

  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
        
      
      <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_7" >
      
      
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
      
      
        <label class="md-nav__link" for="__nav_7" id="__nav_7_label" tabindex="0">
          第五章
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_7_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_7">
          <span class="md-nav__icon md-icon"></span>
          第五章
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../Chapter-5/intro/" class="md-nav__link">
        介绍
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../Chapter-5/5-1/" class="md-nav__link">
        5.1-启发示例:期望值估计
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../Chapter-5/5-2/" class="md-nav__link">
        5.2-MC Basic:最简单的基于蒙特卡洛的算法
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../Chapter-5/5-3/" class="md-nav__link">
        5.3-MC Exploring Starts算法
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../Chapter-5/5-4/" class="md-nav__link">
        5.4-MC-Greedy算法
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../Chapter-5/5-5/" class="md-nav__link">
        5.5-探索与利用:以Greedy策略为例
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../Chapter-5/5-6/" class="md-nav__link">
        5.6-总结
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
      
      
      

  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
        
      
      <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_8" >
      
      
        
          
        
          
        
          
        
          
        
          
        
          
        
      
      
        <label class="md-nav__link" for="__nav_8" id="__nav_8_label" tabindex="0">
          第六章
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_8_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_8">
          <span class="md-nav__icon md-icon"></span>
          第六章
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../Chapter-6/intro/" class="md-nav__link">
        介绍
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../Chapter-6/6-1/" class="md-nav__link">
        6.1-启发示例:期望值估计
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../Chapter-6/6-2/" class="md-nav__link">
        6.2-罗宾斯-门罗算法
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../Chapter-6/6-3/" class="md-nav__link">
        6.3-Dvoretzky定理
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../Chapter-6/6-4/" class="md-nav__link">
        6.4-随机梯度下降
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../Chapter-6/6-5/" class="md-nav__link">
        6.5-总结
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
      
      
      

  
  
    
  
  
    
    <li class="md-nav__item md-nav__item--active md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_9" checked>
      
      
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
      
      
        <label class="md-nav__link" for="__nav_9" id="__nav_9_label" tabindex="0">
          第七章
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_9_label" aria-expanded="true">
        <label class="md-nav__title" for="__nav_9">
          <span class="md-nav__icon md-icon"></span>
          第七章
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../intro/" class="md-nav__link">
        介绍
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../7-1/" class="md-nav__link">
        7.1-状态值估计:时序差分算法
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../7-2/" class="md-nav__link">
        7.2-行动值估计:Sarsa
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../7-3/" class="md-nav__link">
        7.3-行动值估计:n步Sarsa
      </a>
    </li>
  

            
          
            
              
  
  
    
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" type="checkbox" id="__toc">
      
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          7.4-最优行动值估计:Q-Learning
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="./" class="md-nav__link md-nav__link--active">
        7.4-最优行动值估计:Q-Learning
      </a>
      
        

<nav class="md-nav md-nav--secondary" aria-label="目录">
  
  
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      目录
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#74-q-learning" class="md-nav__link">
    7.4 最优行动值估计: Q-learning
  </a>
  
    <nav class="md-nav" aria-label="7.4 最优行动值估计: Q-learning">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#741" class="md-nav__link">
    7.4.1算法描述
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#742-off-policyon-policy" class="md-nav__link">
    7.4.2 Off-policy和On-policy
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#743" class="md-nav__link">
    7.4.3 算法实现
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#744" class="md-nav__link">
    7.4.4 示例
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../7-5/" class="md-nav__link">
        7.5-时序差分算法的统一框架
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../7-6/" class="md-nav__link">
        7.6-总结
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
      
      
      

  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
        
      
      <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_10" >
      
      
        
          
        
          
        
          
        
          
        
          
        
          
        
      
      
        <label class="md-nav__link" for="__nav_10" id="__nav_10_label" tabindex="0">
          第八章
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_10_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_10">
          <span class="md-nav__icon md-icon"></span>
          第八章
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../Chapter-8/intro/" class="md-nav__link">
        介绍
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../Chapter-8/8-1/" class="md-nav__link">
        8.1-价值表示:从表格到函数
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../Chapter-8/8-2/" class="md-nav__link">
        8.2-基于值函数的时序差分算法:状态值估计
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../Chapter-8/8-3/" class="md-nav__link">
        8.3-基于值函数的时序差分算法:行动值估计
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../Chapter-8/8-4/" class="md-nav__link">
        8.4-深度Q-learning
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../Chapter-8/8-5/" class="md-nav__link">
        8.5-总结
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
      
      
      

  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
        
      
      <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_11" >
      
      
        
          
        
          
        
          
        
          
        
          
        
          
        
      
      
        <label class="md-nav__link" for="__nav_11" id="__nav_11_label" tabindex="0">
          第九章
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_11_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_11">
          <span class="md-nav__icon md-icon"></span>
          第九章
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../Chapter-9/intro/" class="md-nav__link">
        介绍
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../Chapter-9/9-1/" class="md-nav__link">
        9.1-策略表示:从表格到函数
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../Chapter-9/9-2/" class="md-nav__link">
        9.2-目标函数:定义最优策略
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../Chapter-9/9-3/" class="md-nav__link">
        9.3-目标函数的梯度
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../Chapter-9/9-4/" class="md-nav__link">
        9.4-蒙特卡洛策略梯度
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../Chapter-9/9-5/" class="md-nav__link">
        9.5-总结
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
      
      
      

  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
        
      
      <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_12" >
      
      
        
          
        
          
        
          
        
          
        
          
        
          
        
      
      
        <label class="md-nav__link" for="__nav_12" id="__nav_12_label" tabindex="0">
          第十章
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_12_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_12">
          <span class="md-nav__icon md-icon"></span>
          第十章
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../Chapter-10/intro/" class="md-nav__link">
        介绍
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../Chapter-10/10-1/" class="md-nav__link">
        10.1-最简单的演员-评论性方法
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../Chapter-10/10-2/" class="md-nav__link">
        10.2-优势演员-评论性方法
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../Chapter-10/10-3/" class="md-nav__link">
        10.3-异策略演员-评论性方法
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../Chapter-10/10-4/" class="md-nav__link">
        10.4-确定性演员-评论性方法
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../Chapter-10/10-5/" class="md-nav__link">
        10.5-总结
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
      
      
      

  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
        
      
      <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_13" >
      
      
        
          
        
      
      
        <label class="md-nav__link" for="__nav_13" id="__nav_13_label" tabindex="0">
          附录
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_13_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_13">
          <span class="md-nav__icon md-icon"></span>
          附录
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../Appendix/1/" class="md-nav__link">
        术语
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="目录">
  
  
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      目录
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#74-q-learning" class="md-nav__link">
    7.4 最优行动值估计: Q-learning
  </a>
  
    <nav class="md-nav" aria-label="7.4 最优行动值估计: Q-learning">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#741" class="md-nav__link">
    7.4.1算法描述
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#742-off-policyon-policy" class="md-nav__link">
    7.4.2 Off-policy和On-policy
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#743" class="md-nav__link">
    7.4.3 算法实现
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#744" class="md-nav__link">
    7.4.4 示例
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  

  
    <a href="https://github.com/wgyhhhh/Mathematical-Foundations-of-Reinforcement-Learning-Notes/edit/master/docs/Chapter-7/7-4.md" title="编辑此页" class="md-content__button md-icon">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M10 20H6V4h7v5h5v3.1l2-2V8l-6-6H6c-1.1 0-2 .9-2 2v16c0 1.1.9 2 2 2h4v-2m10.2-7c.1 0 .3.1.4.2l1.3 1.3c.2.2.2.6 0 .8l-1 1-2.1-2.1 1-1c.1-.1.2-.2.4-.2m0 3.9L14.1 23H12v-2.1l6.1-6.1 2.1 2.1Z"/></svg>
    </a>
  
  
    
      
    
    <a href="https://github.com/wgyhhhh/Mathematical-Foundations-of-Reinforcement-Learning-Notes/raw/master/docs/Chapter-7/7-4.md" title="查看本页的源代码" class="md-content__button md-icon">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M17 18c.56 0 1 .44 1 1s-.44 1-1 1-1-.44-1-1 .44-1 1-1m0-3c-2.73 0-5.06 1.66-6 4 .94 2.34 3.27 4 6 4s5.06-1.66 6-4c-.94-2.34-3.27-4-6-4m0 6.5a2.5 2.5 0 0 1-2.5-2.5 2.5 2.5 0 0 1 2.5-2.5 2.5 2.5 0 0 1 2.5 2.5 2.5 2.5 0 0 1-2.5 2.5M9.27 20H6V4h7v5h5v4.07c.7.08 1.36.25 2 .49V8l-6-6H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h4.5a8.15 8.15 0 0 1-1.23-2Z"/></svg>
    </a>
  


  <h1>7.4-最优行动值估计:Q-Learning</h1>

<h2 id="74-q-learning">7.4 最优行动值估计: Q-learning<a class="headerlink" href="#74-q-learning" title="Permanent link">&para;</a></h2>
<p>本节介绍强化学习中最经典的算法之一——Q-Learning算法[38,39]。需注意的是，Sarsa算法仅能估计给定策略的行动值，必须结合策略改进步骤才能获得最优策略；而Q-Learning算法可直接估计最优行动值，从而直接求解最优策略。</p>
<h3 id="741">7.4.1算法描述<a class="headerlink" href="#741" title="Permanent link">&para;</a></h3>
<p>Q-Learning算法为</p>
<div class="arithmatex">\[\begin{aligned}&amp;q_{t+1}(s_{t},a_{t})=q_{t}(s_{t},a_{t})-\alpha_{t}(s_{t},a_{t})\left[q_{t}(s_{t},a_{t})-\left(r_{t+1}+\gamma\max_{a\in\mathcal{A}(s_{t+1})}q_{t}(s_{t+1},a)\right)\right],\quad(7.18)\\&amp;q_{t+1}(s,a)=q_{t}(s,a),\quad\text{for all }(s,a)\neq(s_{t},a_{t}),\end{aligned}\]</div>
<p>其中 <span class="arithmatex">\(t =0,1,2, \dots\)</span>。此处 <span class="arithmatex">\(q_t(s_t, a_t)\)</span>表示状态-动作对 <span class="arithmatex">\((s_t, a_t)\)</span>的最优行动值估计值，<span class="arithmatex">\(\alpha_t(s_t, a_t)\)</span>为对应状态-行动对的学习率。</p>
<p>Q-learning的表达式与Sarsa相似，二者的区别仅在于<strong>TD目标</strong>：Q-learning的TD目标为<span class="arithmatex">\(r_{t+1} + \gamma \max_a q_t(s_{t+1}, a)\)</span>，而Sarsa的TD目标为<span class="arithmatex">\(r_{t+1} + \gamma q_t(s_{t+1}, a_{t+1})\)</span>。此外，给定<span class="arithmatex">\((s_t, a_t)\)</span>后，Sarsa在每次迭代中需要<span class="arithmatex">\((r_{t+1}, s_{t+1}, a_{t+1})\)</span>，而Q-learning仅需<span class="arithmatex">\((r_{t+1}, s_{t+1})\)</span>。</p>
<p>为什么Q-learning被设计为<span class="arithmatex">\((7.18)\)</span>式中的表达式，它在数学上实现了什么功能？Q-learning是一种随机近似算法，用于求解以下方程：</p>
<div class="arithmatex">\[q(s,a)=\mathbb{E}\left[R_{t+1}+\gamma\max_{a}q(S_{t+1},a)|S_{t}=s,A_{t}=a\right].\tag{7.19}\]</div>
<p>这是用行动值表示的贝尔曼最优方程。其证明过程见Box 7.5。Q-Learning的收敛性分析与定理7.1类似，此处从略。更多细节可参阅文献[32,39]。</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>这里得到的<span class="arithmatex">\(q\)</span>值不是说哪一个策略的<span class="arithmatex">\(q\)</span>值，而是最优的<span class="arithmatex">\(q\)</span>值，对应的即为最优的策略。</p>
</div>
<h3 id="742-off-policyon-policy">7.4.2 Off-policy和On-policy<a class="headerlink" href="#742-off-policyon-policy" title="Permanent link">&para;</a></h3>
<p>接下来我们引入两个重要概念：策略学习(on-policy learning)与非策略学习(off-policy learning)。Q学习与TD算法的细微差异在于：Q学习属于非策略学习，而其他算法均为策略学习。</p>
<p>任何强化学习任务中都存在两种策略：行为策略和目标策略。行为策略用于生成经验样本，而目标策略则持续更新以收敛至最优策略。当行为策略与目标策略相同时，该学习过程称为<strong>on-policy</strong>；若二者不同，则称为<strong>off-policy</strong>。</p>
<p>Off-policy learning的优势在于能够基于其他策略生成的样本进行最优策略学习，这些样本可能来自人类执行的策略等来源。一个重要特例是：行为策略可选择具有强探索性的策略，能够探索所有的状态-行动对。如果我们想要估计所有状态-动作对的行动值，就必须确保轨迹能充分访问每个状态-动作对。虽然Sarsa算法采用<span class="arithmatex">\(\epsilon\)</span>-贪婪策略维持基础探索能力，但<span class="arithmatex">\(\epsilon\)</span>值通常较小，因而探索能力有限。相比之下，若采用具有强探索能力的策略轨迹生成访问每个状态-行动对的足够多的回合，再通过off-policy learning获取最优策略，即可显著提升学习效率。</p>
<p>要判断一个算法是on-policy还是off-policy，可以从两方面考察：第一是算法求解的数学问题；第二是算法所需的经验样本。</p>
<ul>
<li>
<p>Sarsa是一种on-policy算法:</p>
<p>Sarsa算法在每次迭代中包含两个步骤：第一步是通过求解策略<span class="arithmatex">\(\pi\)</span>的贝尔曼方程来评估该策略，这需要<span class="arithmatex">\(\pi\)</span>生成的样本，因此<span class="arithmatex">\(\pi\)</span>此时作为行为策略；第二步是基于<span class="arithmatex">\(\pi\)</span>的估计值获取改进策略，此时<span class="arithmatex">\(\pi\)</span>作为不断更新并最终收敛至最优策略的目标策略。因此行为策略与目标策略是同一策略。</p>
<p>从另一个角度来看，我们可以分析算法所需的样本。Sarsa算法在每次迭代中需要的样本包括<span class="arithmatex">\((s_t, a_t, r_{t+1}, s_{t+1}, a_{t+1})\)</span>。这些样本的生成方式如下所示：</p>
<div class="arithmatex">\[s_t\xrightarrow{\pi_b}a_t\xrightarrow{\mathrm{model}}r_{t+1},s_{t+1}\xrightarrow{\pi_b}a_{t+1}\]</div>
<p>可以看出，行为策略<span class="arithmatex">\(\pi_b\)</span>是在状态<span class="arithmatex">\(s_t\)</span>下生成行动<span class="arithmatex">\(a_t\)</span>、在状态 <span class="arithmatex">\(s_{t+1}\)</span>下生成行动<span class="arithmatex">\(a_{t+1}\)</span>的策略。Sarsa算法的目标是估计目标策略 <span class="arithmatex">\(\pi_T\)</span>在状态-行动对 <span class="arithmatex">\((s_t, a_t)\)</span>处的行动值；之所以称为目标策略，是因为它在每次迭代中会根据估计值进行改进。实际上，<span class="arithmatex">\(\pi_T\)</span>与 <span class="arithmatex">\(\pi_b\)</span>是相同的，因为对 <span class="arithmatex">\(\pi_T\)</span>的评估依赖于样本 <span class="arithmatex">\((r_{t+1}, s_{t+1}, a_{t+1})\)</span>，其中 <span class="arithmatex">\(a_{t+1}\)</span>正是遵循 <span class="arithmatex">\(\pi_b\)</span>(也即<span class="arithmatex">\(\pi_t(s_{t+1})\)</span>)生成的。换言之，Sarsa所评估的策略正是用于生成经验样本的策略。</p>
</li>
<li>
<p>Q-Learning是一种off-policy算法:</p>
<p>根本原因在于，Q-learning是用于求解贝尔曼最优方程的算法，而Sarsa则是求解给定策略的贝尔曼方程。求解贝尔曼方程可以评估对应策略的性能，而求解贝尔曼最优方程则能直接生成最优值与最优策略。</p>
<p>特别地，Q-learning每次迭代所需的样本为<span class="arithmatex">\((s_t, a_t, r_{t+1}, s_{t+1})\)</span>。这些样本的生成方式如下所示：</p>
<div class="arithmatex">\[s_t\xrightarrow{\pi_b}a_t\xrightarrow{\mathrm{model}}r_{t+1},s_{t+1}\]</div>
<p>可以看出，行为策略 <span class="arithmatex">\(\pi_b\)</span>是生成状态<span class="arithmatex">\(s_t\)</span>下行动<span class="arithmatex">\(a_t\)</span>的策略。Q-Learning算法的目标是估计<span class="arithmatex">\((s_t, a_t)\)</span>的最优行动值函数。这一估计过程依赖于样本<span class="arithmatex">\((r_{t+1}, s_{t+1})\)</span>。如果<span class="arithmatex">\((s_t, a_t)\)</span>是给定的，那么<span class="arithmatex">\((r_{t+1},s_{t+1})\)</span>不依赖于策略<span class="arithmatex">\(\pi_b\)</span>，因为它由系统模型(即<span class="arithmatex">\(p(s^\prime|s,a),p(r|s,a)\)</span>)决定。因此，对<span class="arithmatex">\((s_t, a_t)\)</span>最优行动值的估计与<span class="arithmatex">\(\pi_b\)</span>无关，我们可以使用任意<span class="arithmatex">\(\pi_b\)</span>来生成<span class="arithmatex">\(s_t\)</span>状态下的<span class="arithmatex">\(a_t\)</span>。此外，此处目标策略<span class="arithmatex">\(\pi_T\)</span>是基于估计的最优值得到的贪婪策略(算法7.3)。行为策略不必与<span class="arithmatex">\(\pi_T\)</span>相同。</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>注: Q-learning的行为策略就是需要从<span class="arithmatex">\(s_t\)</span>出发根据一个策略得到一个<span class="arithmatex">\(a_t\)</span>，而目标策略就是根据计算得到的<span class="arithmatex">\(q\)</span>来选择对应的行动，当<span class="arithmatex">\(q\)</span>逐渐收敛到最优<span class="arithmatex">\(q\)</span>值时，也就收敛到了最优策略。</p>
</div>
</li>
<li>
<p>MC learning是一种on-policy算法:</p>
<p>原因与Sarsa算法类似。待评估和改进的目标策略与生成样本的行为策略是相同的。</p>
<p>另一个可能与on-policy/off-policy混淆的概念是online/offline。在线学习指智能体在与环境交互的同时更新价值函数和策略；离线学习则指智能体使用预先收集的经验数据进行策略更新，而无需与环境实时交互。若算法属于on-policy，则可在online，但不能使用其他策略生成的预存数据；若算法属于off-policy，则既可online实现也可offline实现。</p>
</li>
</ul>
<p><img alt="" src="../../img/07/3.png" /></p>
<blockquote>
<p>算法7.2</p>
</blockquote>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>看on-policy的Q-learning的update q-value可以看到，与Sarsa的公式不同的是，Sarsa中的最后项是<span class="arithmatex">\(q_t(s_{t+1},a_{t+1})\)</span>，而Q-learning的最后项是<span class="arithmatex">\(\gamma \max_a q_t(s_{t+1},a)\)</span>。可以看到，on-policy版本的Q-learning是根据策略生成经验样本更新<span class="arithmatex">\(q\)</span>值，利用<span class="arithmatex">\(q\)</span>值更新策略，然后重复。</p>
</div>
<p><img alt="" src="../../img/07/4.png" /></p>
<blockquote>
<p>算法7.3</p>
</blockquote>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>这里与on-policy的不同之处在于update target policy从<span class="arithmatex">\(\varepsilon\)</span>-贪婪策略变为了贪婪策略。</p>
</div>
<p><img alt="" src="../../img/07/5.png" /></p>
<blockquote>
<p>图<span class="arithmatex">\(7.3\)</span>：Q-Learning算法的演示示例。所有训练回合均从左上角状态开始，并在到达目标状态后终止。目标是找到从起始状态到目标状态的最优路径。奖励设置为 <span class="arithmatex">\(r_{\text{target}} =0\)</span>、<span class="arithmatex">\(r_{\text{forbidden}} = r_{\text{boundary}} = -10\)</span>，以及 <span class="arithmatex">\(r_{\text{other}} = -1\)</span>。学习率 <span class="arithmatex">\(\alpha =0.1\)</span>，<span class="arithmatex">\(\gamma\)</span>值为0.1。左图显示算法获得的最终策略，右图展示每个训练回合的总奖励与路径长度。</p>
</blockquote>
<h3 id="743">7.4.3 算法实现<a class="headerlink" href="#743" title="Permanent link">&para;</a></h3>
<p>由于Q-learning属于off-policy算法，因此既可采用on-policy也可采用off-policy方式实现。</p>
<p>Q学习的on-policy版本如算法7.2所示。该实现方式与算法7.1中的Sarsa算法类似。其中，行为策略与目标策略相同，均为<span class="arithmatex">\(\varepsilon\)</span>-贪婪策略。</p>
<p>off-policy版本如算法<span class="arithmatex">\(7.3\)</span>所示。行为策略<span class="arithmatex">\(\pi_b\)</span>可以是任意策略，只要其能生成足够的经验样本。当<span class="arithmatex">\(\pi_b\)</span>具有探索性时通常更有利。此处目标策略<span class="arithmatex">\(\pi_T\)</span>采用贪婪策略而非<span class="arithmatex">\(\varepsilon\)</span>-贪婪策略，因其不用于生成样本，故无需具备探索性。此外，本文提出的Q-learning off-policy版本采用offline实现方式：所有经验样本均需先采集后处理。</p>
<h3 id="744">7.4.4 示例<a class="headerlink" href="#744" title="Permanent link">&para;</a></h3>
<p>接下来我们通过示例展示Q-learning算法的应用。</p>
<p>第一个示例如图<span class="arithmatex">\(7.3\)</span>所示，它展示了on-policy Q-learning的过程。该算法的目标是从初始状态找到一条通往目标状态的最优路径。具体实验设置见图<span class="arithmatex">\(7.3\)</span>的图注说明。可以看出，Q-learning最终能够找到最优路径。在学习过程中，每个训练周期的路径长度逐渐缩短，而对应的总奖励值则持续增加。</p>
<p>第二组示例如图<span class="arithmatex">\(7.4\)</span>和图<span class="arithmatex">\(7.5\)</span>所示，展示了off-policy Q-learning过程。其目标是为所有状态寻找最优策略。奖励设置为 <span class="arithmatex">\(r_{\text{boundary}} = r_{\text{forbidden}} = -1\)</span>，且 <span class="arithmatex">\(r_{\text{target}} =1\)</span>。折扣率 <span class="arithmatex">\(\gamma =0.9\)</span>，学习率 <span class="arithmatex">\(\alpha =0.1\)</span>。</p>
<ul>
<li>
<p>基准真值(ground truth)：为验证 Q-learning的有效性，首先需要获知最优策略与最优状态值的基准真值。此处通过基于模型的策略迭代算法获取基准真值，具体数据见图<span class="arithmatex">\(7.4(a)\)</span>与<span class="arithmatex">\((b)\)</span>。</p>
</li>
<li>
<p>经验样本(experience samples)：行为策略采用均匀分布，即在任意状态下采取任一动作的概率均为<span class="arithmatex">\(0.2\)</span>(图<span class="arithmatex">\(7.4(c)\)</span>)。通过生成一个包含100,000步的单一回合(图<span class="arithmatex">\(7.4(d)\)</span>)，由于行为策略具备良好的探索能力，该回合会多次访问所有状态-行动对。</p>
</li>
<li>
<p>学习结果(learned results)：基于行为策略生成的回合，通过Q-learning得到的最终目标策略如图<span class="arithmatex">\(7.4(e)\)</span>所示。该策略是最优的，因为如图<span class="arithmatex">\(7.4(f)\)</span>所示，其状态值估计误差(均方根误差)收敛至零。此外，可以注意到学习得到的最优策略与图<span class="arithmatex">\(7.4(a)\)</span>中的策略并不完全相同。事实上，存在多个具有相同最优状态值的最优策略。</p>
</li>
<li>
<p>不同初始值的影响(different initial values)：由于Q-learning采用自举法(bootstraps)，算法性能取决于行动值的初始估计值。如图<span class="arithmatex">\(7.4(g)\)</span>所示，当初始估计接近真实值时，约<span class="arithmatex">\(10,000\)</span>步内即可收敛；反之则需要更多迭代步数(图<span class="arithmatex">\(7.4(h)\)</span>)。尽管如此，这些结果表明即使初始值不精确，Q-learning仍能保持较快的收敛速度。</p>
</li>
<li>
<p>不同行为策略的表现差异(different behavior policies)：当行为策略不具备探索性时，学习性能会显著下降。例如，考虑图<span class="arithmatex">\(7.5\)</span>所示的策略，它们是<span class="arithmatex">\(\epsilon\)</span>-贪心策略(<span class="arithmatex">\(\epsilon=0.5\)</span>或<span class="arithmatex">\(0.1\)</span>)；图<span class="arithmatex">\(7.4(c)\)</span>中的均匀策略可视为<span class="arithmatex">\(\epsilon=1\)</span>的<span class="arithmatex">\(\epsilon\)</span>-贪心策略）。研究表明，当<span class="arithmatex">\(\epsilon\)</span>从1降至0.5再降至0.1时，学习速度急剧下降。这是因为策略的探索能力较弱，导致经验样本不足。</p>
</li>
</ul>
<p><img alt="" src="../../img/07/6.png" /></p>
<blockquote>
<p>图<span class="arithmatex">\(7.4\)</span>：通过Q-learning演示off-policy学习的示例。<span class="arithmatex">\((a)\)</span>和<span class="arithmatex">\((b)\)</span>分别显示了最优策略和最优状态值。<span class="arithmatex">\((c)\)</span>和<span class="arithmatex">\((d)\)</span>分别展示了行为策略和生成的轨迹。<span class="arithmatex">\((e)\)</span>和<span class="arithmatex">\((f)\)</span>分别呈现了估计策略与误差演变过程。<span class="arithmatex">\((g)\)</span>和<span class="arithmatex">\((h)\)</span>则显示了不同初始值条件下的对比情况。</p>
</blockquote>
<p><img alt="" src="../../img/07/7.png" /></p>
<blockquote>
<p>图<span class="arithmatex">\(7.5\)</span>：当行为策略不具备探索性时，Q-learning算法的性能会下降。左列各图展示了行为策略的分布特征。中间列各图呈现了对应行为策略下生成的轨迹序列，每个示例中的轨迹均包含<span class="arithmatex">\(100,000\)</span>步。右列各图则显示了状态值估计值的均方根误差演变过程。</p>
</blockquote>





                
              </article>
            </div>
          
          
  <script>var tabs=__md_get("__tabs");if(Array.isArray(tabs))e:for(var set of document.querySelectorAll(".tabbed-set")){var tab,labels=set.querySelector(".tabbed-labels");for(tab of tabs)for(var label of labels.getElementsByTagName("label"))if(label.innerText.trim()===tab){var input=document.getElementById(label.htmlFor);input.checked=!0;continue e}}</script>

        </div>
        
          <button type="button" class="md-top md-icon" data-md-component="top" hidden>
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8v12Z"/></svg>
            回到页面顶部
          </button>
        
      </main>
      
        <footer class="md-footer">
  
    
      
      <nav class="md-footer__inner md-grid" aria-label="页脚" >
        
          
          <a href="../7-3/" class="md-footer__link md-footer__link--prev" aria-label="上一页: 7.3-行动值估计:n步Sarsa" rel="prev">
            <div class="md-footer__button md-icon">
              <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12Z"/></svg>
            </div>
            <div class="md-footer__title">
              <span class="md-footer__direction">
                上一页
              </span>
              <div class="md-ellipsis">
                7.3-行动值估计:n步Sarsa
              </div>
            </div>
          </a>
        
        
          
          <a href="../7-5/" class="md-footer__link md-footer__link--next" aria-label="下一页: 7.5-时序差分算法的统一框架" rel="next">
            <div class="md-footer__title">
              <span class="md-footer__direction">
                下一页
              </span>
              <div class="md-ellipsis">
                7.5-时序差分算法的统一框架
              </div>
            </div>
            <div class="md-footer__button md-icon">
              <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M4 11v2h12l-5.5 5.5 1.42 1.42L19.84 12l-7.92-7.92L10.5 5.5 16 11H4Z"/></svg>
            </div>
          </a>
        
      </nav>
    
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
        <div class="md-social">
  
    
    
    
    
    <a href="https://github.com/wgyhhhh/" target="_blank" rel="noopener" title="GitHub" class="md-social__link">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 496 512"><!--! Font Awesome Free 6.4.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2023 Fonticons, Inc.--><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg>
    </a>
  
    
    
    
    
    <a href="https://space.bilibili.com/281217178" target="_blank" rel="noopener" title="哔哩哔哩" class="md-social__link">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 6.4.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2023 Fonticons, Inc.--><path d="M488.6 104.1c16.7 18.1 24.4 39.7 23.3 65.7v202.4c-.4 26.4-9.2 48.1-26.5 65.1-17.2 17-39.1 25.9-65.5 26.7H92.02c-26.45-.8-48.21-9.8-65.28-27.2C9.682 419.4.767 396.5 0 368.2V169.8c.767-26 9.682-47.6 26.74-65.7C43.81 87.75 65.57 78.77 92.02 78h29.38L96.05 52.19c-5.75-5.73-8.63-13-8.63-21.79 0-8.8 2.88-16.06 8.63-21.797C101.8 2.868 109.1 0 117.9 0s16.1 2.868 21.9 8.603L213.1 78h88l74.5-69.397C381.7 2.868 389.2 0 398 0c8.8 0 16.1 2.868 21.9 8.603 5.7 5.737 8.6 12.997 8.6 21.797 0 8.79-2.9 16.06-8.6 21.79L394.6 78h29.3c26.4.77 48 9.75 64.7 26.1zm-38.8 69.7c-.4-9.6-3.7-17.4-10.7-23.5-5.2-6.1-14-9.4-22.7-9.8H96.05c-9.59.4-17.45 3.7-23.58 9.8-6.14 6.1-9.4 13.9-9.78 23.5v194.4c0 9.2 3.26 17 9.78 23.5s14.38 9.8 23.58 9.8H416.4c9.2 0 17-3.3 23.3-9.8 6.3-6.5 9.7-14.3 10.1-23.5V173.8zm-264.3 42.7c6.3 6.3 9.7 14.1 10.1 23.2V273c-.4 9.2-3.7 16.9-9.8 23.2-6.2 6.3-14 9.5-23.6 9.5-9.6 0-17.5-3.2-23.6-9.5-6.1-6.3-9.4-14-9.8-23.2v-33.3c.4-9.1 3.8-16.9 10.1-23.2 6.3-6.3 13.2-9.6 23.3-10 9.2.4 17 3.7 23.3 10zm191.5 0c6.3 6.3 9.7 14.1 10.1 23.2V273c-.4 9.2-3.7 16.9-9.8 23.2-6.1 6.3-14 9.5-23.6 9.5-9.6 0-17.4-3.2-23.6-9.5-7-6.3-9.4-14-9.7-23.2v-33.3c.3-9.1 3.7-16.9 10-23.2 6.3-6.3 14.1-9.6 23.3-10 9.2.4 17 3.7 23.3 10z"/></svg>
    </a>
  
    
    
    
    
    <a href="mailto:wgyhhh001@gmail.com" target="_blank" rel="noopener" title="联系作者" class="md-social__link">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 6.4.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2023 Fonticons, Inc.--><path d="M498.1 5.6c10.1 7 15.4 19.1 13.5 31.2l-64 416c-1.5 9.7-7.4 18.2-16 23s-18.9 5.4-28 1.6L284 427.7l-68.5 74.1c-8.9 9.7-22.9 12.9-35.2 8.1S160 493.2 160 480v-83.6c0-4 1.5-7.8 4.2-10.7l167.6-182.9c5.8-6.3 5.6-16-.4-22s-15.7-6.4-22-.7L106 360.8l-88.3-44.2C7.1 311.3.3 300.7 0 288.9s5.9-22.8 16.1-28.7l448-256c10.7-6.1 23.9-5.5 34 1.4z"/></svg>
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
      <div class="md-consent" data-md-component="consent" id="__consent" hidden>
        <div class="md-consent__overlay"></div>
        <aside class="md-consent__inner">
          <form class="md-consent__form md-grid md-typeset" name="consent">
            


  
    
  




<h4>Cookie 设置</h4>
<p>我们使用 cookies 来识别您的重复访问和偏好，以及衡量我们文档的有效性和用户是否找 到他们正在搜索的内容。<br/> 在您的同意下，您将帮助我们改进我们的文档。<br/> （您稍后仍可以在网页左下角重新修改 cookies 设置）</p>
<input class="md-toggle" type="checkbox" id="__settings" >
<div class="md-consent__settings">
  <ul class="task-list">
    
      
      
        
        
      
      <li class="task-list-item">
        <label class="task-list-control">
          <input type="checkbox" name="github" checked>
          <span class="task-list-indicator"></span>
          GitHub
        </label>
      </li>
    
  </ul>
</div>
<div class="md-consent__controls">
  
    
      <button class="md-button md-button--primary">同意</button>
    
    
    
  
    
    
      <button type="reset" class="md-button md-button--primary">拒绝</button>
    
    
  
    
    
    
      <label class="md-button" for="__settings">管理设定</label>
    
  
</div>
          </form>
        </aside>
      </div>
      <script>var consent=__md_get("__consent");if(consent)for(var input of document.forms.consent.elements)input.name&&(input.checked=consent[input.name]||!1);else"file:"!==location.protocol&&setTimeout(function(){document.querySelector("[data-md-component=consent]").hidden=!1},250);var action,form=document.forms.consent;for(action of["submit","reset"])form.addEventListener(action,function(e){if(e.preventDefault(),"reset"===e.type)for(var n of document.forms.consent.elements)n.name&&(n.checked=!1);__md_set("__consent",Object.fromEntries(Array.from(new FormData(form).keys()).map(function(e){return[e,!0]}))),location.hash="",location.reload()})</script>
    
    <script id="__config" type="application/json">{"base": "../..", "features": ["announce.dismiss", "navigation.tracking", "navigation.tabs", "navigation.sections", "navigation.top", "navigation.footer", "search.suggest", "search.highlight", "search.share", "navigation.expand", "navigation.indexes", "content.tabs.link", "content.tooltips", "content.code.copy", "content.action.edit", "content.action.view", "content.code.annotate"], "search": "../../assets/javascripts/workers/search.74e28a9f.min.js", "translations": {"clipboard.copied": "\u5df2\u590d\u5236", "clipboard.copy": "\u590d\u5236", "search.result.more.one": "\u5728\u8be5\u9875\u4e0a\u8fd8\u6709 1 \u4e2a\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.more.other": "\u5728\u8be5\u9875\u4e0a\u8fd8\u6709 # \u4e2a\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.none": "\u6ca1\u6709\u627e\u5230\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.one": "\u627e\u5230 1 \u4e2a\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.other": "# \u4e2a\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.placeholder": "\u952e\u5165\u4ee5\u5f00\u59cb\u641c\u7d22", "search.result.term.missing": "\u7f3a\u5c11", "select.version": "\u9009\u62e9\u5f53\u524d\u7248\u672c"}, "version": {"alias": true, "provider": "mike"}}</script>
    
    
      <script src="../../assets/javascripts/bundle.220ee61c.min.js"></script>
      
        
          <script src="https://Xiaokang2022.github.io/maliang/js/click-colorful.js"></script>
        
      
        
          <script src="https://cdn.jsdelivr.net/gh/Wcowin/Wcowin.github.io@main/docs/javascripts/extra.js"></script>
        
      
        
          <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
        
      
        
          <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
        
      
        
          <script src="https://cdn.jsdelivr.net/npm/mermaid@10.0.2/dist/add-html-label-6e56ed67.min.js"></script>
        
      
        
          <script src="../../javascripts/extra.js"></script>
        
      
        
          <script src="../../javascripts/mathjax.js"></script>
        
      
        
          <script src="https://unpkg.com/mathjax@3/es5/tex-mml-chtml.js"></script>
        
      
    
  </body>
</html>